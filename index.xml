<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>JX&#39;s log</title>
    <link>https://jiajiexiao.github.io/</link>
    <description>Recent content on JX&#39;s log</description>
    <generator>Hugo -- 0.142.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 12 May 2024 12:30:32 -0800</lastBuildDate>
    <atom:link href="https://jiajiexiao.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Biomedical LLMs (2): Genomics</title>
      <link>https://jiajiexiao.github.io/posts/2024-05-12_biollm_genomics/</link>
      <pubDate>Sun, 12 May 2024 12:30:32 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-05-12_biollm_genomics/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;span style=&#34;color: lightblue&#34;&gt;[Updated in Jan 2025]: Added &lt;a href=&#34;#helm&#34;&gt;HELM&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span style=&#34;color: lightblue&#34;&gt;[Updated in Dec 2024]: Added &lt;a href=&#34;#methylgpt&#34;&gt;MethylGPT&lt;/a&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In &lt;a href=&#34;https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/&#34;&gt;previous post&lt;/a&gt;,
we discussed some of the introduction to Large Language Models (LLMs) and how
they are constructed, trained, and utilized. Beginning with this post in the
Biomedical LLMs series, we will explore their applications in biomedical
domains. This post will concentrate on a few LLMs for genomics (e.g. DNA and
RNA).&lt;/p&gt;
&lt;h2 id=&#34;dna-language-models&#34;&gt;DNA Language Models&lt;/h2&gt;
&lt;h3 id=&#34;dnabert&#34;&gt;DNABERT&lt;/h3&gt;
&lt;p&gt;DNABERT (&lt;a href=&#34;#Ji2021&#34;&gt;Ji et al., 2021&lt;/a&gt;) is designed to encoder genomic DNA
sequences by adapting the Bidirectional Encoder Representations from
Transformers (BERT) model. DNABERT utilizes a Transformer&amp;rsquo;s encoder architecture
characterized by attention mechanisms, which effectively capture both local and
long-range dependencies in DNA sequences and offer contextual representation of
the input DNA sequences. The encoder-only architecture is identical to the BERT
base model, comprising 12 transformer layers, each with 768 hidden units and 12
attention heads.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Biomedical LLMs (1): Intro</title>
      <link>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</link>
      <pubDate>Fri, 10 May 2024 22:34:35 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</guid>
      <description>&lt;p&gt;The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and
efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in
compressing vast amounts of information through unsupervised or self-supervised training, enabling
impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly
attractive for domains where generating extensive task-specific datasets is challenging, such as in
biomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising
results, highlighting their potential to address complex problems where data scarcity is a
significant barrier. Starting from this post, I am planning to write a series on Biomedical LLMs.&lt;/p&gt;</description>
    </item>
    <item>
      <title>What a large p for small n</title>
      <link>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</link>
      <pubDate>Mon, 29 Apr 2024 08:36:29 -0700</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</guid>
      <description>&lt;p&gt;&amp;ldquo;Large p small n&amp;rdquo; describes a scenario where the number of features ($p$) is much greater than the
number of observations ($n$) for model training. While it is not a new problem, it continues to pose
significant challenges in real-world applications of machine learning, especially for domains
lacking rich data or fast and cheap data generation processes. In this blog post, I&amp;rsquo;ll document my
recent thoughts on the &amp;ldquo;large p small n&amp;rdquo; problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toward Robust AI (2): How To Achieve Robust AI</title>
      <link>https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/</link>
      <pubDate>Sat, 06 Jan 2024 20:44:25 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/</guid>
      <description>&lt;p&gt;In my previous &lt;a href=&#34;https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/&#34;&gt;post&lt;/a&gt;, I highlighted the growing influence and adoption of Artificial Intelligence
(AI) and machine learning (ML) systems, discussing how they attain &amp;ldquo;intelligence&amp;rdquo; through a careful
&amp;ldquo;data diet.&amp;rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers
to robust performance and reliable deployment. In particular, covariate shift (&lt;a href=&#34;#eq1&#34;&gt;eq 1&lt;/a&gt;) and
concept drift (&lt;a href=&#34;#eq2&#34;&gt;eq 2&lt;/a&gt;) are two major types of OOD frequently encountered in practice,
demanding mitigation for robust model deployment.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Toward Robust AI (1): Why Robustness Matters</title>
      <link>https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</link>
      <pubDate>Sun, 17 Dec 2023 14:06:46 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</guid>
      <description>&lt;h2 id=&#34;brilliant-aiml-models-remain-brittle&#34;&gt;Brilliant AI/ML Models Remain Brittle&lt;/h2&gt;
&lt;p&gt;Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their
potential to emulate, and sometimes surpass, human capabilities across diverse domains such as
vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable
Diffusion has fueled optimism, with many speculating not &lt;em&gt;if&lt;/em&gt;, but &lt;em&gt;when&lt;/em&gt;, Artificial General
Intelligence (AGI) will emerge.&lt;/p&gt;
&lt;p&gt;Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical
models. They are trained to transform inputs into predictive outputs, which includes tasks like
classification, regression, media generation, data clustering, and action planning. Despite the
awe-inspiring results, the deployment of even the most sophisticated models reveals a
fundamental fragility.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hello World</title>
      <link>https://jiajiexiao.github.io/posts/2023-12-03_hello_world/</link>
      <pubDate>Sun, 03 Dec 2023 11:50:43 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2023-12-03_hello_world/</guid>
      <description>&lt;p&gt;Greetings! This is JJ, and I am thrilled to welcome you to my corner of the internet! Taking
inspiration from &lt;a href=&#34;https://lilianweng.github.io/&#34;&gt;Lilian Weng&lt;/a&gt;, whose blog has been an
invaluable resource during my studies and work in AI/ML, I&amp;rsquo;ve decided to also share my learning
notes, thoughts, and updates through here.&lt;/p&gt;
&lt;h2 id=&#34;why-blogging&#34;&gt;Why Blogging?&lt;/h2&gt;
&lt;p&gt;While I used to constantly write some diary during my childhood, I have to admit that I haven&amp;rsquo;t done so
for quite a while. Blogging on this site, for me, it may be more than just a digital diary. I hope
to crystallize my thoughts, document my learning experiences, and engage in
meaningful conversations with readers.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://jiajiexiao.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://jiajiexiao.github.io/about/</guid>
      <description>&lt;h1 align=&#34;center&#34;&gt;Hi ðŸ‘‹, I&#39;m &lt;a href=&#34;https://jiajiexiao.github.io/about/&#34; target=&#34;blank&#34;&gt;
Jiajie &#34;JJ&#34; Xiao&lt;/a&gt;.&lt;/h1&gt;
&lt;!-- &lt;h3 align=&#34;center&#34;&gt;Description &lt;/h3&gt; --&gt;
&lt;!-- &lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://komarev.com/ghpvc/?username=jiajiexiao&amp;label=Profile%20views&amp;color=0e75b6&amp;style=flat&#34; alt=&#34;jiajiexiao&#34; /&gt; &lt;/p&gt; --&gt;
&lt;!-- &lt;a target=&#34;_blank&#34; align=&#34;center&#34;&gt;
  &lt;img align=&#34;right&#34; top=&#34;500&#34; height=&#34;300&#34; width=&#34;400&#34; alt=&#34;GIF&#34; src=&#34;https://media.giphy.com/media/SWoSkN6DxTszqIKEqv/giphy.gif&#34;&gt;
&lt;/a&gt; --&gt;
&lt;!-- &lt;a target=&#34;_blank&#34; align=&#34;center&#34;&gt;
  &lt;img align=&#34;right&#34; top=&#34;500&#34; width=&#34;200&#34; alt=&#34;Avatar&#34;  src=&#34;profile.jpeg&#34; &gt;
&lt;/a&gt; --&gt;
&lt;!-- &lt;a target=&#34;_blank&#34; align=&#34;center&#34;&gt;
  &lt;img align=&#34;right&#34; style=&#34;border-radius: 50%;&#34; width=&#34;200&#34; alt=&#34;profile&#34; src=&#34;../static/images/profile.jpeg&#34;&gt;
&lt;/a&gt; --&gt;
&lt;a target=&#34;_blank&#34; align=&#34;center&#34;&gt;
  &lt;img align=&#34;right&#34; style=&#34;border-radius: 60%;&#34; width=&#34;200&#34; alt=&#34;profile&#34; src=&#34;./images/profile.jpeg&#34;&gt;
&lt;/a&gt;
&lt;p&gt;Thank you for visiting my personal website and taking an interest in my journey.&lt;/p&gt;
&lt;p&gt;I am passionate about contributing to a better world through the lens of science and technology.
Currently, I serve as a Staff Machine Learning Scientist at Freenome, where I work on multi-domain
(e.g. multiomics, EHR) models to facilitate sensitive and specific blood-based early cancer
diagnosis. My expertise also extends to therapeutic development such as ML-guided protein design
and drug discovery, gained through my experiences at GSK and WFU.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
