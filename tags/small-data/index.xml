<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Small Data on JX&#39;s log</title>
    <link>https://jiajiexiao.github.io/tags/small-data/</link>
    <description>Recent content in Small Data on JX&#39;s log</description>
    <generator>Hugo -- 0.142.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Apr 2024 08:36:29 -0700</lastBuildDate>
    <atom:link href="https://jiajiexiao.github.io/tags/small-data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>What a large p for small n</title>
      <link>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</link>
      <pubDate>Mon, 29 Apr 2024 08:36:29 -0700</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</guid>
      <description>&lt;p&gt;&amp;ldquo;Large p small n&amp;rdquo; describes a scenario where the number of features ($p$) is much greater than the
number of observations ($n$) for model training. While it is not a new problem, it continues to pose
significant challenges in real-world applications of machine learning, especially for domains
lacking rich data or fast and cheap data generation processes. In this blog post, I&amp;rsquo;ll document my
recent thoughts on the &amp;ldquo;large p small n&amp;rdquo; problem.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
