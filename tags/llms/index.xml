<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLMs on JX&#39;s log</title>
    <link>https://jiajiexiao.github.io/tags/llms/</link>
    <description>Recent content in LLMs on JX&#39;s log</description>
    <generator>Hugo -- 0.142.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 22:34:35 -0800</lastBuildDate>
    <atom:link href="https://jiajiexiao.github.io/tags/llms/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Biomedical LLMs (1): Intro</title>
      <link>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</link>
      <pubDate>Fri, 10 May 2024 22:34:35 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</guid>
      <description>&lt;p&gt;The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and
efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in
compressing vast amounts of information through unsupervised or self-supervised training, enabling
impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly
attractive for domains where generating extensive task-specific datasets is challenging, such as in
biomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising
results, highlighting their potential to address complex problems where data scarcity is a
significant barrier. Starting from this post, I am planning to write a series on Biomedical LLMs.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
