<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Toward Robust AI (2): How To Achieve Robust AI | JX&#39;s log</title>
<meta name="keywords" content="AI/ML, Robustness">
<meta name="description" content="In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain &ldquo;intelligence&rdquo; through a careful &ldquo;data diet.&rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment.">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.afe408fbde9e6232e4bb41db6dc1a6f427226bd3dcc8f7bd3765525b7678f46b.css" integrity="sha256-r&#43;QI&#43;96eYjLku0HbbcGm9Ccia9PcyPe9N2VSW3Z49Gs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jiajiexiao.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jiajiexiao.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jiajiexiao.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jiajiexiao.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://jiajiexiao.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>




<script async src="https://www.googletagmanager.com/gtag/js?id=G-VT65G42LLD"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-VT65G42LLD', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Toward Robust AI (2): How To Achieve Robust AI" />
<meta property="og:description" content="In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain &ldquo;intelligence&rdquo; through a careful &ldquo;data diet.&rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-01-06T20:44:25-08:00" />
<meta property="article:modified_time" content="2024-01-06T20:44:25-08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Toward Robust AI (2): How To Achieve Robust AI"/>
<meta name="twitter:description" content="In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain &ldquo;intelligence&rdquo; through a careful &ldquo;data diet.&rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jiajiexiao.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Toward Robust AI (2): How To Achieve Robust AI",
      "item": "https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Toward Robust AI (2): How To Achieve Robust AI",
  "name": "Toward Robust AI (2): How To Achieve Robust AI",
  "description": "In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain \u0026ldquo;intelligence\u0026rdquo; through a careful \u0026ldquo;data diet.\u0026rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment.",
  "keywords": [
    "AI/ML", "Robustness"
  ],
  "articleBody": "In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain “intelligence” through a careful “data diet.” However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment.\n\\begin{equation} \\begin{aligned} \\text{Covariate shift:} \\quad P_\\text{S} (X) \\neq P_\\text{T} (X) \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\quad \\text{Concept drift:} \\quad P_\\text{S} (Y | X) \\neq P_\\text{T} (Y | X) \\end{aligned} \\end{equation}\nIn this post, we delve into strategies to tackle OOD and enhance the robustness of AI/ML models.\n1. Baseline: Quality Control and Normalization In various discussions today, people often talk about data quality, batch/cohort effects, or “garbage in, garbage out”. These are actually quite relevant to robustness of your model. As a result, the first thing we should consider prioritizing is to establish quality control in your data generation/collection pipeline and to conduct data normalization. For scenarios prone to data biases (e.g., batch effects in biological experiments), designing control measurements becomes crucial for later data normalization. Quality control and normalization ensure that the data’s quality is suitable for model training and inference, and that the inputs are on comparable scales.\nFig 1. Quality control and normalization workflow adopt in digital pathology. Prostate (a) and lung (b) tissue images stained with hematoxylin and eosin were normalized against target images and evaluated by pathologists. Source image is from Fig2 in Michielli, et al (2022).\nFig. 1 illustrates a clinical workflow in digital pathology. Despite variance in stain levels and random artifacts, stain normalization significantly improves image quality and enhances clinical diagnostic confidence (Michielli, et al (2022)). When developing and deploying computer vision (CV)-based AI/ML systems for assisting pathologists, stain normalization and quality controls help mitigate covariate shifts in the stain images. In other words, after these data preprocessing steps, the marginal distribution of images in source and target domains becomes comparable. Consequently, OOD problems transform into IID ones.\n$$ \\text{Biases} \\xrightarrow{\\text{e.g. batch effects}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{QC}]{\\text{normalization}} P_\\text{S} (X’) = P_\\text{T} (X’) $$\n2. Domain Adaptation when Target Domain is Accessible 2.1 Instance-Reweighting Despite our efforts in quality control and normalization, covariate shifts may persist. Such situations often indicate selection bias, where samples from the source domain may not cover all possible feature distributions, failing to fully reflect the target domain. While acquiring less biased or more representative data seems intuitive, it can be prohibitively costly in terms of both money and time, often requiring cross-functional efforts over months to years. Consequently, computational tactics or mitigations become essential and may prompt inquiries from managers or even CxOs.\nTo address this, let’s begin by checking for any known information about the target domain. The observation of covariate shifts implies some knowledge about the target domain, such as the statistical distributions of features. This information becomes valuable for guiding the use of source domain data to build a model that performs well in the target domain. Such a goal is also known as domain adaptation, because the aim is to adapt the model trained on the source domain to generalize effectively in the target domain with different distributions.\nFig 2. Instance-reweighting adapts the classifier trained in source domain to generalize to target domain. Source images are from Jong (2017).\nInstance-reweighting is a domain adaptation method leveraging the target domain distribution. To illustrate, I just use the great examples from Johann de Jong’s blog. Fig. 2 displays the distributions of features x1, x2, labels of each data point, and learned and ground truth decision boundaries. Due to selection biases, the source domain exhibits different marginal distributions compared to the target domain (Fig. 2a). Training a classifier solely on source domain data yields a decision boundary diverging from the ground truth for the target domain (Fig. 2b). Instance-reweighting involves adjusting each training instance’s weight in the source domain to match the target domain distribution (Fig. 2c). This reweighted training significantly improves the learned decision boundary’s performance in the target domain. Instance-reweighting is widely adopted when instance-specific considerations are needed for model training and evaluation. For example, addressing problems with long-tailed distributions involves static reweighting (constant sample weights) or dynamic reweighting (e.g., via focal loss 1) to penalize minority groups more, resulting in more robust performance against rare events.\nIn summary, instance-reweighting aims to mitigate encountered covariate shifts by adjusting the sample distribution. With the reweighting scheme matching the target domain, the reweighted source domain distribution $P_\\text{S}’ (X)$ aligns with the target domain distribution $P_\\text{T} (X)$.\n$$ \\text{Biases} \\xrightarrow{\\text{e.g. selection biases}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow{\\text{reweighting}} P_\\text{S}’ (X) = P_\\text{T} (X) $$\nThe additional knowledge used to derive the weights introduces some inductive bias for the model; thus, the accuracy of this additional knowledge about the target domain can be critical to the model’s robustness.\n2.2 Semi-Supervised Learning In addition to target domain statistics, if we have access to unlabeled data from the target domain, we can explore other domain adaptation methods leveraging the intrinsic structure behind the data to improve OOD performance. For instance, employing a semi-supervised learning algorithm allows incorporating unlabeled data from the target domain during model training. The initial model is trained based on the source domain data. Subsequently, this model is applied to the unlabeled target domain data to generate pseudo-labels for those unlabeled samples. Samples with confident pseudo-labels are selected as additional training data, and the model is retrained alongside the source domain samples. This iterative process refines the model, enhancing its performance in the target domain.\nFig 3. Semi-supervised learning aids domain aptation. (a) Massive unlabeled data representing the target domain is useful to overcome selection biases in the source domain and assist the model in generalizing to the target domain. (b) Pseudo-labeling algorithm iteratively augments the source domain data and regularizes the model training.\n2.3 Test-Time Training Additionally, Test-Time Training (TTT) (Sun, et al (2020)) can be explored even when there is no access to the target domain until running model testing. This technique introduces additional self-supervision tasks that can be applied to unlabeled data from the target domain. In an image classification task example as shown in Fig. 4, the model first projects the images into a latent space via an encoder. Then, the latent representation will be used for predicting the rotation angle of the images in addition to predicting the object label of the images. Self-supervised targets can be easily obtained since you know the angle at which the image is rotated in the data-augmentation process. During testing, we now have access to the target domain data as it is input for the model for making predictions. Each test image can be augmented via rotation and passed to the model for self-supervised learning. This self-supervised learning offers a chance to update the encoder based on the target domain, which learns how to project the target domain images into a comparable latent space relative to the source domain. This is the test-time training.\nFig 4. Test-Time Training. Source image is from the authors’ page (link) of TTT (Sun, et al (2020)).\nBoth semi-supervised learning and test-time training alleviate covariate shifts by seeking data augmentation to get equivalent IID.\n$$ \\text{Biases} \\xrightarrow[\\text{batch effects}]{\\text{e.g. selection biases}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{self-supervised regularization}]{\\text{data augmentation}} P_\\text{S} (X’) = P_\\text{T} (X) $$\nWhile these are effective methods and tactics in many real-world applications, there may be other factors limiting their adoption. For example, in scenarios with strong regulations, such as when the deployed model needs to be fully locked and requires FDA approval, using the target domain data (e.g., clinical trial data and samples collected post-approval) to update the model may not be allowed or under regulation. For applications that require low latency in inference time, TTT may be too slow to be deployed. All these mitigations require domain-specific consideration before being pursued.\n2.4 Transfer Learning and Fine-Tuning When we have access to the target domain’s labeled data during model development stage, although it has a very limited sample size compared to the source domain, we can conduct transfer learning and fine-tuning to adapt the model to the new domain.\nTransfer learning aims to apply knowledge learned from one domain or one task to another related domain or task, where the knowledge is often encoded as learnable parameters in deep neural networks nowadays. The rationale behind transfer learning is that there is transferable knowledge across related domains and tasks. Thus, it is beneficial to start from the pre-trained network based on the source domain with lots of data, rather than training the network from scratch based on the target domain with a limited amount of data. Transfer learning typically freezes the parameters pre-trained based on the source domain but, on top of that, adds a few additional layers whose parameters are fitted based on the target domain.\nSimilarly, fine-tuning also starts from the same pre-trained network along with possible optional layers. However, in contrast to transfer learning 2, fine-tuning also updates the weights of the pre-trained network or a subset of its layers based on the target domain.\nFig 5. Domain adpation learns domain-invariant transformations and aligns domain distributions. Source image is from Fig. 1 in Choudhary2020, et al (2020), where domain adaptation is treated as a transductive transfer learning method. Here, this image illustrates the idea that covariate shift disappears once the different domains are aligned.\nEssentially, both transfer learning and fine-tuning adapt the parameters learned from the source domain and seek further minimum adjustments to make the source and target domains comparable in the projection space (i.e., latent space) of the features. Like other domain adaptation approaches we’ve seen previously, this mitigates the covariate shift and allows the model to generalize to the target domain (Fig. 5).\n$$ \\text{Related tasks or domains} \\xrightarrow{} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{fine-tuning}]{\\text{transfer learning}} P_\\text{S} (X’) = P_\\text{T} (X’) $$\n3. Domain Generalization when Target Domain is Inaccessible So far, we have examined relatively simple OOD cases. However, more challenging scenarios can arise. In some instances, there might be no reliable prior information or even access to the target domain when training and locking the model for deployment. This challenge is often encountered in areas with limited training data and stringent regulations, where capturing a representative set becomes particularly difficult.\nMachine learning literature uses the term domain generalization to characterize the goal of building robust models for target domains that are entirely inaccessible during model development. This presents a more challenging but potentially more needed extension of domain adaptation.\nApart from covariate shift, another OOD challenge we haven’t addressed is concept drift. It can seem daunting when the relationships between features and labels differ in the target and source domains, and this shift is unknown until after building, selecting, and deploying the models. Well, performance degrade in shifted target domain may not be a big issue in low-stakes scenarios, just further train the model or retrain. However, it’s a common challenge in healthcare, where AI/ML-based or AI/ML-derived products must meet primary and secondary goals in clinical trials for disease diagnosis and treatment.\nSo, what can we do in these more difficult cases? Consider a scenario where high school students are only allowed to take the real SAT test once. They should be allowed to take as many mocks as they want, right? Would that be helpful? I guess the more closely the mocks can reflect the real test, the higher the chance to achieve similar performance in the actual exam 3. Similarly, in domain generalization, we still need to think about how we can make the source domain data more like the target domain.\nIn the realm of concept drift, the relationships between Y and X are subject to change. In reality, there can also be situations where both P(Y|X) and P(X) change across domains. The key question is whether there are features or projections of features that establish a stable relationship with target labels, regardless of the domains.\n3.1 Correlation vs Causality In our quest for a more stable relationship between features and targets, let’s revisit how AI/ML models are trained.\nModels utilize differences between model outputs and targets to update parameters. This leads to that fact that the model leverages the correlation between the features and targets to learn. A feature more correlated with the targets makes the model more likely to use it for predictions.\nFig 6. Correlation and causality. X and Y are two random variables that appear to be correlated. When digging into possible data generation process, it can be simplified as either they have a causal relationship or they have a common cause Z.\nHowever, correlation is not a stable causal relationship; it can be spurious for various reasons such as sample collection biases. According to Reichenbach’s common cause principle (Hitchcock2021), if we observe a correlation or association between two random variables, it means either one of the variables causes the other or there is a third variable that causes both (known as confounding) (Fig. 6). Causal relationships are more stable than correlation, as spurious correlations can easily change across domains or environments.\nFor instance, consider a predictive model trained on medical data in the source domain, where an attribute like “number of hospital visits” shows a high correlation with disease outcomes due to selection biases. This attribute might seem crucial in the source domain, but once the selection biases disappear in the target domain, the correlation weakens, and the attribute loses its predictive power for disease outcomes. This scenario resembles a concept drift, highlighting opportunities to address OOD by identifying domain-invariant components in features that have a (ideally) causal relationship with target labels.\n3.2 Multitask Learning and Adversarial Training To identify invariant components in features, classical approaches like feature selection and engineering might come to mind. These handcrafted pre-processing methods rely on additional prior knowledge and are often employed in statistical learning and settings with small training sizes. However, such prior knowledge, acting as an inductive bias, may limit further performance improvements. For more complex problems with reasonable training sizes, we need an end-to-end training framework to learn invariant components in features with a stable relationship to target labels.\nMultitask learning provides such a framework, allowing flexible representation learning. As depicted in the left part of Fig. 7, features can be encoded into a latent representation that predicts multiple attributes related to the main task (original target label) and auxiliary tasks (other attributes of sample instances). This facilitates the model to extract a more meaningful dense representation for predictions. Similar to Test-Time Training, well-designed auxiliary tasks can offer useful regularization on the networks, preventing overfitting on the main task.\nFig 7. Domain-adversarial training a neural network that learns both class and domain label. A neural network can be divided into encoding and decoding parts. The left side illustrates a feature extractor $G_f$ encoding inputs $X$ into latent features $f$. The right side shows latent features $f$ being decoded to predict class label $y$ and domain label $d$. While the loss $L_y$ for the class label is normally backpropagated to update the whole network, the loss $L_d$ for the domain label needs to be reversed when used for adversarial training the feature extractor. Source image is from Fig1 in Ganin, et al (2016).\nIn situations with biased attributes showing high correlation with the target label (confounding), it’s crucial for the network not to exploit such shortcuts. Adversarial training becomes relevant in this context, as it can explicitly penalize any direct or indirect use of biased attributes and confounders. The right-hand side of Fig. 7 illustrates the decoding part in multitask learning along with adversarial training. The latent feature is used to predict both class label and domain label. However, since the domain label may introduce confounding effects, one may want the constructed latent space to be less predictive of the domain label. Thus, the prediction loss for the domain label is reversed during backpropagation to the encoding layers. This process is known as adversarial training and can be effective in mitigating known biases in the source domain if being well tuned. See eq3 for exact gradient descent operation for the whole training process in math4.\n\\begin{equation} \\begin{align*} \\theta_{y} \u0026= \\theta_{y} - \\eta \\frac{\\partial L_y}{\\partial \\theta_{y}} \\\\\\ \\theta_{d} \u0026= \\theta_{d} - \\lambda \\frac{\\partial L_d}{\\partial \\theta_{d}} \\\\\\ \\theta_{f} \u0026= \\theta_{f} - \\left( \\eta \\frac{\\partial L_y}{\\partial \\theta_{y}} - \\lambda \\frac{\\partial L_d}{\\partial \\theta_{d}} \\right) \\end{align*} \\end{equation}\nThrough these approaches, the goal is to find a more meaningful and less biased representation across domains, mitigating the concept drift issue.\n$$ \\text{Confounders, biases, etc} \\xrightarrow{} P_\\text{S} (Y|X) \\neq P_\\text{T} (Y|X) \\xrightarrow{} P_\\text{S} (Y|X’) = P_\\text{T} (Y|X’) $$\nUnlike domain adaptation seen previously, these approaches leverage previously ignored meta information that may reflect variance within the source domain itself. These methods don’t require access to the target domain at all, making them suitable for domain generalization. Moreover, they can be advantageous, especially when there’s no need for access to bias or sensitive attributes during inference in the target domain. On the flip side, these methods may involve more complex training and learning dynamics due to additional regularization terms.\n3.3 Causality-inspired Representation Disentanglement and Invariant Risk Minimization When performing representation learning, we can further ask if we can segregating a portion that holds more causal relevance to the target labels, and another portion that is closely associated with confounders or bias attributes. As discussed in the previous post of this series, a vision model trained on a source domain with images of cows on grassland may exhibit misclassification when confronted with a cow on ice (Causality2024). It’s obvious that the pixels of a cow should be a causal component for correct recognition of a cow while the pixels of background is related to the dataset biases (Fig. 8a).\nFig 8. Illustration for Representation Disentanglement. (a) An image of a cow standing on a grassland can be decomposed into a cow and the background of the class land. For image recognition of a cow, the pixels of the cow are the causal factor with an invariant relationship with the concept label of a cow, while the background is with spurious correlation with the label of a cow. (b) Illustration for how causality-inspired representation disentanglement may look like. Raw inputs $X^e$ are encoded into $X_{\\text{inv}}$ and $X_{\\text{spu}}$, which are invariant across domains/environments (denoted as $e$) and spuriously correlated to environments, respectively. $X_{\\text{inv}}$ and $X_{\\text{spu}}$ should be independent from each other conditionally on the original class label $Y$ and environment $e$. Later, $X_{\\text{inv}}$ and $X_{\\text{spu}}$ are decoded to $y_c$ and $y_e$ for predicting the original class of interest and domain/environment label, respectively. This results in three loss terms, covering prediction errors for $Y$ and $e$ and conditional independence requirements. Source image is from a talk given by Koyejo in 2023ICML.\nTo address this, we can design the neural network to encourage disentanglement of the latent representation based on a causality-inspired decomposition (Fig. 8b). This approach is similar to the multitask learning framework discussed in last section, with the distinction that the latent space is now divided into two components. A key enhancement involves introducing a regularization term to promote the conditionally independent disentanglement of these components. This additional regularization ensures the separation of domain-invariant and domain-specific components during training. With the domain-invariant (hopefully causal) component from the latent representation space, we can now find a more stable $P(Y|X)$ across domains, mitigating the concept drift challenge.\n$$ \\text{Confounders, biases, etc} \\xrightarrow{} P_\\text{S} (Y|X) \\neq P_\\text{T} (Y|X) \\xrightarrow{} P_\\text{S} (Y|X_{\\text{inv}}) = P_\\text{T} (Y|X_{\\text{inv}}) $$\nMoving beyond disentanglement, the pursuit of fostering the invariance of learned representations across diverse domains or environments is encapsulated in Invariant Risk Minimization (IRM) (Arjovsky, et al (2019)). In contrast to the conventional training approach solely focused on minimizing empirical risk, known as Empirical Risk Minimization (ERM), as illustrated in more details in previous post, IRM takes a step further. By minimizing the risk across different environments, IRM renders the model less sensitive to variations that are irrelevant to the causal factors. The result is a representation that not only disentangles causal and spurious components but also ensures the invariance of causal components across diverse domains, thereby fortifying the model’s generalization capabilities. While IRM may only present significant improvement over EMR in scenarios involving anti-causal data-generation process (Wang \u0026 Veitch (2023)), IMR itself is so intriguing and worth a separate blog post or series in the future.\n3.4 Multimodal Integration and Alignment We’ve covered various tactics to enhance OOD robustness in AI/ML models. Let’s delve into one more tactic: Multimodal Integration and Alignment. This approach might not be commonly mentioned when talking about OOD robustness, but it’s an emerging strategy that proves effective. Before exploring the details of how Multimodal Integration and Alignment contribute to robustness improvement, let’s examine an example as shown below.\nFig 9. A cow playing saxophone on ice. Images were generated DALL·E 3.\nFig. 9 was generated by DALL·E 3 after receiving a text prompt of “a cow playing saxophone on ice” (link). Remarkably, the model behind DALL·E 3 seems to accurately understand various concepts, such as the cow, saxophone, and ice. This is particularly impressive given the fact that various biases present in real-world data and what such a prompt describes doesn’t exist in reality. The ML model involved in this example integrates two modalities: vision and text (Betker, et al (2023)). These modalities are integrated and aligned to match each concept before generating images based on the prompt. While the image generation part is beyond the scope of this post, multimodal integration and alignment represent a crucial tactic for enhancing the robustness of AI/ML models.\nFig 10. Contrastive Language-Image Pre-training. Source image from Fig 1 in Radford, et al (2021).\nFig. 10 illustrates Contrastive Language-Image Pre-training (CLIP), the core technique enabling vision-language integration and alignment in DALL·E. To achieve multi-modal pre-training, various images and their corresponding captions pass through an image encoder and text encoder, respectively. These encoders extract and represent the summary of information from an image $i$ and a caption $j$ as latent vectors $I_i$ and $T_j$, respectively. Training involves making the latent vectors for paired image and caption inputs ($I_i$ and $T_i$) as similar as possible, while for non-paired inputs, the vectors should be as different as possible. This process aligns the vision latent space with the text latent space, employing a contrastive learning strategy discussed in “How AI/ML Models Learn” in the last post (Xiao (2023)). CLIP leverages rich information from each modality input, capturing invariant concepts embedded in the latent space of the two modalities. Consequently, CLIP mitigates the concept drift issue. With such a pre-trained latent space, one can further conduct few-shot learning or zero-shot prediction.\n3.5 Debiasing Training Tricks In the previously discussed tactics, gradient-based learning plays a significant role. Several training techniques exist to mitigate biases in models during training. For instance, if positive and negative samples are known to be sampled from biased attribute groups, a practical approach is to design a batch sampler ensuring that all positive and negative samples within a batch originate from the same bias group. By doing so, backpropagated gradients merely reflect the target attribute of interest rather than those bias attributes.\nHowever, when the bias attribute is unknown, alternative methods come into play. One strategy involves identifying bias groups based on the latent representations of samples during the learning process. By controlling learning dynamics or applying appropriate regularization according to the latent representations, the model can be adjusted to mitigate the adverse effects of spurious correlations between biased and target attributes. Given the length of this post, I recommend interested readers explore specific examples provided in references such as Yang2023, Hong2021 and Nam2020 for further insights into these debiasing techniques.\n4. Concluding Remarks: The Pas de Deux of Data and Models In this post, we explored various strategies to address out-of-distribution (OOD) challenges, encompassing both covariate shift and concept drift, in the pursuit of robust AI/ML models. Our discussion covered domain adaptation and domain generalization methods, considering scenarios with and without prior information about the target domain. At a high level, these strategies revolve around acquiring additional data or devising more suitable model training schemes.\nBefore concluding, it’s essential to reflect on the impact of data and model architecture on performance. The top panels in Fig. 11 illustrate different fitting conditions concerning model sizes. Panels A to C depict the classic bias and variance trade-offs, where the goal of statistical learning is to approach an ideal fit (i.e., ground truth) with a reasonable number of parameters. However, with the rise of deep neural networks and improved hardware capabilities, overparameterized models have become more prevalent (Panel D in Fig. 10). These models exhibit high learning capacity to directly fit every data point, showcasing the double-decent phenomenon (Nakkiran, et al. (2021)). This phenomenon challenges the conventional bias and variance tradeoff in statistical learning. However, what’s more important here is, this toy example suggests us two modeling options: ideal fit and direct fit when faced with data.\nFig 11. Double decent phenomenon and visualization of interpolation and extrapolation zoons. Source image from Fig. 1 in Hasson, et al (2020).\nMeanwhile, when comparing the generalization in this toy case with the known ideal fit, we implicitly evaluate the accuracy of the model’s interpolation 5 and extrapolation 6. Extrapolation is generally more challenging and less accurate than interpolation, and OOD is more likely to occur in the extrapolation zone (Fig. 11G). Thus, achieving reliable extrapolation is crucial for OOD robustness. When dealing with impoverished data, seeking an ideal fit model with potential help from prior knowledge and inductive biases is still an attractive approach, especially considering its potentially better extrapolation ability compared to a direct-fit model. However, for cases with abundant data, the learning capacity of an overparameterized model may be appreciated more. Such a direct-fit on big data results in a larger interpolation zone and a smaller extrapolation zone, contributing to model robustness by relying more on interpolation than extrapolation (Fig. 11F).\nOverall, for simple problems, an ideal fit model trained through appropriate learning strategies can provide reliable extrapolation for OOD. In more complex real-world problems, finding such an ideal fit model may be challenging. However, with rich data fed to overparameterized models, the interpolation zone becomes larger, and the model’s inability to extrapolate becomes less of a liability. This example underscores the complementary nature of models and data for generalization and robustness. Appreciating the pas de deux of data and models is crucial when building trustworthy AI/ML systems. Additionally, there are other requirements for trustworthy AI/ML, such as calibration/quality of uncertainty, fairness, explainability and transparency, and privacy, which will be explored in future discussions on the road to making model predictions trustworthy decisions.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (Jan 2024). Toward Robust AI Part (2): How To Achieve Robust AI. JX’s log. Available at: https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/.\nor add the following to your BibTeX file.\n@article{xiao2023howtoachieverobustai, title = \"Toward Robust AI Part (2): How To Achieve Robust AI\", author = \"Xiao, Jiajie\", journal = \"JX's log\", year = \"2024\", month = \"Jan\", url = \"https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/\" } References Michielli, N., Caputo, A., Scotto, M., Mogetta, A., Pennisi, O. A. M., Molinari, F., … \u0026 Salvi, M. (2022). Stain normalization in digital pathology: Clinical multi-center evaluation of image quality. Journal of Pathology Informatics, 13, 100145.\nJong, J. (2017). Transfer learning: domain adaptation by instance-reweighting. Retrieved Jan 06, 2024, from https://johanndejong.wordpress.com/2017/10/15/transfer-learning-domain-adaptation-by-instance-reweighting/.\nLin, T. Y., Goyal, P., Girshick, R., He, K., \u0026 Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).\nSun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., \u0026 Hardt, M. (2020, November). Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning (pp. 9229-9248). PMLR.\nChoudhary, A., Tong, L., Zhu, Y., Mendelson, D., Rubin, D., Litjens, G., … \u0026 Zhu, J. (2020). Advancing medical imaging informatics by deep learning-based domain adaptation. Yearbook of medical informatics, 29(01), 129-138.\nHitchcock, C., \u0026 Rédei, M. (2021). Reichenbach’s Common Cause Principle (E. N. Zalta, Ed.). Stanford Encyclopedia of Philosophy; Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/entries/physics-Rpcc/\nGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., … \u0026 Lempitsky, V. (2016). Domain-adversarial training of neural networks. Journal of machine learning research, 17(59), 1-35.\nCausality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from https://ff13.fastforwardlabs.com/#how-irm-works.\nKoyejo, S. (2023). On learning domain general predictors. https://icml.cc/virtual/2023/28441.\nArjovsky, M., Bottou, L., Gulrajani, I., \u0026 Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint arXiv:1907.02893.\nWang, Z. \u0026 Veitch, V.. (2023). The Causal Structure of Domain Invariant Supervised Representation Learning. arXiv preprint arXiv:2208.06987.\nBetker, J., Goh, G., Jing, L., TimBrooks, †., Wang, J., Li, L., LongOuyang, †., JuntangZhuang, †., JoyceLee, †., YufeiGuo, †., WesamManassra, †., PrafullaDhariwal, †., CaseyChu, †., YunxinJiao, †., \u0026 Ramesh, A. (2023) Improving Image Generation with Better Captions.\nChen, W., Wang, W., Liu, L., \u0026 Lew, M. S. (2021). New ideas and trends in deep multimodal content understanding: A review. Neurocomputing, 426, 195-215.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … \u0026 Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\nXiao, Jiajie. (Dec 2023). Toward Robust AI Part (1): Why Robustness Matters. JX’s log. Available at: https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/.\nHasson, U., Nastase, S. A., \u0026 Goldstein, A. (2020). Direct fit to nature: an evolutionary perspective on biological and artificial neural networks. Neuron, 105(3), 416-434.\nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., \u0026 Sutskever, I. (2021). Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12), 124003.\nYang, Z., Huang, T., Ding, M., Dong, Y., Ying, R., Cen, Y., … \u0026 Tang, J. (2023). BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. arXiv preprint arXiv:2306.03355.\nHong, Y., \u0026 Yang, E. (2021). Unbiased classification through bias-contrastive and bias-balanced learning. Advances in Neural Information Processing Systems, 34, 26449-26461.\nNam, J., Cha, H., Ahn, S., Lee, J., \u0026 Shin, J. (2020). Learning from failure: De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems, 33, 20673-20684.\nFocal loss adds a modulating term to conventional cross-entropy loss, focusing learning on hard misclassified examples. It dynamically scales the cross-entropy loss during the training process to penalize hard misclassified samples more than others (Lin, et al (2017)). ↩︎\nFine-tuning may be treated as a type of transfer learning method by people sometimes. By this definition, transfer learning can update the weights of the pre-trained model as well. As a result, the difference between transfer learning and fine-tuning becomes more blurred, and transfer learning and fine-tuning may be used interchangeably. I personally prefer to distinguish them a bit so that it can be clearer to readers how the training was actually done. ↩︎\nOne may also think about toughening the mock exams more than the actual test. This approach ensures that achieving high performance in the mock exams translates to good or even better performance in the real test. But here, consistent performance in mock exams and real test is emphasized. Thus similarity between mocks and real test are desired. ↩︎\n$\\eta$ and $\\lambda$ in eq3 are two learning rates that update different modules in the network. ↩︎\nInterpolation is the process of estimating values within the range of known data points. In the context of machine learning, it refers to predicting or estimating values for data points that fall within the observed range of the training data. ↩︎\nExtrapolation, on the other hand, involves predicting values for data points that extend beyond the range of the observed data. It’s an extension of the model’s predictions beyond the range of the training data. ↩︎\n",
  "wordCount" : "5320",
  "inLanguage": "en",
  "datePublished": "2024-01-06T20:44:25-08:00",
  "dateModified": "2024-01-06T20:44:25-08:00",
  "author":{
    "@type": "Person",
    "name": "Jiajie Xiao"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JX's log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jiajiexiao.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jiajiexiao.github.io/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jiajiexiao.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jiajiexiao.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://jiajiexiao.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Toward Robust AI (2): How To Achieve Robust AI
    </h1>
    <div class="post-meta"><span title='2024-01-06 20:44:25 -0800 PST'>2024-01-06</span>&nbsp;·&nbsp;25 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-baseline-quality-control-and-normalization" aria-label="1. Baseline: Quality Control and Normalization">1. Baseline: Quality Control and Normalization</a></li>
                    <li>
                        <a href="#2-domain-adaptation-when-target-domain-is-accessible" aria-label="2. Domain Adaptation when Target Domain is Accessible">2. Domain Adaptation when Target Domain is Accessible</a><ul>
                            
                    <li>
                        <a href="#21-instance-reweighting" aria-label="2.1 Instance-Reweighting">2.1 Instance-Reweighting</a></li>
                    <li>
                        <a href="#22-semi-supervised-learning" aria-label="2.2 Semi-Supervised Learning">2.2 Semi-Supervised Learning</a></li>
                    <li>
                        <a href="#23-test-time-training" aria-label="2.3 Test-Time Training">2.3 Test-Time Training</a></li>
                    <li>
                        <a href="#24-transfer-learning-and-fine-tuning" aria-label="2.4 Transfer Learning and Fine-Tuning">2.4 Transfer Learning and Fine-Tuning</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-domain-generalization-when-target-domain-is-inaccessible" aria-label="3. Domain Generalization when Target Domain is Inaccessible">3. Domain Generalization when Target Domain is Inaccessible</a><ul>
                            
                    <li>
                        <a href="#31-correlation-vs-causality" aria-label="3.1 Correlation vs Causality">3.1 Correlation vs Causality</a></li>
                    <li>
                        <a href="#32-multitask-learning-and-adversarial-training" aria-label="3.2 Multitask Learning and Adversarial Training">3.2 Multitask Learning and Adversarial Training</a></li>
                    <li>
                        <a href="#33-causality-inspired-representation-disentanglement-and-invariant-risk-minimization" aria-label="3.3 Causality-inspired Representation Disentanglement and Invariant Risk Minimization">3.3 Causality-inspired Representation Disentanglement and Invariant Risk Minimization</a></li>
                    <li>
                        <a href="#34-multimodal-integration-and-alignment" aria-label="3.4 Multimodal Integration and Alignment">3.4 Multimodal Integration and Alignment</a></li>
                    <li>
                        <a href="#35-debiasing-training-tricks" aria-label="3.5 Debiasing Training Tricks">3.5 Debiasing Training Tricks</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-concluding-remarks-the-pas-de-deux-of-data-and-models" aria-label="4. Concluding Remarks: The Pas de Deux of Data and Models">4. Concluding Remarks: The Pas de Deux of Data and Models</a></li>
                    <li>
                        <a href="#citation" aria-label="Citation">Citation</a></li>
                    <li>
                        <a href="#references" aria-label="References">References</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>In my previous <a href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">post</a>, I highlighted the growing influence and adoption of Artificial Intelligence
(AI) and machine learning (ML) systems, discussing how they attain &ldquo;intelligence&rdquo; through a careful
&ldquo;data diet.&rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers
to robust performance and reliable deployment. In particular, covariate shift (<a href="#eq1">eq 1</a>) and
concept drift (<a href="#eq2">eq 2</a>) are two major types of OOD frequently encountered in practice,
demanding mitigation for robust model deployment.</p>
<p><a id="eq1"></a>
\begin{equation}
\begin{aligned}
\text{Covariate shift:} \quad P_\text{S} (X) \neq P_\text{T} (X)
\end{aligned}
\end{equation}</p>
<p><a id="eq2"></a>
\begin{equation}
\begin{aligned}
\quad \text{Concept drift:} \quad P_\text{S} (Y | X) \neq P_\text{T} (Y | X)
\end{aligned}
\end{equation}</p>
<p>In this post, we delve into strategies to tackle OOD and enhance the robustness of AI/ML models.</p>
<h2 id="1-baseline-quality-control-and-normalization">1. Baseline: Quality Control and Normalization<a hidden class="anchor" aria-hidden="true" href="#1-baseline-quality-control-and-normalization">#</a></h2>
<p>In various discussions today, people often talk about data quality, batch/cohort effects, or
&ldquo;garbage in, garbage out&rdquo;. These are actually quite relevant to robustness of your model. As a
result, the first thing we should consider prioritizing is to establish quality control in your data
generation/collection pipeline and to conduct data normalization. For scenarios prone to data biases
(e.g., batch effects in biological experiments), designing control measurements becomes crucial for
later data normalization. Quality control and normalization ensure that the data&rsquo;s quality is
suitable for model training and inference, and that the inputs are on comparable scales.</p>
<figure id="fig1" 
     class="align-center ">
    <img loading="lazy" src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9577129/bin/gr2.jpg#center"
         alt="Fig 1. Quality control and normalization workflow adopt in digital pathology. Prostate (a) and lung (b) tissue images stained with hematoxylin and eosin were normalized against target images and evaluated by pathologists. Source image is from Fig2 in Michielli, et al (2022)." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 1. Quality control and normalization workflow adopt in digital pathology.</strong> Prostate (a) and lung (b) tissue images stained with hematoxylin and eosin were normalized against target images and evaluated by pathologists. Source image is from Fig2 in <a href="#Michielli2022">Michielli, et al (2022)</a>.</p>
        </figcaption>
</figure>

<p><a href="#fig1">Fig. 1</a> illustrates a clinical workflow in digital pathology. Despite variance in stain
levels and random artifacts, stain normalization significantly improves image quality and enhances
clinical diagnostic confidence (<a href="#Michielli2022">Michielli, et al (2022)</a>). When developing and
deploying computer vision (CV)-based AI/ML systems for assisting pathologists, stain normalization
and quality controls help mitigate covariate shifts in the stain images. In other words, after these
data preprocessing steps, the marginal distribution of images in source and target domains becomes
comparable. Consequently, OOD problems transform into IID ones.</p>
<p>$$ \text{Biases} \xrightarrow{\text{e.g. batch effects}} P_\text{S} (X) \neq P_\text{T} (X) \xrightarrow[\text{QC}]{\text{normalization}}  P_\text{S} (X&rsquo;) = P_\text{T} (X&rsquo;)  $$</p>
<h2 id="2-domain-adaptation-when-target-domain-is-accessible">2. Domain Adaptation when Target Domain is Accessible<a hidden class="anchor" aria-hidden="true" href="#2-domain-adaptation-when-target-domain-is-accessible">#</a></h2>
<h3 id="21-instance-reweighting">2.1 Instance-Reweighting<a hidden class="anchor" aria-hidden="true" href="#21-instance-reweighting">#</a></h3>
<p>Despite our efforts in quality control and normalization, covariate shifts may persist. Such
situations often indicate selection bias, where samples from the source domain may not cover all
possible feature distributions, failing to fully reflect the target domain. While acquiring less
biased or more representative data seems intuitive, it can be prohibitively costly in terms of both
money and time, often requiring cross-functional efforts over months to years. Consequently,
computational tactics or mitigations become essential and may prompt inquiries from managers or even
CxOs.</p>
<p>To address this, let&rsquo;s begin by checking for any known information about the target domain. The
observation of covariate shifts implies some knowledge about the target domain, such as the
statistical distributions of features. This information becomes valuable for guiding the use of
source domain data to build a model that performs well in the target domain. Such a goal is also
known as <em>domain adaptation</em>, because the aim is to adapt the model trained on the source domain to
generalize effectively in the target domain with different distributions.</p>
<figure id="fig2" 
     class="align-center ">
    <img loading="lazy" src="../../images/instance_reweighting.png#center"
         alt="Fig 2. Instance-reweighting adapts the classifier trained in source domain to generalize to target domain. Source images are from Jong (2017)." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 2. Instance-reweighting adapts the classifier trained in source domain to generalize to target domain.</strong> Source images are from <a href="#Jong2017">Jong (2017)</a>.</p>
        </figcaption>
</figure>

<p>Instance-reweighting is a domain adaptation method leveraging the target domain distribution. To
illustrate, I just use the great examples from <a href="https://johanndejong.wordpress.com/2017/10/15/transfer-learning-domain-adaptation-by-instance-reweighting/">Johann de Jong&rsquo;s
blog</a>.
<a href="#fig2">Fig. 2</a> displays the distributions of features x1, x2, labels of each data point, and
learned and ground truth decision boundaries. Due to selection biases, the source domain exhibits
different marginal distributions compared to the target domain (Fig. 2a). Training a classifier
solely on source domain data yields a decision boundary diverging from the ground truth for the
target domain (Fig. 2b). Instance-reweighting involves adjusting each training instance&rsquo;s weight in
the source domain to match the target domain distribution (Fig. 2c). This reweighted training
significantly improves the learned decision boundary&rsquo;s performance in the target domain.
Instance-reweighting is widely adopted when instance-specific considerations are needed for model
training and evaluation. For example, addressing problems with long-tailed distributions involves
static reweighting (constant sample weights) or dynamic reweighting (e.g., via focal loss <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>) to
penalize minority groups more, resulting in more robust performance against rare events.</p>
<p>In summary, instance-reweighting aims to mitigate encountered covariate shifts by adjusting the
sample distribution. With the reweighting scheme matching the target domain, the reweighted source
domain distribution $P_\text{S}&rsquo; (X)$ aligns with the target domain distribution $P_\text{T} (X)$.</p>
<p>$$ \text{Biases} \xrightarrow{\text{e.g. selection biases}} P_\text{S} (X) \neq P_\text{T} (X) \xrightarrow{\text{reweighting}} P_\text{S}&rsquo; (X) = P_\text{T} (X)  $$</p>
<p>The additional knowledge used to derive the weights introduces some inductive bias for the model;
thus, the accuracy of this additional knowledge about the target domain can be critical to the
model&rsquo;s robustness.</p>
<h3 id="22-semi-supervised-learning">2.2 Semi-Supervised Learning<a hidden class="anchor" aria-hidden="true" href="#22-semi-supervised-learning">#</a></h3>
<p>In addition to target domain statistics, if we have access to unlabeled data from the target domain,
we can explore other domain adaptation methods leveraging the intrinsic structure behind the data to
improve OOD performance. For instance, employing a semi-supervised learning algorithm allows
incorporating unlabeled data from the target domain during model training. The initial model is
trained based on the source domain data. Subsequently, this model is applied to the unlabeled target
domain data to generate pseudo-labels for those unlabeled samples. Samples with confident
pseudo-labels are selected as additional training data, and the model is retrained alongside the
source domain samples. This iterative process refines the model, enhancing its performance in the
target domain.</p>
<figure id="fig3" 
     class="align-center ">
    <img loading="lazy" src="../../images/semi-supervised_learning.png#center"
         alt="Fig 3. Semi-supervised learning aids domain aptation. (a) Massive unlabeled data representing the target domain is useful to overcome selection biases in the source domain and assist the model in generalizing to the target domain. (b) Pseudo-labeling algorithm iteratively augments the source domain data and regularizes the model training." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 3. Semi-supervised learning aids domain aptation.</strong> (a) Massive unlabeled data representing the target domain is useful to overcome selection biases in the source domain and assist the model in generalizing to the target domain. (b) Pseudo-labeling algorithm iteratively augments the source domain data and regularizes the model training.</p>
        </figcaption>
</figure>

<h3 id="23-test-time-training">2.3 Test-Time Training<a hidden class="anchor" aria-hidden="true" href="#23-test-time-training">#</a></h3>
<p>Additionally, Test-Time Training (TTT) (<a href="#Sun2020">Sun, et al (2020)</a>) can be explored even when
there is no access to the target domain until running model testing. This technique introduces
additional self-supervision tasks that can be applied to unlabeled data from the target domain. In
an image classification task example as shown in <a href="#fig4">Fig. 4</a>, the model first projects the
images into a latent space via an encoder. Then, the latent representation will be used for
predicting the rotation angle of the images in addition to predicting the object label of the
images. Self-supervised targets can be easily obtained since you know the angle at which the image
is rotated in the data-augmentation process. During testing, we now have access to the target domain
data as it is input for the model for making predictions. Each test image can be augmented via
rotation and passed to the model for self-supervised learning. This self-supervised learning offers
a chance to update the encoder based on the target domain, which learns how to project the target
domain images into a comparable latent space relative to the source domain. This is the test-time
training.</p>
<figure id="fig4" 
     class="align-center ">
    <img loading="lazy" src="https://yueatsprograms.github.io/ttt/method.png#center"
         alt="Fig 4. Test-Time Training. Source image is from the authors&amp;rsquo; page (link) of TTT (Sun, et al (2020))." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 4. Test-Time Training.</strong> Source image is from the authors&rsquo; page (<a href="https://yueatsprograms.github.io/ttt/home.html">link</a>) of TTT (<a href="#Sun2020">Sun, et al (2020)</a>).</p>
        </figcaption>
</figure>

<p>Both semi-supervised learning and test-time training alleviate covariate shifts by seeking data
augmentation to get equivalent IID.</p>
<p>$$ \text{Biases} \xrightarrow[\text{batch effects}]{\text{e.g. selection biases}} P_\text{S} (X) \neq P_\text{T} (X) \xrightarrow[\text{self-supervised regularization}]{\text{data augmentation}} P_\text{S} (X&rsquo;) = P_\text{T} (X)  $$</p>
<p>While these are effective methods and tactics in many real-world applications, there may be other
factors limiting their adoption. For example, in scenarios with strong regulations, such as when the
deployed model needs to be fully locked and requires FDA approval, using the target domain data
(e.g., clinical trial data and samples collected post-approval) to update the model may not be
allowed or under regulation. For applications that require low latency in inference time, TTT may be
too slow to be deployed. All these mitigations require domain-specific consideration before being
pursued.</p>
<h3 id="24-transfer-learning-and-fine-tuning">2.4 Transfer Learning and Fine-Tuning<a hidden class="anchor" aria-hidden="true" href="#24-transfer-learning-and-fine-tuning">#</a></h3>
<p>When we have access to the target domain&rsquo;s labeled data during model development stage, although it
has a very limited sample size compared to the source domain, we can conduct transfer learning and
fine-tuning to adapt the model to the new domain.</p>
<p>Transfer learning aims to apply knowledge learned from one domain or one task to another related
domain or task, where the knowledge is often encoded as learnable parameters in deep neural networks
nowadays. The rationale behind transfer learning is that there is transferable knowledge across
related domains and tasks. Thus, it is beneficial to start from the pre-trained network based on the
source domain with lots of data, rather than training the network from scratch based on the target
domain with a limited amount of data. Transfer learning typically freezes the parameters pre-trained
based on the source domain but, on top of that, adds a few additional layers whose parameters are
fitted based on the target domain.</p>
<p>Similarly, fine-tuning also starts from the same pre-trained network along with possible optional
layers. However, in contrast to transfer learning <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, fine-tuning also updates the weights of the
pre-trained network or a subset of its layers based on the target domain.</p>
<figure id="fig5" 
     class="align-center ">
    <img loading="lazy" src="../../images/domain_adpation.jpg#center"
         alt="Fig 5. Domain adpation learns domain-invariant transformations and aligns domain distributions. Source image is from Fig. 1 in Choudhary2020, et al (2020), where domain adaptation is treated as a transductive transfer learning method. Here, this image illustrates the idea that covariate shift disappears once the different domains are aligned." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 5. Domain adpation learns domain-invariant transformations and aligns domain distributions.</strong> Source image is from Fig. 1 in <a href="#Choudhary2020">Choudhary2020, et al (2020)</a>, where domain adaptation is treated as a <em>transductive transfer learning</em> method. Here, this image illustrates the idea that covariate shift disappears once the different domains are aligned.</p>
        </figcaption>
</figure>

<p>Essentially, both transfer learning and fine-tuning adapt the parameters learned from the source
domain and seek further minimum adjustments to make the source and target domains comparable in the
projection space (i.e., latent space) of the features. Like other domain adaptation approaches we&rsquo;ve
seen previously, this mitigates the covariate shift and allows the model to generalize to the target
domain (<a href="#fig5">Fig. 5</a>).</p>
<p>$$ \text{Related tasks or domains} \xrightarrow{} P_\text{S} (X) \neq P_\text{T} (X) \xrightarrow[\text{fine-tuning}]{\text{transfer learning}} P_\text{S} (X&rsquo;) = P_\text{T} (X&rsquo;)  $$</p>
<h2 id="3-domain-generalization-when-target-domain-is-inaccessible">3. Domain Generalization when Target Domain is Inaccessible<a hidden class="anchor" aria-hidden="true" href="#3-domain-generalization-when-target-domain-is-inaccessible">#</a></h2>
<p>So far, we have examined relatively simple OOD cases. However, more challenging scenarios can arise.
In some instances, there might be no reliable prior information or even access to the target domain
when training and locking the model for deployment. This challenge is often encountered in areas
with limited training data and stringent regulations, where capturing a representative set becomes
particularly difficult.</p>
<p>Machine learning literature uses the term <em>domain generalization</em> to characterize the goal of
building robust models for target domains that are entirely inaccessible during model development.
This presents a more challenging but potentially more needed extension of domain adaptation.</p>
<p>Apart from covariate shift, another OOD challenge we haven&rsquo;t addressed is concept drift. It can seem
daunting when the relationships between features and labels differ in the target and source domains,
and this shift is unknown until after building, selecting, and deploying the models.  Well,
performance degrade in shifted target domain may not be a big issue in low-stakes scenarios, just
further train the model or retrain. However, it&rsquo;s a common challenge in healthcare, where
AI/ML-based or AI/ML-derived products must meet primary and secondary goals in clinical trials for
disease diagnosis and treatment.</p>
<p>So, what can we do in these more difficult cases? Consider a scenario where high school students are
only allowed to take the real SAT test once. They should be allowed to take as many mocks as they
want, right? Would that be helpful? I guess the more closely the mocks can reflect the real test,
the higher the chance to achieve similar performance in the actual exam <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. Similarly, in domain
generalization, we still need to think about how we can make the source domain data more like the
target domain.</p>
<p>In the realm of concept drift, the relationships between Y and X are subject to change. In reality,
there can also be situations where both P(Y|X) and P(X) change across domains. The key question is
whether there are features or projections of features that establish a stable relationship with
target labels, regardless of the domains.</p>
<h3 id="31-correlation-vs-causality">3.1 Correlation vs Causality<a hidden class="anchor" aria-hidden="true" href="#31-correlation-vs-causality">#</a></h3>
<p>In our quest for a more stable relationship between features and targets, let&rsquo;s revisit how AI/ML
models are trained.</p>
<p>Models utilize differences between model outputs and targets to update parameters. This leads to
that fact that the model leverages the correlation between the features and targets to learn. A
feature more correlated with the targets makes the model more likely to use it for predictions.</p>
<figure id="fig6" 
     class="align-center ">
    <img loading="lazy" src="../../images/reichenbach_common_cause.png#center"
         alt="Fig 6. Correlation and causality. X and Y are two random variables that appear to be correlated. When digging into possible data generation process, it can be simplified as either they have a causal relationship or they have a common cause Z." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 6. Correlation and causality.</strong> X and Y are two random variables that appear to be correlated. When digging into possible data generation process, it can be simplified as either they have a causal relationship or they have a common cause Z.</p>
        </figcaption>
</figure>

<p>However, correlation is not a stable causal relationship; it can be spurious for various reasons
such as sample collection biases. According to Reichenbach&rsquo;s common cause principle
(<a href="#Hitchcock2021">Hitchcock2021</a>), if we observe a correlation or association between two random
variables, it means either one of the variables causes the other or there is a third variable that
causes both (known as confounding) (<a href="#fig6">Fig. 6</a>). Causal relationships are more stable than
correlation, as spurious correlations can easily change across domains or environments.</p>
<p>For instance, consider a predictive model trained on medical data in the source domain, where an
attribute like &ldquo;number of hospital visits&rdquo; shows a high correlation with disease outcomes due to
selection biases. This attribute might seem crucial in the source domain, but once the selection
biases disappear in the target domain, the correlation weakens, and the attribute
loses its predictive power for disease outcomes. This scenario resembles a concept drift,
highlighting opportunities to address OOD by identifying domain-invariant components in features
that have a (ideally) causal relationship with target labels.</p>
<h3 id="32-multitask-learning-and-adversarial-training">3.2 Multitask Learning and Adversarial Training<a hidden class="anchor" aria-hidden="true" href="#32-multitask-learning-and-adversarial-training">#</a></h3>
<p>To identify invariant components in features, classical approaches like feature selection and
engineering might come to mind. These handcrafted pre-processing methods rely on additional
prior knowledge and are often employed in statistical learning and settings with small training
sizes. However, such prior knowledge, acting as an inductive bias, may limit further
performance improvements. For more complex problems with reasonable training sizes, we need an
end-to-end training framework to learn invariant components in features with a stable relationship
to target labels.</p>
<p>Multitask learning provides such a framework, allowing flexible representation learning. As depicted
in the left part of <a href="#fig7">Fig. 7</a>, features can be encoded into a latent representation that
predicts multiple attributes related to the main task (original target label) and auxiliary tasks
(other attributes of sample instances). This facilitates the model to extract a more meaningful
dense representation for predictions. Similar to Test-Time Training, well-designed auxiliary tasks
can offer useful regularization on the networks, preventing overfitting on the main task.</p>
<figure id="fig7" 
     class="align-center ">
    <img loading="lazy" src="../../images/domain_adversarial_training.png#center"
         alt="Fig 7. Domain-adversarial training a neural network that learns both class and domain label. A neural network can be divided into encoding and decoding parts. The left side illustrates a feature extractor $G_f$ encoding inputs $X$ into latent features $f$. The right side shows latent features $f$ being decoded to predict class label $y$ and domain label $d$. While the loss $L_y$ for the class label is normally backpropagated to update the whole network, the loss $L_d$ for the domain label needs to be reversed when used for adversarial training the feature extractor. Source image is from Fig1 in Ganin, et al (2016)." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 7. Domain-adversarial training a neural network that learns both class and domain label.</strong> A neural network can be divided into encoding and decoding parts. The left side illustrates a feature extractor $G_f$ encoding inputs $X$ into latent features $f$. The right side shows latent features $f$ being decoded to predict class label $y$ and domain label $d$. While the loss $L_y$ for the class label is normally backpropagated to update the whole network, the loss $L_d$ for the domain label needs to be reversed when used for adversarial training the feature extractor. Source image is from Fig1 in <a href="#Ganin2016">Ganin, et al (2016)</a>.</p>
        </figcaption>
</figure>

<p>In situations with biased attributes showing high correlation with the target label (confounding),
it&rsquo;s crucial for the network not to exploit such shortcuts. Adversarial training becomes relevant in
this context, as it can explicitly penalize any direct or indirect use of biased attributes and confounders.
The right-hand side of <a href="#fig7">Fig. 7</a> illustrates the decoding part in multitask learning along
with adversarial training. The latent feature is used to predict both class label and domain label.
However, since the domain label may introduce confounding effects, one may want the constructed
latent space to be less predictive of the domain label. Thus, the prediction loss for the domain
label is reversed during backpropagation to the encoding layers. This process is known as
adversarial training  and can be effective in mitigating known biases in the source domain if being
well tuned. See <a href="#eq3">eq3</a> for exact gradient descent operation for the whole training process in math<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<p><a id="eq3"></a>
\begin{equation}
\begin{align*}
\theta_{y} &amp;= \theta_{y} - \eta \frac{\partial L_y}{\partial \theta_{y}} \\\
\theta_{d} &amp;= \theta_{d} - \lambda \frac{\partial L_d}{\partial \theta_{d}} \\\
\theta_{f} &amp;= \theta_{f} - \left( \eta \frac{\partial L_y}{\partial \theta_{y}} - \lambda \frac{\partial L_d}{\partial \theta_{d}} \right)
\end{align*}
\end{equation}</p>
<p>Through these approaches, the goal is to find a more meaningful and less biased representation across domains, mitigating the concept drift issue.</p>
<p>$$ \text{Confounders, biases, etc} \xrightarrow{} P_\text{S} (Y|X) \neq P_\text{T} (Y|X) \xrightarrow{} P_\text{S} (Y|X&rsquo;) = P_\text{T} (Y|X&rsquo;) $$</p>
<p>Unlike domain adaptation seen previously, these approaches leverage previously ignored meta information that may reflect
variance within the source domain itself. These methods don&rsquo;t require access to the target domain at
all, making them suitable for domain generalization. Moreover, they can be advantageous, especially
when there&rsquo;s no need for access to bias or sensitive attributes during inference in the target
domain. On the flip side, these methods may involve more complex training and learning dynamics due
to additional regularization terms.</p>
<h3 id="33-causality-inspired-representation-disentanglement-and-invariant-risk-minimization">3.3 Causality-inspired Representation Disentanglement and Invariant Risk Minimization<a hidden class="anchor" aria-hidden="true" href="#33-causality-inspired-representation-disentanglement-and-invariant-risk-minimization">#</a></h3>
<p>When performing representation learning,  we can further ask if we can segregating a portion that
holds more causal relevance to the target labels, and another portion that is closely associated
with confounders or bias attributes. As discussed in the previous post of this series, a vision
model trained on a source domain with images of cows on grassland may exhibit misclassification when
confronted with a cow on ice (<a href="#Causality2024">Causality2024</a>). It&rsquo;s obvious that the pixels of a
cow should be a causal component for correct recognition of a cow while the pixels of background is
related to the dataset biases (<a href="#fig8">Fig. 8a</a>).</p>
<figure id="fig8" 
     class="align-center ">
    <img loading="lazy" src="../../images/representation_disentanglement.png#center"
         alt="Fig 8. Illustration for Representation Disentanglement. (a) An image of a cow standing on a grassland can be decomposed into a cow and the background of the class land. For image recognition of a cow, the pixels of the cow are the causal factor with an invariant relationship with the concept label of a cow, while the background is with spurious correlation with the label of a cow. (b) Illustration for how causality-inspired representation disentanglement may look like. Raw inputs $X^e$ are encoded into $X_{\text{inv}}$ and $X_{\text{spu}}$, which are invariant across domains/environments (denoted as $e$) and spuriously correlated to environments, respectively. $X_{\text{inv}}$ and $X_{\text{spu}}$ should be independent from each other conditionally on the original class label $Y$ and environment $e$. Later, $X_{\text{inv}}$ and $X_{\text{spu}}$ are decoded to $y_c$ and $y_e$ for predicting the original class of interest and domain/environment label, respectively. This results in three loss terms, covering prediction errors for $Y$ and $e$ and conditional independence requirements. Source image is from a talk given by Koyejo in 2023ICML." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 8. Illustration for Representation Disentanglement.</strong> (a) An image of a cow standing on a grassland can be decomposed into a cow and the background of the class land. For image recognition of a cow, the pixels of the cow are the causal factor with an invariant relationship with the concept label of a cow, while the background is with spurious correlation with the label of a cow. (b) Illustration for how causality-inspired representation disentanglement may look like. Raw inputs $X^e$ are encoded into $X_{\text{inv}}$ and $X_{\text{spu}}$, which are invariant across domains/environments (denoted as $e$) and spuriously correlated to environments, respectively. $X_{\text{inv}}$ and $X_{\text{spu}}$ should be independent from each other conditionally on the original class label $Y$ and environment $e$. Later, $X_{\text{inv}}$ and $X_{\text{spu}}$ are decoded to $y_c$ and $y_e$ for predicting the original class of interest and domain/environment label, respectively. This results in three loss terms, covering prediction errors for $Y$ and $e$ and conditional independence requirements.  Source image is from a talk given by <a href="#Koyejo2023">Koyejo in 2023ICML</a>.</p>
        </figcaption>
</figure>

<p>To address this, we can design the neural network to encourage disentanglement of the latent
representation based on a causality-inspired decomposition (<a href="#fig8">Fig. 8b</a>). This approach is
similar to the multitask learning framework discussed in <a href="#32-multitask-learning-and-adversarial-training">last
section</a>, with the distinction that the latent
space is now divided into two components. A key enhancement involves introducing a regularization
term to promote the conditionally independent disentanglement of these components. This additional
regularization ensures the separation of domain-invariant and domain-specific components during
training. With the domain-invariant (hopefully causal) component from the latent representation
space, we can now find a more stable $P(Y|X)$ across domains, mitigating the concept drift
challenge.</p>
<p>$$ \text{Confounders, biases, etc} \xrightarrow{} P_\text{S} (Y|X) \neq P_\text{T} (Y|X) \xrightarrow{} P_\text{S} (Y|X_{\text{inv}}) = P_\text{T} (Y|X_{\text{inv}}) $$</p>
<p>Moving beyond disentanglement, the pursuit of fostering the invariance of learned representations
across diverse domains or environments is encapsulated in Invariant Risk Minimization (IRM)
(<a href="#Arjovsky2019">Arjovsky, et al (2019)</a>). In contrast to the conventional training approach solely
focused on minimizing empirical risk, known as Empirical Risk Minimization (ERM), as illustrated in
more details in <a href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">previous post</a>, IRM
takes a step further. By minimizing the risk across different environments, IRM renders the model
less sensitive to variations that are irrelevant to the causal factors. The result is a
representation that not only disentangles causal and spurious components but also ensures the
invariance of causal components across diverse domains, thereby fortifying the model&rsquo;s
generalization capabilities. While IRM may only present significant improvement over EMR in
scenarios involving anti-causal data-generation process (<a href="#Wang2023">Wang &amp; Veitch (2023)</a>), IMR
itself is so intriguing and worth a separate blog post or series in the future.</p>
<h3 id="34-multimodal-integration-and-alignment">3.4 Multimodal Integration and Alignment<a hidden class="anchor" aria-hidden="true" href="#34-multimodal-integration-and-alignment">#</a></h3>
<p>We&rsquo;ve covered various tactics to enhance OOD robustness in AI/ML models. Let&rsquo;s delve into one more
tactic: <em>Multimodal Integration and Alignment</em>. This approach might not be commonly mentioned when
talking about OOD robustness, but it&rsquo;s an emerging strategy that proves effective. Before exploring
the details of how Multimodal Integration and Alignment contribute to robustness improvement, let&rsquo;s
examine an example as shown below.</p>
<figure id="fig9" 
     class="align-center ">
    <img loading="lazy" src="../../images/cow_playing_saxophone_on_ice.jpeg#center"
         alt="Fig 9. A cow playing saxophone on ice. Images were generated DALL·E 3." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 9. A cow playing saxophone on ice.</strong> Images were generated DALL·E 3.</p>
        </figcaption>
</figure>

<p><a href="#fig9">Fig. 9</a> was generated by DALL·E 3 after receiving a text prompt of &ldquo;a cow playing saxophone
on ice&rdquo;
(<a href="https://www.bing.com/images/create/a-cow-playing-saxophone-on-ice/1-65af2d92ab7741e9b8168d7b763bf90e?FORM=GENCRE">link</a>).
Remarkably, the model behind DALL·E 3 seems to accurately understand various concepts, such as the
cow, saxophone, and ice. This is particularly impressive given the fact that various biases present
in real-world data and what such a prompt describes doesn&rsquo;t exist in reality. The ML model involved
in this example integrates two modalities: vision and text (<a href="#Betker2023">Betker, et al (2023)</a>).
These modalities are integrated and aligned to match each concept before generating images based on
the prompt. While the image generation part is beyond the scope of this post, multimodal integration
and alignment represent a crucial tactic for enhancing the robustness of AI/ML models.</p>
<figure id="fig10" 
     class="align-center ">
    <img loading="lazy" src="../../images/clip.jpeg#center"
         alt="Fig 10. Contrastive Language-Image Pre-training. Source image from Fig 1 in Radford, et al (2021)." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 10. Contrastive Language-Image Pre-training.</strong> Source image from Fig 1 in <a href="#Radford2021">Radford, et al (2021)</a>.</p>
        </figcaption>
</figure>

<p><a href="#fig10">Fig. 10</a> illustrates Contrastive Language-Image Pre-training (CLIP), the core technique
enabling vision-language integration and alignment in DALL·E. To achieve multi-modal pre-training,
various images and their corresponding captions pass through an image encoder and text encoder,
respectively. These encoders extract and represent the summary of information from an image $i$ and
a caption $j$ as latent vectors $I_i$ and $T_j$, respectively. Training involves making the latent
vectors for paired image and caption inputs ($I_i$ and $T_i$) as similar as possible, while for
non-paired inputs, the vectors should be as different as possible. This process aligns the vision
latent space with the text latent space, employing a contrastive learning strategy discussed in
<a href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/#the-data-diet-how-aiml-models-learn">&ldquo;How AI/ML Models
Learn&rdquo;</a>
in the last post (<a href="#xiao2023">Xiao (2023)</a>). CLIP leverages rich information from each modality
input, capturing invariant concepts embedded in the latent space of the two modalities.
Consequently, CLIP mitigates the concept drift issue. With such a pre-trained latent space, one can
further conduct few-shot learning or zero-shot prediction.</p>
<h3 id="35-debiasing-training-tricks">3.5 Debiasing Training Tricks<a hidden class="anchor" aria-hidden="true" href="#35-debiasing-training-tricks">#</a></h3>
<p>In the previously discussed tactics, gradient-based learning plays a significant role. Several
training techniques exist to mitigate biases in models during training. For instance, if positive
and negative samples are known to be sampled from biased attribute groups, a practical approach is
to design a batch sampler ensuring that all positive and negative samples within a batch originate
from the same bias group. By doing so, backpropagated gradients merely reflect the target
attribute of interest rather than those bias attributes.</p>
<p>However, when the bias attribute is unknown, alternative methods come into play. One strategy
involves identifying bias groups based on the latent representations of samples during the learning
process. By controlling learning dynamics or applying appropriate regularization according to the
latent representations, the model can be adjusted to mitigate the adverse effects of spurious
correlations between biased and target attributes. Given the length of this post, I recommend
interested readers explore specific examples provided in references such as <a href="#Yang2023">Yang2023</a>,
<a href="#Hong2021">Hong2021</a> and <a href="#Nam2020">Nam2020</a> for further insights into these debiasing techniques.</p>
<h2 id="4-concluding-remarks-the-pas-de-deux-of-data-and-models">4. Concluding Remarks: The Pas de Deux of Data and Models<a hidden class="anchor" aria-hidden="true" href="#4-concluding-remarks-the-pas-de-deux-of-data-and-models">#</a></h2>
<p>In this post, we explored various strategies to address out-of-distribution (OOD) challenges,
encompassing both covariate shift and concept drift, in the pursuit of robust AI/ML models. Our
discussion covered domain adaptation and domain generalization methods, considering scenarios with
and without prior information about the target domain. At a high level, these strategies revolve
around acquiring additional data or devising more suitable model training schemes.</p>
<p>Before concluding, it&rsquo;s essential to reflect on the impact of data and model architecture on
performance. The top panels in <a href="#fig11">Fig. 11</a> illustrate different fitting conditions concerning
model sizes. Panels A to C depict the classic bias and variance trade-offs, where the goal of
statistical learning is to approach an ideal fit (i.e., ground truth) with a reasonable number of
parameters. However, with the rise of deep neural networks and improved hardware capabilities,
overparameterized models have become more prevalent (Panel D in Fig. 10). These models exhibit high
learning capacity to directly fit every data point, showcasing the double-decent phenomenon
(<a href="#Nakkiran2021">Nakkiran, et al. (2021)</a>). This phenomenon challenges the conventional bias and
variance tradeoff in statistical learning. However, what&rsquo;s more important here is, this toy example
suggests us two modeling options: ideal fit and direct fit when faced with data.</p>
<figure id="fig11" 
     class="align-center ">
    <img loading="lazy" src="../../images/data_model_dance.jpg#center"
         alt="Fig 11. Double decent phenomenon and visualization of interpolation and extrapolation zoons. Source image from Fig. 1 in Hasson, et al (2020)." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 11. Double decent phenomenon and visualization of interpolation and extrapolation zoons.</strong> Source image from Fig. 1 in <a href="#Hasson2020">Hasson, et al (2020)</a>.</p>
        </figcaption>
</figure>

<p>Meanwhile, when comparing the generalization in this toy case with the known ideal fit, we
implicitly evaluate the accuracy of the model&rsquo;s interpolation <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> and extrapolation <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.
Extrapolation is generally more challenging and less accurate than interpolation, and OOD is more
likely to occur in the extrapolation zone (<a href="#fig11">Fig. 11G</a>). Thus, achieving reliable
extrapolation is crucial for OOD robustness. When dealing with impoverished data, seeking an ideal
fit model with potential help from prior knowledge and inductive biases is still an attractive
approach, especially considering its potentially better extrapolation ability compared to a
direct-fit model. However, for cases with abundant data, the learning capacity of an
overparameterized model may be appreciated more. Such a direct-fit on big data results in a larger
interpolation zone and a smaller extrapolation zone, contributing to model robustness by relying
more on interpolation than extrapolation (<a href="#fig11">Fig. 11F</a>).</p>
<p>Overall, for simple problems, an ideal fit model trained through appropriate learning strategies can
provide reliable extrapolation for OOD. In more complex real-world problems, finding such an ideal
fit model may be challenging. However, with rich data fed to overparameterized models, the
interpolation zone becomes larger, and the model&rsquo;s inability to extrapolate becomes less of a
liability. This example underscores the complementary nature of models and data for generalization
and robustness. Appreciating the pas de deux of data and models is crucial when building trustworthy
AI/ML systems. Additionally, there are other requirements for trustworthy AI/ML, such as
calibration/quality of uncertainty, fairness, explainability and transparency, and privacy, which
will be explored in future discussions on the road to making model predictions trustworthy
decisions.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>If you find this post helpful and are interested in referencing it in your write-up, you can cite it as</p>
<blockquote>
<p>Xiao, Jiajie. (Jan 2024). <em>Toward Robust AI Part (2): How To Achieve Robust AI</em>. JX&rsquo;s log. Available
at: <a href="https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/">https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/</a>.</p>
</blockquote>
<p>or add the following to your BibTeX file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bib" data-lang="bib"><span style="display:flex;"><span><span style="color:#a6e22e">@article</span>{xiao2023howtoachieverobustai,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">title</span>   = <span style="color:#e6db74">&#34;Toward Robust AI Part (2): How To Achieve Robust AI&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">author</span>  = <span style="color:#e6db74">&#34;Xiao, Jiajie&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">journal</span> = <span style="color:#e6db74">&#34;JX&#39;s log&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">year</span>    = <span style="color:#e6db74">&#34;2024&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">month</span>   = <span style="color:#e6db74">&#34;Jan&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">url</span>     = <span style="color:#e6db74">&#34;https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>
<p><a id="Michielli2022"></a> Michielli, N., Caputo, A., Scotto, M., Mogetta, A., Pennisi, O. A. M., Molinari, F., &hellip; &amp; Salvi, M. (2022). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10000688/">Stain normalization in digital pathology: Clinical multi-center evaluation of image quality</a>. Journal of Pathology Informatics, 13, 100145.</p>
</li>
<li>
<p><a id="Jong2017"></a> Jong, J. (2017). Transfer learning: domain adaptation by instance-reweighting. Retrieved Jan 06, 2024, from <a href="https://johanndejong.wordpress.com/2017/10/15/transfer-learning-domain-adaptation-by-instance-reweighting/">https://johanndejong.wordpress.com/2017/10/15/transfer-learning-domain-adaptation-by-instance-reweighting/</a>.</p>
</li>
<li>
<p><a id="Lin2017"></a> Lin, T. Y., Goyal, P., Girshick, R., He, K., &amp; Dollár, P. (2017). <a href="https://arxiv.org/abs/1708.02002">Focal loss for dense object detection</a>. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).</p>
</li>
<li>
<p><a id="Sun2020"></a> Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., &amp; Hardt, M. (2020, November). <a href="https://arxiv.org/abs/1909.13231">Test-time training with self-supervision for generalization under distribution shifts</a>. In International conference on machine learning (pp. 9229-9248). PMLR.</p>
</li>
<li>
<p><a id="Choudhary2020"></a> Choudhary, A., Tong, L., Zhu, Y., Mendelson, D., Rubin, D., Litjens, G., &hellip; &amp; Zhu, J. (2020). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7442502/">Advancing medical imaging informatics by deep learning-based domain adaptation</a>. Yearbook of medical informatics, 29(01), 129-138.</p>
</li>
<li>
<p><a id="Hitchcock2021"></a> Hitchcock, C., &amp; Rédei, M. (2021). Reichenbach’s Common Cause Principle (E. N. Zalta, Ed.). Stanford Encyclopedia of Philosophy; Metaphysics Research Lab, Stanford University. <a href="https://plato.stanford.edu/entries/physics-Rpcc/">https://plato.stanford.edu/entries/physics-Rpcc/</a></p>
</li>
<li>
<p><a id="Ganin2016"></a>  Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., &hellip; &amp; Lempitsky, V. (2016). <a href="https://arxiv.org/abs/1505.07818">Domain-adversarial training of neural networks</a>. Journal of machine learning research, 17(59), 1-35.</p>
</li>
<li>
<p><a id="Causality2024"></a> Causality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from <a href="https://ff13.fastforwardlabs.com/#how-irm-works">https://ff13.fastforwardlabs.com/#how-irm-works</a>.</p>
</li>
<li>
<p><a id="Koyejo2023"></a> Koyejo, S. (2023). On learning domain general predictors. <a href="https://icml.cc/virtual/2023/28441">https://icml.cc/virtual/2023/28441</a>.</p>
</li>
<li>
<p><a id="Arjovsky2019"></a> Arjovsky, M., Bottou, L., Gulrajani, I., &amp; Lopez-Paz, D. (2019). <a href="https://arxiv.org/abs/1907.02893">Invariant risk minimization</a>. arXiv preprint arXiv:1907.02893.</p>
</li>
<li>
<p><a id="Wang2023"></a> Wang, Z. &amp; Veitch, V.. (2023). <a href="https://arxiv.org/abs/2208.06987">The Causal Structure of Domain Invariant Supervised Representation Learning</a>. arXiv preprint arXiv:2208.06987.</p>
</li>
<li>
<p><a id="Betker2023"></a> Betker, J., Goh, G., Jing, L., TimBrooks, †., Wang, J., Li, L., LongOuyang, †., JuntangZhuang, †., JoyceLee, †., YufeiGuo, †., WesamManassra, †., PrafullaDhariwal, †., CaseyChu, †., YunxinJiao, †., &amp; Ramesh, A. (2023) <a href="https://cdn.openai.com/papers/dall-e-3.pdf">Improving Image Generation with Better Captions</a>.</p>
</li>
<li>
<p><a id="Chen2021"></a> Chen, W., Wang, W., Liu, L., &amp; Lew, M. S. (2021). <a href="https://arxiv.org/abs/2010.08189">New ideas and trends in deep multimodal content understanding: A review</a>. Neurocomputing, 426, 195-215.</p>
</li>
<li>
<p><a id="Radford2021"></a> Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., &hellip; &amp; Sutskever, I. (2021, July). <a href="https://arxiv.org/abs/2103.00020">Learning transferable visual models from natural language supervision</a>. In International conference on machine learning (pp. 8748-8763). PMLR.</p>
</li>
<li>
<p><a id="Xiao2023"></a> Xiao, Jiajie. (Dec 2023). Toward Robust AI Part (1): Why Robustness Matters. JX’s log. Available at: <a href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</a>.</p>
</li>
<li>
<p><a id="Hasson2020"></a>  Hasson, U., Nastase, S. A., &amp; Goldstein, A. (2020). <a href="https://www.sciencedirect.com/science/article/pii/S089662731931044X">Direct fit to nature: an evolutionary perspective on biological and artificial neural networks</a>. Neuron, 105(3), 416-434.</p>
</li>
<li>
<p><a id="Nakkiran2021"></a> Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., &amp; Sutskever, I. (2021). <a href="https://iopscience.iop.org/article/10.1088/1742-5468/ac3a74/meta">Deep double descent: Where bigger models and more data hurt</a>. Journal of Statistical Mechanics: Theory and Experiment, 2021(12), 124003.</p>
</li>
<li>
<p><a id="Yang2023"></a> Yang, Z., Huang, T., Ding, M., Dong, Y., Ying, R., Cen, Y., &hellip; &amp; Tang, J. (2023). <a href="https://arxiv.org/abs/2306.03355">BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs</a>. arXiv preprint arXiv:2306.03355.</p>
</li>
<li>
<p><a id="Hong2021"></a> Hong, Y., &amp; Yang, E. (2021). <a href="https://proceedings.neurips.cc/paper/2021/hash/de8aa43e5d5fa8536cf23e54244476fa-Abstract.html">Unbiased classification through bias-contrastive and bias-balanced learning</a>. Advances in Neural Information Processing Systems, 34, 26449-26461.</p>
</li>
<li>
<p><a id="Nam2020"></a> Nam, J., Cha, H., Ahn, S., Lee, J., &amp; Shin, J. (2020). <a href="https://arxiv.org/abs/2007.02561">Learning from failure: De-biasing classifier from biased classifier</a>. Advances in Neural Information Processing Systems, 33, 20673-20684.</p>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Focal loss adds a modulating term to conventional cross-entropy loss, focusing learning on
hard misclassified examples. It dynamically scales the cross-entropy loss during the training
process to penalize hard misclassified samples more than others (<a href="#Lin2017">Lin, et al (2017)</a>).&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Fine-tuning may be treated as a type of transfer learning method by people sometimes. By this
definition, transfer learning can update the weights of the pre-trained model as well. As a result,
the difference between transfer learning and fine-tuning becomes more blurred, and transfer learning
and fine-tuning may be used interchangeably. I personally prefer to distinguish them a bit so that
it can be clearer to readers how the training was actually done.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>One may also think about toughening the mock exams more than the actual test. This approach
ensures that achieving high performance in the mock exams translates to good or even better
performance in the real test. But here, consistent performance in mock exams and real test is
emphasized. Thus similarity between mocks and real test are desired.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>$\eta$ and $\lambda$ in <a href="#eq3">eq3</a> are two learning rates that update different modules in the network.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Interpolation is the process of estimating values within the range of known data points. In
the context of machine learning, it refers to predicting or estimating values for data points
that fall within the observed range of the training data.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Extrapolation, on the other hand, involves predicting values for data points that extend
beyond the range of the observed data. It&rsquo;s an extension of the model&rsquo;s predictions beyond the
range of the training data.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jiajiexiao.github.io/tags/ai/ml/">AI/ML</a></li>
      <li><a href="https://jiajiexiao.github.io/tags/robustness/">Robustness</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/">
    <span class="title">« Prev</span>
    <br>
    <span>What a large p for small n</span>
  </a>
  <a class="next" href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">
    <span class="title">Next »</span>
    <br>
    <span>Toward Robust AI (1): Why Robustness Matters</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Toward Robust AI (2): How To Achieve Robust AI on x"
            href="https://x.com/intent/tweet/?text=Toward%20Robust%20AI%20%282%29%3a%20How%20To%20Achieve%20Robust%20AI&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-01-06_how_robust_ai%2f&amp;hashtags=AI%2fML%2cRobustness">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Toward Robust AI (2): How To Achieve Robust AI on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-01-06_how_robust_ai%2f&amp;title=Toward%20Robust%20AI%20%282%29%3a%20How%20To%20Achieve%20Robust%20AI&amp;summary=Toward%20Robust%20AI%20%282%29%3a%20How%20To%20Achieve%20Robust%20AI&amp;source=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-01-06_how_robust_ai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'jjxiao';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2024 <a href="https://jiajiexiao.github.io/">JX&#39;s log</a> | <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a> | </span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
