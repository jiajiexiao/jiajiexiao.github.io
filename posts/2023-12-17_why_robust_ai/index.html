<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Toward Robust AI (1): Why Robustness Matters | JX&#39;s log</title>
<meta name="keywords" content="AI/ML, Robustness">
<meta name="description" content="Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.
Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models.">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.afe408fbde9e6232e4bb41db6dc1a6f427226bd3dcc8f7bd3765525b7678f46b.css" integrity="sha256-r&#43;QI&#43;96eYjLku0HbbcGm9Ccia9PcyPe9N2VSW3Z49Gs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jiajiexiao.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jiajiexiao.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jiajiexiao.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jiajiexiao.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://jiajiexiao.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>




<script async src="https://www.googletagmanager.com/gtag/js?id=G-VT65G42LLD"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-VT65G42LLD', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Toward Robust AI (1): Why Robustness Matters" />
<meta property="og:description" content="Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.
Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-12-17T14:06:46-08:00" />
<meta property="article:modified_time" content="2023-12-17T14:06:46-08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Toward Robust AI (1): Why Robustness Matters"/>
<meta name="twitter:description" content="Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.
Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jiajiexiao.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Toward Robust AI (1): Why Robustness Matters",
      "item": "https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Toward Robust AI (1): Why Robustness Matters",
  "name": "Toward Robust AI (1): Why Robustness Matters",
  "description": "Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.\nYet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models.",
  "keywords": [
    "AI/ML", "Robustness"
  ],
  "articleBody": "Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.\nYet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models. They are trained to transform inputs into predictive outputs, which includes tasks like classification, regression, media generation, data clustering, and action planning. Despite the awe-inspiring results, the deployment of even the most sophisticated models reveals a fundamental fragility.\nThis fragility becomes apparent in terms of unexpected or unreliable predictions. For example, you may have experienced or heard that a chatbot spewing gibberish instead of useful information. This phenomenon is called hallucinations, where the model generates text that is irrelevant or nonsensical concerning the given inputs and desired outputs. Such hallucinations are arguably inevitable in auto-regressive large language models (LLMs).\nThe implications of this fragility are profound, particularly in risk-sensitive applications. Errors from the AI/ML systems can have severe consequences. In healthcare, a misdiagnosis by an AI-powered diagnostic tool or test can lead to severe impacts on patient health and quality of life. Similarly, in autonomous vehicles, a computer vision system’s failure to accurately detect objects can result in fatal accidents.\nWhile AI/ML models often demonstrate impressive performance across numerous benchmarks during model development phases, these real-world errors persist. Degradation in performance, along with unforeseen errors, remains a significant challenge. As AI/ML technologies become increasingly integrated into society, the need for robust performance becomes paramount. The tremendous potential of AI/ML must be harnessed responsibly to ensure these models function reliably in the complex and dynamic real-world environment.\nThe Data Diet: How AI/ML Models Learn To understand why AI/ML models can stumble, we need to slightly peek under the hood at how they learn. Think of it like training a personal chef: you provide them with recipes and feedback (labels or rewards), and they gradually figure out how to transform ingredients (inputs) into delicious dishes (outputs). With this analogy, we’ll see how major types of AI/ML models learn as below.\nSupervised Learning: The most common approach, where you give the model both features (like image pixels) and labels (like “cat” or “dog”). The training process is to update the model parameters in order to reduce the error between the predicted outputs and the groundtruth labels. It’s like handing a chef a recipe book with labeled ingredients. While this method offers clarity and precision, acquiring annotated datasets can be costly.\nReinforcement Learning: A trial-and-error approach where the model explores and learns from rewards and punishments. Supervised learning also applies here, as the feedback from the reinforcement learning environment serves as labels, guiding the model to adjust its policy or value-action function for optimal long-term planning. Imagine a chef experimenting with different ingredient combinations without following a recipe and adjusting based on your reactions. That may be challenging since it requires you to taste all experimental dishes and share feedbacks.\nUnsupervised Learning: Unlike supervised learning that finds the relationship between features and labels, unsupervised learning aims to extract inherent structures or patterns from unlabeled data. It’s like a chef intuitively discerning flavor profiles and accumulating, free from the constraints of explicit recipes or examination of ingredient labels. Unsupervised methods present their own set of challenges, as models must decipher complex data structures based on simply feature values.\nSelf-Supervised Learning: Cleverly design proxy tasks that help models learn without explicit labels, like masking parts of an image or sentence and asking the model to fill in the blanks. Alternatively, one can also train the model to assess if two augmented versions of an input originate from the same base in the latent projection space, which is also called contrastive learning. These are like challenging a chef to identify mystery ingredients or create dishes from a limited pantry, which trains the chef to understand relationships among ingredients, recipes and dishes. Afterwards, the chef can likely handle more abstract or more creative meal requests from you. The self-supervised learning method eliminates the need for labeled data by using the inherent structure of the data itself, enabling the model to learn a (compressed) representation that captures intrinsic patterns within the inputs. As a result, self-supervised learning becomes more and more popular than classic unsupervised learning these days.\nRegardless of learning methods, in the training process, your AI chef is constantly adjusting their internal recipe book (model parameters) to improve their culinary skills. In other words, across these learning paradigms, a central tenet emerges: based on a set of training data, models continually adapt and refine their configurations, aiming to optimize alignment between their predictions and desired outcomes. But just like any human chef, they can be misled by faulty ingredients or biased information. Compared to the data used for model development, any discrepancies or shifts (i.e. so-called dataset shift (Hein2022)) in the distribution of data encountered during deployment may degrade performance. Unfortunately, as describing more in the next section, such dataset mismatch is common that results in AI/ML model fragility. We’ll delve deeper into these challenges, exploring the implications of distributional shifts and charting pathways to bolster AI model resilience.\nCommon yet Tricky Out-Of-Distribution We’ve seen that AI/ML models are taught to align the model outputs to desired targets based on a specific set of training data (Fig. 1). This training paradigm helps the model find “optimal” parameter values, ensuring accurate alignment between predictions and targets. However, the effectiveness of AI/ML models hinges on the similarity between the test data and the training data. In essence, the more congruent the test data is with the training data, the more reliable the model’s performance tends to be. This effectiveness pattern is common in machine learning practice.\nFig 1. Illustration of AI/ML Model Learning Process. AI/ML models, represented as parameterized hypothesis functions Hθ, transform inputs X into outputs Hθ(X). Through iterative training and optimization, the parameters θ are adjusted to minimize the discrepancy L between the model’s outputs and the target values Y.\nTo explore this phenomenon further, let’s delve into the terminologies commonly employed in contemporary literature. The dataset used for training is referred to as the source domain, while the dataset used for testing is termed the target domain (Kouw2018). These datasets are typically categorized as either Independent and Identically Distributed (IID) or Out-of-Distribution (OOD) (Hein2022). It’s crucial to understand that the effectiveness pattern mentioned earlier, rooted in a fundamental principle of PAC learning 1, assumes optimal consistency between the test data and the training data (Mohri2018). This alignment of data distribution, often referred to as the IID assumption when deploying predictive models, is a benchmark for reliable model performance. However, real-world scenarios often deviate from this idealized setting, presenting challenges in model generalization for OOD data. For instance, an AI model trained exclusively on standard bacterial DNA might misclassify a novel bacterial species, mistaking it for a known variant, rather than acknowledging uncertainty (Ren2019). Likewise, image classifiers may falter when presented with objects in unfamiliar poses or contexts that deviate from their training data (like a cow on ice (Causality2024)).\nFormally, IID and OOD are commonly defined by assessing the equality between the joint probability distributions of features and labels in both the source and target domains:\n\\begin{equation} \\begin{aligned} \\text{IID:} \\quad P_\\text{S} (X, Y) = P_\\text{T} (X, Y) \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\text{OOD:} \\quad P_\\text{S} (X, Y) \\neq P_\\text{T} (X, Y) \\end{aligned} \\end{equation}\nThis joint distribution can be deconstructed into the conditional probability, denoted as P(Y|X), which encapsulates the relationship between inputs X and targets Y, and the marginal probability P(X), which focuses solely on inputs. OOD scenarios predominantly manifest in two distinct forms 2:\nCovariate Shift: The conditional probability holds fixed across domains but input marginal probabilities differ. This is probably the most prevalent type of for OOD. For example, training data may lack samples for a particular feature range observed at test-time and thus make it hard for the model to reliably infer unseen regimes. (see toy example in Fig2). Covariate shifts are often seen when there are some selection biases or there are batch effects on the data generation processes. Fig 2. Model Behavior under Covariate Shift. In the source domain, data points with x \u003e 2 are absent, whereas the target domain features numerous such instances. Consequently, the model’s performance is compromised for x \u003e 2 in the target domain.\nConcept Drift: The conditional probability between inputs and targets itself shifts across domains, even if input distributions look similar. Relationships learned during training fail to transfer (see toy example in Fig3). Concept drift can be seen when there are any changes in mechanistic changes in the data generation process that may be even harder to anticipate in advanced compared to covariate shifts. Fig 3. Model Behavior under Concept Drift. The relationship between x and y evolves across domains, rendering the previously learned model inadequate for the target domain.\nWhile the landscape of OOD encompasses various nuanced scenarios (e.g. both P(Y|X) and P(X) may vary across domains), these two categories cover most common situations. As illustrated in Figs 2 and 3, even basic examples of covariate shift and concept drift can pose challenges. From a mathematical standpoint, it’s established that IID ensures consistent performance across both source and target domains. However, achieving such consistency in an OOD context proves more challenging. In moe details, a hypothesis model ℎ’s empirical risk 3 in the target domain, denoted as \\( R_{\\text{T}}(h) \\) , can be estimated by the source domain loss ℓ weighted by the ratio between the joint distributions in the target and source domain as below:\n\\begin{equation} \\begin{align*} R_{\\text{T}}(h) \u0026\\equiv \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\ell(h(x), y) P_{\\text{T}}(x, y) dx \\\\\\ \u0026= \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\frac{\\ell(h(x), y) P_{\\text{T}}(x, y)}{P_{\\text{S}}(x, y)} P_{\\text{S}}(x, y) dx \\\\\\ \u0026= \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\ell(h(x), y) P_{\\text{S}}(x,y) \\frac{P_\\text{T}(x, y)}{P_\\text{S}(x, y)} dx \\\\\\ \u0026\\approx \\frac{1}{n} \\sum_{i=1, x_i \\in \\mathcal{X_{\\text{S}}}, y_i \\in Y_{\\text{S}}}^{n} \\ell(h(x_i), y_i) \\frac{P_\\text{T}(x_i, y_i)}{P_\\text{S}(x_i, y_i)}. \\end{align*} \\end{equation}\nAs demonstrated by the equations above, achieving equality between the estimated target risk $\\widehat{R}_{\\text{T}}(h)$ and the estimated source risk \\( \\widehat{R}_{\\text{S}}(h) \\) typically requires \\( P_{\\text{T}}(x, y) = P_{\\text{S}}(x, y) \\) unless \\( \\ell_{\\text{T}}(h(x), y) = \\ell_{\\text{S}}(h(x), y) = 0 \\) .\nIn practice, while OOD scenarios are common, our goal remains: to achieve accurate and robust performance irrespective of whether we’re dealing with IID or OOD data. That is the requirement of robust AI/ML regardless of the IID or OOD. Consequently, the pursuit of designing AI/ML models that are resilient to a variety of OOD scenarios is crucial to ensure robust and dependable performance.\nSummary In wrapping up, this post has elucidated the foundational aspects of constructing compelling AI/ML models and shed light on the potential hurdles they encounter, particularly when confronted with OOD data. Understanding these challenges underscores the pressing need for robust AI. Ensuring that our AI systems can handle diverse and unexpected scenarios isn’t just a technical challenge—it’s crucial for their real-world applicability and trustworthiness. As we look ahead, bolstering AI’s resilience will be paramount. Join me in the forthcoming blog post, where we will explore in-depth strategies to fortify AI against these uncertainties and pave the way for more dependable and resilient machine learning solutions.\nCitation If you find this post helpful and are interested in citing it in your write-up, you can cite it as\nXiao, Jiajie. (Dec 2023). Toward Robust AI Part (1): Why Robustness Matters. JX’s log. Available at: https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/.\nor add the following to your BibTeX file.\n@article{xiao2023whyrobustness, title = \"Toward Robust AI (1): Why Robustness Matters\", author = \"Xiao, Jiajie\", journal = \"JX's log\", year = \"2023\", month = \"Dec\", url = \"https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/\" } References Hein, M., Joaquin Quiñonero-candela, Sugiyama, M., Schwaighofer, A., \u0026 Lawrence, N. D. (Eds.). (2022). Dataset Shift in Machine Learning (Neural Information Processing). The MIT Press.\nMohri, M., Rostamizadeh, A. and Talwalkar, A. (2018) Foundations of Machine Learning. Cambridge, MA: The MIT Press. Chapter 2: The PAC Learning Framework, Available at: https://mitpress.ublish.com/ebook/foundations-of-machine-learning--2-preview/7093/9.\nKouw, W. M., \u0026 Loog, M. (2018). An introduction to domain adaptation and transfer learning. arXiv preprint arXiv:1812.11806.\nRen, J., Liu, P. J., Fertig, E., Snoek, J., Poplin, R., Depristo, M., … \u0026 Lakshminarayanan, B. (2019). Likelihood ratios for out-of-distribution detection. Advances in neural information processing systems, 32.\nCausality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from https://ff13.fastforwardlabs.com/#how-irm-works.\nPAC learning stands for Probable Approximately Correct (PAC) learning framework, which is a foundational concept in computational learning theory that provides guarantees on the generalization performance of a learner. ↩︎\nThe joint distribution of P(X, Y) can also be expressed in terms of P(X|Y) and P(Y). Thus, literature sometimes also mentions a third OOD scenario called label shift, meaning the P(Y) varies across domains while P(X|Y) stays stable. ↩︎\nEmpirical risk is a measure of the average loss incurred by a hypothesis model ℎ on a given dataset. In simpler terms, it quantifies how well a hypothesis fits the observed data. In a broader sense, the risk of a hypothesis ℎ is the expected loss it will incur when applied to new, unseen data, drawn from the underlying distribution. This is a measure of how well the hypothesis generalizes to new data. The empirical risk serves as an estimate or proxy for the true risk. When we train a model on a finite dataset, we compute its empirical risk to assess its performance on that dataset (Kouw2018). ↩︎\n",
  "wordCount" : "2259",
  "inLanguage": "en",
  "datePublished": "2023-12-17T14:06:46-08:00",
  "dateModified": "2023-12-17T14:06:46-08:00",
  "author":{
    "@type": "Person",
    "name": "Jiajie Xiao"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JX's log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jiajiexiao.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jiajiexiao.github.io/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jiajiexiao.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jiajiexiao.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://jiajiexiao.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Toward Robust AI (1): Why Robustness Matters
    </h1>
    <div class="post-meta"><span title='2023-12-17 14:06:46 -0800 PST'>2023-12-17</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#brilliant-aiml-models-remain-brittle" aria-label="Brilliant AI/ML Models Remain Brittle">Brilliant AI/ML Models Remain Brittle</a></li>
                    <li>
                        <a href="#the-data-diet-how-aiml-models-learn" aria-label="The Data Diet: How AI/ML Models Learn">The Data Diet: How AI/ML Models Learn</a></li>
                    <li>
                        <a href="#common-yet-tricky-out-of-distribution" aria-label="Common yet Tricky Out-Of-Distribution">Common yet Tricky Out-Of-Distribution</a></li>
                    <li>
                        <a href="#summary" aria-label="Summary">Summary</a></li>
                    <li>
                        <a href="#citation" aria-label="Citation">Citation</a></li>
                    <li>
                        <a href="#references" aria-label="References">References</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="brilliant-aiml-models-remain-brittle">Brilliant AI/ML Models Remain Brittle<a hidden class="anchor" aria-hidden="true" href="#brilliant-aiml-models-remain-brittle">#</a></h2>
<p>Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their
potential to emulate, and sometimes surpass, human capabilities across diverse domains such as
vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable
Diffusion has fueled optimism, with many speculating not <em>if</em>, but <em>when</em>, Artificial General
Intelligence (AGI) will emerge.</p>
<p>Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical
models. They are trained to transform inputs into predictive outputs, which includes tasks like
classification, regression, media generation, data clustering, and action planning. Despite the
awe-inspiring results, the deployment of even the most sophisticated models reveals a
fundamental fragility.</p>
<p>This fragility becomes apparent in terms of unexpected or unreliable predictions. For example, you
may have experienced or heard that a chatbot spewing gibberish instead of useful information. This
phenomenon is called hallucinations, where the model generates text that is irrelevant or
nonsensical concerning the given inputs and desired outputs. Such hallucinations are arguably
inevitable in auto-regressive large language models (LLMs).</p>
<p>The implications of this fragility are profound, particularly in risk-sensitive applications. Errors
from the AI/ML systems can have severe consequences. In healthcare, a misdiagnosis by an AI-powered
diagnostic tool or test can lead to severe impacts on patient health and quality of life. Similarly,
in autonomous vehicles, a computer vision system&rsquo;s failure to accurately detect objects can result
in fatal accidents.</p>
<p>While AI/ML models often demonstrate impressive performance across numerous benchmarks during model
development phases, these real-world errors persist. Degradation in performance, along with
unforeseen errors, remains a significant challenge. As AI/ML technologies become increasingly
integrated into society, the need for robust performance becomes paramount. The tremendous potential
of AI/ML must be harnessed responsibly to ensure these models function reliably in the complex and
dynamic real-world environment.</p>
<h2 id="the-data-diet-how-aiml-models-learn">The Data Diet: How AI/ML Models Learn<a hidden class="anchor" aria-hidden="true" href="#the-data-diet-how-aiml-models-learn">#</a></h2>
<p>To understand why AI/ML models can stumble, we need to slightly peek under the hood at how they
learn. Think of it like training a personal chef: you provide them with recipes and feedback (labels
or rewards), and they gradually figure out how to transform ingredients (inputs) into delicious
dishes (outputs). With this analogy, we&rsquo;ll see how major types of AI/ML models learn as below.</p>
<ul>
<li>
<p><em>Supervised Learning</em>: The most common approach, where you give the model both features (like
image pixels) and labels (like &ldquo;cat&rdquo; or &ldquo;dog&rdquo;). The training process is to update the model
parameters in order to reduce the error between the predicted outputs and the groundtruth labels.
It&rsquo;s like handing a chef a recipe book with labeled ingredients. While this method offers clarity
and precision, acquiring annotated datasets can be costly.</p>
</li>
<li>
<p><em>Reinforcement Learning</em>: A trial-and-error approach where the model explores and learns from
rewards and punishments. Supervised learning also applies here, as the feedback from the
reinforcement learning environment serves as labels, guiding the model to adjust its policy or
value-action function for optimal long-term planning. Imagine a chef experimenting with different
ingredient combinations without following a recipe and adjusting based on your reactions. That may
be challenging since it requires you to taste all experimental dishes and share feedbacks.</p>
</li>
<li>
<p><em>Unsupervised Learning</em>: Unlike supervised learning that finds the relationship between features
and labels, unsupervised learning aims to extract inherent structures or patterns from unlabeled
data. It&rsquo;s like a chef intuitively discerning flavor profiles and accumulating, free from the
constraints of explicit recipes or examination of ingredient labels.  Unsupervised methods present
their own set of challenges, as models must decipher complex data structures based on simply
feature values.</p>
</li>
<li>
<p><em>Self-Supervised Learning</em>: Cleverly design proxy tasks that help models learn without explicit
labels, like masking parts of an image or sentence and asking the model to fill in the blanks.
Alternatively, one can also train the model to assess if two augmented versions of an input
originate from the same base in the latent projection space, which is also called contrastive
learning. These are like challenging a chef to identify mystery ingredients or create dishes from
a limited pantry, which trains the chef to understand relationships among ingredients, recipes and
dishes. Afterwards, the chef can likely handle more abstract or more creative meal requests from
you. The self-supervised learning method eliminates the need for labeled data by using the inherent
structure of the data itself, enabling the model to learn a (compressed) representation that
captures intrinsic patterns within the inputs. As a result, self-supervised learning becomes more
and more popular than classic unsupervised learning these days.</p>
</li>
</ul>
<p>Regardless of learning methods, in the training process, your AI chef is constantly adjusting their
internal recipe book (model parameters) to improve their culinary skills. In other words, across
these learning paradigms, a central tenet emerges: based on a set of training data, models
continually adapt and refine their configurations, aiming to optimize alignment between their
predictions and desired outcomes. But just like any human chef, they can be misled by faulty
ingredients or biased information. Compared to the data used for model development, any
discrepancies or shifts (i.e. so-called <em>dataset shift</em> (<a href="#Hein2022">Hein2022</a>)) in the distribution
of data encountered during deployment may degrade performance. Unfortunately, as describing more in
the next section, such dataset mismatch is common that results in AI/ML model fragility. We&rsquo;ll delve
deeper into these challenges, exploring the implications of distributional shifts and charting
pathways to bolster AI model resilience.</p>
<h2 id="common-yet-tricky-out-of-distribution">Common yet Tricky Out-Of-Distribution<a hidden class="anchor" aria-hidden="true" href="#common-yet-tricky-out-of-distribution">#</a></h2>
<p>We&rsquo;ve seen that AI/ML models are taught to align the model outputs to desired targets based on a
specific set of training data (<a href="#fig1">Fig. 1</a>). This training paradigm helps the model find
&ldquo;optimal&rdquo; parameter values, ensuring accurate alignment between predictions and targets. However,
the effectiveness of AI/ML models hinges on the similarity between the test data and the training
data. In essence, the more congruent the test data is with the training data, the more reliable the
model&rsquo;s performance tends to be. This effectiveness pattern is common in machine learning practice.</p>
<figure id="fig1" 
     class="align-center ">
    <img loading="lazy" src="../../images/how_ml_learn.png#center"
         alt="Fig 1. Illustration of AI/ML Model Learning Process. AI/ML models, represented as parameterized hypothesis functions Hθ, transform inputs X into outputs Hθ(X). Through iterative training and optimization, the parameters θ are adjusted to minimize the discrepancy L between the model&amp;rsquo;s outputs and the target values Y." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 1. Illustration of AI/ML Model Learning Process.</strong> AI/ML models, represented as parameterized hypothesis functions H<sub><em>θ</em></sub>, transform inputs X into outputs H<sub><em>θ</em></sub>(<em>X</em>). Through iterative training and optimization, the parameters <em>θ</em> are adjusted to minimize the discrepancy <em>L</em> between the model&rsquo;s outputs and the target values <em>Y</em>.</p>
        </figcaption>
</figure>

<p>To explore this phenomenon further, let&rsquo;s delve into the terminologies commonly employed in
contemporary literature. The dataset used for training is referred to as the source domain, while
the dataset used for testing is termed the target domain (<a href="#Kouw2018">Kouw2018</a>). These datasets are
typically categorized as either Independent and Identically Distributed (<em>IID</em>) or
Out-of-Distribution (<em>OOD</em>) (<a href="#Hein2022">Hein2022</a>). It&rsquo;s crucial to understand that the effectiveness
pattern mentioned earlier, rooted in a fundamental principle of PAC learning <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, assumes optimal
consistency between the test data and the training data (<a href="#Mohri2018">Mohri2018</a>). This alignment of
data distribution, often referred to as the <em>IID</em> assumption when deploying predictive models, is a
benchmark for reliable model performance. However, real-world scenarios often deviate from this
idealized setting, presenting challenges in model generalization for <em>OOD</em> data. For instance, an AI
model trained exclusively on standard bacterial DNA might misclassify a novel bacterial species,
mistaking it for a known variant, rather than acknowledging uncertainty (<a href="#Ren2019">Ren2019</a>).
Likewise, image classifiers may falter when presented with objects in unfamiliar poses or contexts
that deviate from their training data (like a cow on ice (<a href="#Causality2024">Causality2024</a>)).</p>
<p>Formally, <em>IID</em> and <em>OOD</em> are commonly defined by assessing the equality between the joint
probability distributions of features and labels in both the source and target domains:</p>
<p>\begin{equation}
\begin{aligned}
\text{IID:} \quad P_\text{S} (X, Y) = P_\text{T} (X, Y)
\end{aligned}
\end{equation}</p>
<p>\begin{equation}
\begin{aligned}
\text{OOD:} \quad P_\text{S} (X, Y) \neq P_\text{T} (X, Y)
\end{aligned}
\end{equation}</p>
<p>This joint distribution can be deconstructed into the conditional
probability, denoted as P(Y|X), which encapsulates the relationship between inputs X and targets Y,
and the marginal probability P(X), which focuses solely on inputs. <em>OOD</em> scenarios predominantly
manifest in two distinct forms <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>:</p>
<ul>
<li><strong>Covariate Shift</strong>: The conditional probability holds fixed across domains but input marginal
probabilities differ. This is probably the most prevalent type of for <em>OOD</em>. For example,
training data may lack samples for a particular feature range observed at test-time and thus make
it hard for the model to reliably infer unseen regimes. (see toy example in <a href="#fig2">Fig2</a>).
Covariate shifts are often seen when there are some selection biases or there are batch effects
on the data generation processes.</li>
</ul>
<figure id="fig2" 
     class="align-center ">
    <img loading="lazy" src="../../images/covariate_shift.png#center"
         alt="Fig 2. Model Behavior under Covariate Shift. In the source domain, data points with x &amp;gt; 2 are absent, whereas the target domain features numerous such instances. Consequently, the model&amp;rsquo;s performance is compromised for x &amp;gt; 2 in the target domain." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 2. Model Behavior under Covariate Shift.</strong> In the source domain, data points with x &gt; 2 are absent, whereas the target domain features numerous such instances. Consequently, the model&rsquo;s performance is compromised for x &gt; 2 in the target domain.</p>
        </figcaption>
</figure>

<ul>
<li><strong>Concept Drift</strong>: The conditional probability between inputs and targets itself shifts across
domains, even if input distributions look similar. Relationships learned during training fail to
transfer (see toy example in <a href="#fig3">Fig3</a>). Concept drift can be seen when there are any changes
in mechanistic changes in the data generation process that may be even harder to anticipate in
advanced compared to covariate shifts.</li>
</ul>
<figure id="fig3" 
     class="align-center ">
    <img loading="lazy" src="../../images/concept_drift.png#center"
         alt="Fig 3. Model Behavior under Concept Drift. The relationship between x and y evolves across domains, rendering the previously learned model inadequate for the target domain." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 3. Model Behavior under Concept Drift.</strong> The relationship between x and y evolves across domains, rendering the previously learned model inadequate for the target domain.</p>
        </figcaption>
</figure>

<p>While the landscape of <em>OOD</em> encompasses various nuanced scenarios (e.g. both P(Y|X) and P(X) may
vary across domains), these two categories cover most common situations. As illustrated in Figs
<a href="#fig2">2</a> and <a href="#fig3">3</a>, even basic examples of covariate shift and concept drift can pose
challenges. From a mathematical standpoint, it&rsquo;s established that IID ensures consistent performance
across both source and target domains. However, achieving such consistency in an OOD context proves
more challenging. In moe details, a hypothesis model ℎ&rsquo;s empirical risk <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> in the target domain,
denoted as  \( R_{\text{T}}(h) \) , can be estimated by the
source domain loss ℓ weighted by the ratio between the joint distributions in the target and source
domain as below:</p>
<p>\begin{equation}
\begin{align*}
R_{\text{T}}(h) &amp;\equiv \sum_{y \in Y_{\text{T}}} \int_{\mathcal{X_{\text{T}}}} \ell(h(x), y) P_{\text{T}}(x, y) dx \\\
&amp;= \sum_{y \in Y_{\text{T}}} \int_{\mathcal{X_{\text{T}}}} \frac{\ell(h(x), y) P_{\text{T}}(x, y)}{P_{\text{S}}(x, y)} P_{\text{S}}(x, y) dx \\\
&amp;= \sum_{y \in Y_{\text{T}}} \int_{\mathcal{X_{\text{T}}}} \ell(h(x), y) P_{\text{S}}(x,y) \frac{P_\text{T}(x, y)}{P_\text{S}(x, y)} dx  \\\
&amp;\approx \frac{1}{n} \sum_{i=1, x_i \in \mathcal{X_{\text{S}}}, y_i \in Y_{\text{S}}}^{n} \ell(h(x_i), y_i) \frac{P_\text{T}(x_i, y_i)}{P_\text{S}(x_i, y_i)}.
\end{align*}
\end{equation}</p>
<p>As demonstrated by the equations above, achieving equality between the estimated target risk $\widehat{R}_{\text{T}}(h)$ and the estimated source risk  \( \widehat{R}_{\text{S}}(h) \)  typically requires  \( P_{\text{T}}(x, y) = P_{\text{S}}(x, y) \)  unless  \( \ell_{\text{T}}(h(x), y) = \ell_{\text{S}}(h(x), y) = 0 \) .</p>
<!-- For some reason, using $$ twice for widehat doesn't render properly. -->
<p>In practice, while <em>OOD</em> scenarios are common, our goal remains: to achieve accurate and robust
performance irrespective of whether we&rsquo;re dealing with <em>IID</em> or <em>OOD</em> data. That is the requirement
of robust AI/ML regardless of the <em>IID</em> or <em>OOD</em>. Consequently, the pursuit of designing AI/ML
models that are resilient to a variety of <em>OOD</em> scenarios is crucial to ensure robust and dependable
performance.</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>In wrapping up, this post has elucidated the foundational aspects of constructing compelling AI/ML
models and shed light on the potential hurdles they encounter, particularly when confronted with
<em>OOD</em> data. Understanding these challenges underscores the pressing need for robust AI. Ensuring
that our AI systems can handle diverse and unexpected scenarios isn&rsquo;t just a technical
challenge—it&rsquo;s crucial for their real-world applicability and trustworthiness. As we look ahead,
bolstering AI&rsquo;s resilience will be paramount. Join me in the forthcoming blog post, where we will
explore in-depth strategies to fortify AI against these uncertainties and pave the way for more
dependable and resilient machine learning solutions.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>If you find this post helpful and are interested in citing it in your write-up, you can cite it as</p>
<blockquote>
<p>Xiao, Jiajie. (Dec 2023). <em>Toward Robust AI Part (1): Why Robustness Matters</em>. JX&rsquo;s log. Available
at: <a href="https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/">https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</a>.</p>
</blockquote>
<p>or add the following to your BibTeX file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bib" data-lang="bib"><span style="display:flex;"><span><span style="color:#a6e22e">@article</span>{xiao2023whyrobustness,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">title</span>   = <span style="color:#e6db74">&#34;Toward Robust AI (1): Why Robustness Matters&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">author</span>  = <span style="color:#e6db74">&#34;Xiao, Jiajie&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">journal</span> = <span style="color:#e6db74">&#34;JX&#39;s log&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">year</span>    = <span style="color:#e6db74">&#34;2023&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">month</span>   = <span style="color:#e6db74">&#34;Dec&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">url</span>     = <span style="color:#e6db74">&#34;https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li>
<p><a id="Hein2022"></a> Hein, M., Joaquin Quiñonero-candela, Sugiyama, M., Schwaighofer, A., &amp; Lawrence, N. D. (Eds.). (2022). Dataset Shift in Machine Learning (Neural Information Processing). The MIT Press.</p>
</li>
<li>
<p><a id="Mohri2018"></a> Mohri, M., Rostamizadeh, A. and Talwalkar, A. (2018) Foundations of Machine Learning. Cambridge,
MA: The MIT Press. Chapter 2: The PAC Learning Framework, Available at:
<a href="https://mitpress.ublish.com/ebook/foundations-of-machine-learning--2-preview/7093/9">https://mitpress.ublish.com/ebook/foundations-of-machine-learning--2-preview/7093/9</a>.</p>
</li>
<li>
<p><a id="Kouw2018"></a>  Kouw, W. M., &amp; Loog, M. (2018). An introduction to domain adaptation and transfer learning. arXiv preprint arXiv:1812.11806.</p>
</li>
<li>
<p><a id="Ren2019"></a>  Ren, J., Liu, P. J., Fertig, E., Snoek, J., Poplin, R., Depristo, M., &hellip; &amp; Lakshminarayanan, B. (2019). Likelihood ratios for out-of-distribution detection. Advances in neural information processing systems, 32.</p>
</li>
<li>
<p><a id="Causality2024"></a> Causality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from <a href="https://ff13.fastforwardlabs.com/#how-irm-works">https://ff13.fastforwardlabs.com/#how-irm-works</a>.</p>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>PAC learning stands for Probable Approximately Correct (PAC) learning framework, which is a
foundational concept in computational learning theory that provides guarantees on the generalization
performance of a learner.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The joint distribution of P(X, Y) can also be expressed in terms of P(X|Y) and P(Y). Thus,
literature sometimes also mentions a third <em>OOD</em> scenario called label shift, meaning the P(Y)
varies across domains while P(X|Y) stays stable.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Empirical risk is a measure of the average loss incurred by a hypothesis model ℎ on a given
dataset. In simpler terms, it quantifies how well a hypothesis fits the observed data. In a broader
sense, the risk of a hypothesis ℎ is the expected loss it will incur when applied to new, unseen
data, drawn from the underlying distribution. This is a measure of how well the hypothesis
generalizes to new data. The empirical risk serves as an estimate or proxy for the true risk. When
we train a model on a finite dataset, we compute its empirical risk to assess its performance on
that dataset (<a href="#Kouw2018">Kouw2018</a>).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jiajiexiao.github.io/tags/ai/ml/">AI/ML</a></li>
      <li><a href="https://jiajiexiao.github.io/tags/robustness/">Robustness</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/">
    <span class="title">« Prev</span>
    <br>
    <span>Toward Robust AI (2): How To Achieve Robust AI</span>
  </a>
  <a class="next" href="https://jiajiexiao.github.io/posts/2023-12-03_hello_world/">
    <span class="title">Next »</span>
    <br>
    <span>Hello World</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Toward Robust AI (1): Why Robustness Matters on x"
            href="https://x.com/intent/tweet/?text=Toward%20Robust%20AI%20%281%29%3a%20Why%20Robustness%20Matters&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2023-12-17_why_robust_ai%2f&amp;hashtags=AI%2fML%2cRobustness">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Toward Robust AI (1): Why Robustness Matters on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2023-12-17_why_robust_ai%2f&amp;title=Toward%20Robust%20AI%20%281%29%3a%20Why%20Robustness%20Matters&amp;summary=Toward%20Robust%20AI%20%281%29%3a%20Why%20Robustness%20Matters&amp;source=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2023-12-17_why_robust_ai%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'jjxiao';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2024 <a href="https://jiajiexiao.github.io/">JX&#39;s log</a></span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    <p xmlns:cc="http://creativecommons.org/ns#" > 
        Posts on this site are licensed under 
        <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a>
    </p>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
