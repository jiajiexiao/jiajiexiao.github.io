<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on JX&#39;s log</title>
    <link>https://jiajiexiao.github.io/posts/</link>
    <description>Recent content in Posts on JX&#39;s log</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 May 2024 22:34:35 -0800</lastBuildDate>
    <atom:link href="https://jiajiexiao.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Biomedical LLMs (1): Intro</title>
      <link>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</link>
      <pubDate>Fri, 10 May 2024 22:34:35 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</guid>
      <description>The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical</description>
    </item>
    <item>
      <title>What a large p for small n</title>
      <link>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</link>
      <pubDate>Mon, 29 Apr 2024 08:36:29 -0700</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/</guid>
      <description>&amp;ldquo;Large p small n&amp;rdquo; describes a scenario where the number of features ($p$) is much greater than the number of observations ($n$) for model training. While it is not a new problem, it continues to pose significant challenges in real-world applications of machine learning, especially for domains lacking rich data or fast and cheap data generation processes. In this blog post, I&amp;rsquo;ll document my recent thoughts on the &amp;ldquo;large p small n&amp;rdquo; problem.</description>
    </item>
    <item>
      <title>Toward Robust AI (2): How To Achieve Robust AI</title>
      <link>https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/</link>
      <pubDate>Sat, 06 Jan 2024 20:44:25 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/</guid>
      <description>In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain &amp;ldquo;intelligence&amp;rdquo; through a careful &amp;ldquo;data diet.&amp;rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment.</description>
    </item>
    <item>
      <title>Toward Robust AI (1): Why Robustness Matters</title>
      <link>https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</link>
      <pubDate>Sun, 17 Dec 2023 14:06:46 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/</guid>
      <description>Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.
Yet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models.</description>
    </item>
    <item>
      <title>Hello World</title>
      <link>https://jiajiexiao.github.io/posts/2023-12-03_hello_world/</link>
      <pubDate>Sun, 03 Dec 2023 11:50:43 -0800</pubDate>
      <guid>https://jiajiexiao.github.io/posts/2023-12-03_hello_world/</guid>
      <description>Greetings! This is JJ, and I am thrilled to welcome you to my corner of the internet! Taking inspiration from Lilian Weng, whose blog has been an invaluable resource during my studies and work in AI/ML, I&amp;rsquo;ve decided to also share my learning notes, thoughts, and updates through here.
Why Blogging? While I used to constantly write some diary during my childhood, I have to admit that I haven&amp;rsquo;t done so for quite a while.</description>
    </item>
  </channel>
</rss>
