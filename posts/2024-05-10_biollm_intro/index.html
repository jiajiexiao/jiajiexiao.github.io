<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Biomedical LLMs (1): Intro | JX&#39;s log</title>
<meta name="keywords" content="AI/ML, LLMs, Biomedical">
<meta name="description" content="The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.afe408fbde9e6232e4bb41db6dc1a6f427226bd3dcc8f7bd3765525b7678f46b.css" integrity="sha256-r&#43;QI&#43;96eYjLku0HbbcGm9Ccia9PcyPe9N2VSW3Z49Gs=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://jiajiexiao.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://jiajiexiao.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://jiajiexiao.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://jiajiexiao.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://jiajiexiao.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>




<script async src="https://www.googletagmanager.com/gtag/js?id=G-VT65G42LLD"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-VT65G42LLD', { 'anonymize_ip': false });
}
</script>
<meta property="og:title" content="Biomedical LLMs (1): Intro" />
<meta property="og:description" content="The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-10T22:34:35-08:00" />
<meta property="article:modified_time" content="2024-05-10T22:34:35-08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Biomedical LLMs (1): Intro"/>
<meta name="twitter:description" content="The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://jiajiexiao.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Biomedical LLMs (1): Intro",
      "item": "https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Biomedical LLMs (1): Intro",
  "name": "Biomedical LLMs (1): Intro",
  "description": "The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical",
  "keywords": [
    "AI/ML", "LLMs", "Biomedical"
  ],
  "articleBody": "The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising results, highlighting their potential to address complex problems where data scarcity is a significant barrier. Starting from this post, I am planning to write a series on Biomedical Large Language Models (LLMs).\n1. General Introduction to LLMs LLMs are sophisticated deep learning models designed to understand and generate human language. They leverage vast datasets to learn the statistical properties of language, allowing them to generate coherent and contextually appropriate text. The development of LLMs has been significantly influenced by the introduction of the Transformer architecture (Fig. 1) by Vaswani et al. (2017), which enabled models to efficiently (relatively speaking) capture long-range dependencies in text through self-attention mechanisms. Subsequent models such as BERT (Bidirectional Encoder Representations from Transformers) by Devlin et al. (2019) and GPT (Generative Pre-trained Transformer) by Radford et al. (2018) have set new benchmarks in NLP, demonstrating state-of-the-art performance across various tasks.\nFig 1. Transformer Architecture.. The nice illustration is from D2l.ai.\n2. Training Large Language Models The training of LLMs involves multiple methodologies designed to enhance their language understanding and generation capabilities. The primary training schemes include:\n2.1 Autoregressive Language Modeling (ALM) Autoregressive language modeling involves training the model to predict the next word in a sequence based on the preceding words:\n\\begin{equation} \\begin{aligned} \\pi_\\theta (\\mathbf{y} \\mid \\mathbf{x}) = \\prod_{t} \\pi_\\theta (y_t \\mid \\mathbf{x}, \\mathbf{y}_{",
  "wordCount" : "3917",
  "inLanguage": "en",
  "datePublished": "2024-05-10T22:34:35-08:00",
  "dateModified": "2024-05-10T22:34:35-08:00",
  "author":{
    "@type": "Person",
    "name": "Jiajie Xiao"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "JX's log",
    "logo": {
      "@type": "ImageObject",
      "url": "https://jiajiexiao.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://jiajiexiao.github.io/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://jiajiexiao.github.io/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://jiajiexiao.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://jiajiexiao.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://jiajiexiao.github.io/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Biomedical LLMs (1): Intro
    </h1>
    <div class="post-meta"><span title='2024-05-10 22:34:35 -0800 -0800'>2024-05-10</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#1-general-introduction-to-llms" aria-label="1. General Introduction to LLMs">1. General Introduction to LLMs</a></li>
                    <li>
                        <a href="#2-training-large-language-models" aria-label="2. Training Large Language Models">2. Training Large Language Models</a><ul>
                            
                    <li>
                        <a href="#21-autoregressive-language-modeling-alm" aria-label="2.1 Autoregressive Language Modeling (ALM)">2.1 Autoregressive Language Modeling (ALM)</a></li>
                    <li>
                        <a href="#22-masked-language-modeling-mlm" aria-label="2.2 Masked Language Modeling (MLM)">2.2 Masked Language Modeling (MLM)</a></li>
                    <li>
                        <a href="#23-other-training-schemes" aria-label="2.3 Other Training Schemes">2.3 Other Training Schemes</a></li></ul>
                    </li>
                    <li>
                        <a href="#3-typical-llm-frameworks" aria-label="3. Typical LLM Frameworks">3. Typical LLM Frameworks</a><ul>
                            
                    <li>
                        <a href="#31-encoder-decoder-architectures" aria-label="3.1 Encoder-Decoder Architectures">3.1 Encoder-Decoder Architectures</a></li>
                    <li>
                        <a href="#32-encoder-only-architectures" aria-label="3.2 Encoder-Only Architectures">3.2 Encoder-Only Architectures</a></li>
                    <li>
                        <a href="#33-decoder-only-architectures" aria-label="3.3 Decoder-Only Architectures">3.3 Decoder-Only Architectures</a></li>
                    <li>
                        <a href="#34-other-structures" aria-label="3.4 Other Structures">3.4 Other Structures</a></li></ul>
                    </li>
                    <li>
                        <a href="#4-employing-pre-trained-llms" aria-label="4. Employing Pre-Trained LLMs">4. Employing Pre-Trained LLMs</a><ul>
                            
                    <li>
                        <a href="#41-prompting" aria-label="4.1 Prompting">4.1 Prompting</a></li>
                    <li>
                        <a href="#42-instruction-learning" aria-label="4.2 Instruction Learning">4.2 Instruction Learning</a></li>
                    <li>
                        <a href="#43-in-context-learning" aria-label="4.3 In-Context Learning">4.3 In-Context Learning</a></li>
                    <li>
                        <a href="#44-vocabulary-extension" aria-label="4.4 Vocabulary Extension">4.4 Vocabulary Extension</a></li>
                    <li>
                        <a href="#45-supervised-fine-tuning" aria-label="4.5 Supervised Fine-Tuning">4.5 Supervised Fine-Tuning</a></li>
                    <li>
                        <a href="#46-alignment-via-rfhf" aria-label="4.6 Alignment via RFHF">4.6 Alignment via RFHF</a><ul>
                            
                    <li>
                        <a href="#461-ppo" aria-label="4.6.1 PPO">4.6.1 PPO</a></li>
                    <li>
                        <a href="#462-dpo" aria-label="4.6.2 DPO">4.6.2 DPO</a></li>
                    <li>
                        <a href="#463-sppo" aria-label="4.6.3 SPPO">4.6.3 SPPO</a></li></ul>
                    </li>
                    <li>
                        <a href="#47-efficient-fine-tuning" aria-label="4.7 Efficient Fine-Tuning">4.7 Efficient Fine-Tuning</a><ul>
                            
                    <li>
                        <a href="#471-low-rank-adaptation-lora" aria-label="4.7.1 Low-Rank Adaptation (LoRA)">4.7.1 Low-Rank Adaptation (LoRA)</a></li>
                    <li>
                        <a href="#472-adapter-modules" aria-label="4.7.2 Adapter Modules">4.7.2 Adapter Modules</a></li>
                    <li>
                        <a href="#473-prefix-tuning" aria-label="4.7.3 Prefix-Tuning">4.7.3 Prefix-Tuning</a></li>
                    <li>
                        <a href="#474-other-parameter-efficient-fine-tuning-techniques" aria-label="4.7.4 Other Parameter-Efficient Fine-Tuning Techniques">4.7.4 Other Parameter-Efficient Fine-Tuning Techniques</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#5-plans-for-this-series" aria-label="5. Plans for this Series">5. Plans for this Series</a></li>
                    <li>
                        <a href="#citation" aria-label="Citation">Citation</a><ul>
                            
                    <li>
                        <a href="#references" aria-label="References">References</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and
efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in
compressing vast amounts of information through unsupervised or self-supervised training, enabling
impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly
attractive for domains where generating extensive task-specific datasets is challenging, such as in
biomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising
results, highlighting their potential to address complex problems where data scarcity is a
significant barrier. Starting from this post, I am planning to write a series on Biomedical Large
Language Models (LLMs).</p>
<h2 id="1-general-introduction-to-llms">1. General Introduction to LLMs<a hidden class="anchor" aria-hidden="true" href="#1-general-introduction-to-llms">#</a></h2>
<p>LLMs are sophisticated deep learning models designed to understand and generate human language. They
leverage vast datasets to learn the <strong>statistical properties</strong> of language, allowing them to generate
coherent and contextually appropriate text. The development of LLMs has been significantly
influenced by the introduction of the Transformer architecture (<a href="#fig1">Fig. 1</a>) by <a href="#Vaswani2017">Vaswani et al.
(2017)</a>, which enabled models to efficiently (relatively speaking) capture long-range
dependencies in text through self-attention mechanisms. Subsequent models such as BERT
(Bidirectional Encoder Representations from Transformers) by <a href="#Devlin2018">Devlin et al. (2019)</a> and
GPT (Generative Pre-trained Transformer) by <a href="#Radford2018">Radford et al. (2018)</a> have set new
benchmarks in NLP, demonstrating state-of-the-art performance across various tasks.</p>
<figure id="fig1" 
     class="align-center ">
    <img loading="lazy" src="../../images/transformer.png#center"
         alt="Fig 1. Transformer Architecture.. The nice illustration is from D2l.ai." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 1. Transformer Architecture.</strong>. The nice illustration is from <a href="https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html">D2l.ai</a>.</p>
        </figcaption>
</figure>

<h2 id="2-training-large-language-models">2. Training Large Language Models<a hidden class="anchor" aria-hidden="true" href="#2-training-large-language-models">#</a></h2>
<p>The training of LLMs involves multiple methodologies designed to enhance their language
understanding and generation capabilities. The primary training schemes include:</p>
<h3 id="21-autoregressive-language-modeling-alm">2.1 Autoregressive Language Modeling (ALM)<a hidden class="anchor" aria-hidden="true" href="#21-autoregressive-language-modeling-alm">#</a></h3>
<p>Autoregressive language modeling involves training the model to predict the next word in a sequence
based on the preceding words:</p>
<p><a id="eq1"></a>
\begin{equation}
\begin{aligned}
\pi_\theta (\mathbf{y} \mid \mathbf{x}) = \prod_{t} \pi_\theta (y_t \mid \mathbf{x}, \mathbf{y}_{&lt;t}),
\end{aligned}
\end{equation}</p>
<p>where $y_t$ is the $t^{\text{th}}$ token in the response and $y_{&lt;t}$ is tokens in the response
before $y_t$, $x$ are optional inputs as conditional generation and $\pi_\theta$ is the policy
parameterized by $theta$ for the LLM model.</p>
<p>This approach, utilized by GPT models, enables the generation of text
in a sequential manner, ensuring coherence and contextual relevance (<a href="#Radford2018">Radford et al.
2018</a>). This approach is more and more popular these days. One of the reasons for this
trend is not just for the generative capabilities of LLMs, but also because of the belief/hypothesis
that the model must be capable understand the language of the world so it can generate coherent and
contextually appropriate text. This reminds me of a quote from Richard Feynman on his blackboard at
the time of death:</p>
<blockquote>
<p>&ldquo;What I cannot create, I do not understand. &quot;   &mdash; Richard P. Feynman</p>
</blockquote>
<h3 id="22-masked-language-modeling-mlm">2.2 Masked Language Modeling (MLM)<a hidden class="anchor" aria-hidden="true" href="#22-masked-language-modeling-mlm">#</a></h3>
<p>Masked language modeling, employed by models such as BERT, involves masking certain words in a
sentence and training the model to predict these masked words based on the surrounding context.</p>
<p><a id="eq2"></a>
\begin{equation}
\begin{aligned}
p(y_{\text{masked}} \mid y_{\text{unmasked}}) = g \circ f(y_{\text{unmasked}}),
\end{aligned}
\end{equation}</p>
<p>where $y$ are the tokens, $f$ represents the contextualized embeddings of the unmasked tokens and $g$ is linear probing followed by a softmax operation that returns the probabilities of masked tokens being particular tokens in the context, i.e. $p(y_{\text{masked}} \mid y_{\text{unmasked}})$.</p>
<p>This fill-in-the-blank <em>Cloze</em> task in a bidirectional training fashion allows the model to capture
contextualized representation of the inputs, leading to improved performance on a range of NLP tasks (<a href="#Devlin2018">Devlin et al., 2019</a>).</p>
<h3 id="23-other-training-schemes">2.3 Other Training Schemes<a hidden class="anchor" aria-hidden="true" href="#23-other-training-schemes">#</a></h3>
<p>It&rsquo;s worth noting that there are additional training tasks in BERT except for the masked language modeling task. BERT also has a pre-training task that involves <em>next sentence prediction</em> (NSP) task that is used to train the model to predict whether two segments are adjacent to each other in a document. This task aims to learn the relationships between segments for downstream tasks such as summarization and question answering that require reasoning about the relationships between pairs of sentences. However, there are other studies showing removing the NSP task can match or slightly improve the downstream tasks (<a href="#Liu2019">Liu, et al. 2019</a>).</p>
<p>Other notable training schemes include sequence-to-sequence learning, as implemented in models like
T5 (Text-to-Text Transfer Transformer) (<a href="#Raffel2020">Raffel et al., 2020</a>). This approach involves
training the model to convert one sequence of text into another. The multitask settings makes it
versatile for tasks such as translation, summarization, and question answering.</p>
<p>Moreover, as I plan to write about later, diffusion models have also been explored in LLMs (<a href="#Singh2023">Singh, et al. 2023</a>, <a href="#Wang2024">Wang, et al. 2024</a>). These models iteratively transform a simple noise distribution into a complex data distribution, effectively learning to reverse a diffusion process. This technique, although more common in image generation tasks, is being explored for text generation to enhance the diversity and quality of generated sequences.</p>
<h2 id="3-typical-llm-frameworks">3. Typical LLM Frameworks<a hidden class="anchor" aria-hidden="true" href="#3-typical-llm-frameworks">#</a></h2>
<p>LLMs are built using various architectural frameworks, each typically severing for different purposes. The most prevalent frameworks include:</p>
<h3 id="31-encoder-decoder-architectures">3.1 Encoder-Decoder Architectures<a hidden class="anchor" aria-hidden="true" href="#31-encoder-decoder-architectures">#</a></h3>
<p>The encoder-decoder framework, exemplified by models like T5 (<a href="#Raffel2020">Raffel et al., 2020</a>)
and BART (Bidirectional and Auto-Regressive Transformers) (<a href="#Lewis2019">Lewis et al., 2020</a>),
involves an encoder to process the input text and a decoder to generate the output text. This
architecture is particularly effective for tasks that require text transformation, such as
translation and summarization.</p>
<h3 id="32-encoder-only-architectures">3.2 Encoder-Only Architectures<a hidden class="anchor" aria-hidden="true" href="#32-encoder-only-architectures">#</a></h3>
<p>Encoder-only models, such as BERT, focus on understanding the input text. These models are optimized
for tasks like text classification, named entity recognition, and question answering, where
comprehending the input context is crucial (<a href="#Devlin2018">Devlin et al. (2019)</a>). Therefore, encoder-only models are typically used as representation learning for various downstream tasks.</p>
<h3 id="33-decoder-only-architectures">3.3 Decoder-Only Architectures<a hidden class="anchor" aria-hidden="true" href="#33-decoder-only-architectures">#</a></h3>
<p>Decoder-only models, such as GPT, are designed for text generation tasks. They excel at producing
coherent and contextually relevant text, making them suitable for applications in language modeling,
text completion, and creative writing (<a href="#Radford2018">Radford et al. 2018</a>).</p>
<h3 id="34-other-structures">3.4 Other Structures<a hidden class="anchor" aria-hidden="true" href="#34-other-structures">#</a></h3>
<p>Beyond these standard frameworks, ongoing research continues to explore new architectures and
training methodologies to enhance LLM capabilities. Hybrid models and multimodal models, which
integrate text with other data types (e.g., images, audio) (<a href="#Radford2021">Radford, et al. 2021</a>),
represent cutting-edge advancements in the field.</p>
<h2 id="4-employing-pre-trained-llms">4. Employing Pre-Trained LLMs<a hidden class="anchor" aria-hidden="true" href="#4-employing-pre-trained-llms">#</a></h2>
<p>Pre-trained LLMs serve as foundation models that can be employed for specific tasks using several techniques, starting with zero-shot approaches and moving towards more customized fine-tuning methods.</p>
<h3 id="41-prompting">4.1 Prompting<a hidden class="anchor" aria-hidden="true" href="#41-prompting">#</a></h3>
<p>Prompting involves crafting specific input prompts to guide the model&rsquo;s output. This technique leverages the model&rsquo;s pre-existing knowledge and can be used to elicit specific information or perform particular tasks. Effective prompting can significantly enhance the model&rsquo;s performance on a wide range of tasks without additional fine-tuning that updates the LLM itself. Prompting is particularly effective with autoregressive language models (ALMs) and models with a decoder architecture, such as GPT.</p>
<div style="background-color: #FFFFE0; padding: 10px; border-left: 6px solid #FFD700; border-radius: 5px; margin: 10px 0; color: #000000;">
  <strong>Example:</strong>
  <p>
    Consider a pre-trained language model being used in the context of prompting. The input sentence is:
  </p>
  <p>
    "Tell me a story about poison apple."
  </p>
  <p>
    With prompting, the model uses this input to generate a continuation based on its pre-trained knowledge, producing an output such as:
  </p>
  <p>
    "Once upon a time, in a faraway kingdom, there lived a beautiful princess who..."
  </p>
  <p>
    Here, the model utilizes its extensive training data to create coherent and contextually appropriate text without additional fine-tuning.
  </p>
</div>
<h3 id="42-instruction-learning">4.2 Instruction Learning<a hidden class="anchor" aria-hidden="true" href="#42-instruction-learning">#</a></h3>
<p>Instruction learning involves providing the model with explicit instructions within the input text to perform a specific task. This method allows the model to understand and execute complex tasks by interpreting the given instructions. Instruction learning is particularly useful for tasks requiring precise control over the model&rsquo;s behavior and is suitable for models with decoder components.</p>
<div style="background-color: #FFFFE0; padding: 10px; border-left: 6px solid #FFD700; border-radius: 5px; margin: 10px 0; color: #000000;">
  <strong>Example:</strong>
  <p>
    Consider a pre-trained language model being adapted through instruction learning. The instruction is:
  </p>
  <p>
    "Here is a Chinese sentence that is often used to say Hi. Translate it to English: '你吃了吗?'"
  </p>
  <p>
    With instruction learning, the model uses this directive to perform a specific task, generating the output:
  </p>
  <p>
    "Hello, how are you?"
  </p>
  <p>
    In this case, the model is fine-tuned on a dataset of translation pairs to follow explicit instructions for translating text between languages.
  </p>
</div>
<h3 id="43-in-context-learning">4.3 In-Context Learning<a hidden class="anchor" aria-hidden="true" href="#43-in-context-learning">#</a></h3>
<p>In-context learning allows the model to learn tasks by providing examples within the input context, without additional training. This method leverages the model&rsquo;s ability to infer patterns and relationships from the provided examples, enabling it to perform new tasks based on the contextual information alone. This approach works well with models that have a decoder component.</p>
<div style="background-color: #FFFFE0; padding: 10px; border-left: 6px solid #FFD700; border-radius: 5px; margin: 10px 0; color: #000000;">
  <strong>Example:</strong>
  <p>
    Consider a pre-trained language model using in-context learning. The model is given the following context:
  </p>
  <p>
    "Q: What is the capital of France? A: Paris. Q: What is the capital of Germany?"
  </p>
  <p>
    With in-context learning, the model uses this context to infer the pattern and generate the appropriate continuation:
  </p>
  <p>
    "A: Berlin."
  </p>
  <p>
    Here, the model leverages the provided examples to predict the answer to the new question based on the pattern observed in the context.
  </p>
</div>
<h3 id="44-vocabulary-extension">4.4 Vocabulary Extension<a hidden class="anchor" aria-hidden="true" href="#44-vocabulary-extension">#</a></h3>
<p>Vocabulary extension involves adding new tokens to the model&rsquo;s existing vocabulary to better handle
domain-specific terminology. This is crucial in fields like biomedicine, where specialized terms
frequently appear that were not part of the original corpus (i.e., out-of-vocabulary). The process
typically involves:</p>
<ol>
<li>Updating the Tokenizer: Include new tokens by training a new tokenizer on the domain-specific
corpus and merging its vocabulary with the existing one.</li>
<li>Modifying the Embedding Layer: Resize the embedding matrix to accommodate the new tokens and
initialize their embeddings, either randomly or using informed methods.</li>
<li>Fine-tuning the Model: Adjust the new embeddings and model parameters by fine-tuning the model on
the new domain corpus.</li>
<li>Evaluating the Model: Ensure the model effectively handles the new tokens by evaluating it on a
validation set from the new domain and making necessary adjustments.</li>
</ol>
<p>This process enables the LLM to understand and generate text in specialized domains effectively.</p>
<p>Apart from extending the vocabulary and fine-tuning a pre-trained LLM, other strategies like feature
fusion can handle unknown tokens in specific domains. Feature fusion involves adding additional
features to the embeddings of known tokens. This can include using pre-trained embeddings from
various sources or incorporating domain-specific information. For example, when working with LLMs
trained on DNA sequences, methylation status may be available in the target domain corpus but not in
the original training corpus. By combining these different types of features, the model&rsquo;s
performance is enhanced, especially in handling domain-specific terminology.</p>
<h3 id="45-supervised-fine-tuning">4.5 Supervised Fine-Tuning<a hidden class="anchor" aria-hidden="true" href="#45-supervised-fine-tuning">#</a></h3>
<p>Supervised fine-tuning (SFT) involves continuing the training of a pre-trained model using
domain-specific or task-specific data. This process adjusts the model parameters to better suit the
nuances of the new domain or new task. For example, a pre-trained LLM might be fine-tuned on a
dataset of clinical notes to improve its ability to understand and generate medical text,
significantly enhancing its performance on specialized tasks. SFT can be employed for both GPT-style
and BERT-style LLMs.</p>
<h3 id="46-alignment-via-rfhf">4.6 Alignment via RFHF<a hidden class="anchor" aria-hidden="true" href="#46-alignment-via-rfhf">#</a></h3>
<p>Alignment through reinforcement learning from human feedback (RFHF) aims to fine-tune the
unsupervised LMs to align the model&rsquo;s outputs with human values and preferences, ensuring that the
generated responses are accurate, ethical, and contextually appropriate (<a href="Ziegler2019">Ziegler, et al.
2019</a>, <a href="#Ouyang2022">Ouyang, et al. 2022</a>). In this approach, human feedback is used to
reward or penalize the model&rsquo;s predictions, guiding it to produce more desirable and reliable
outputs. This method is particularly useful for ensuring that LLMs generate responses that are not
only accurate but also ethically and contextually appropriate.</p>
<p>To obtain human feedback, additional data called comparison data that show different human rankings
on different model outputs given the same input. Such a comparison data will be used to build a
reward model as human evaluators. Human evaluators review the model&rsquo;s outputs and provide feedback,
which is used to adjust the model&rsquo;s parameters as fine-tuning via minimize the following loss:</p>
<p><a id="eq3"></a>
\begin{equation}
\begin{aligned}
\mathcal{L_r(\pi_t)} = -\mathbb{E}_{x \sim p_d, y \sim \pi_t} \left[ r(x, y) - \beta \log \frac{\pi_t(y \mid x)}{\pi_r(y \mid x)} \right],
\end{aligned}
\end{equation}</p>
<p>where $r$is the reward function reflecting human preferences in the comparison data$p_d$. $\pi_r$
is the original reference model used for regularizing $\pi_t$ with Kullback–Leibler divergence.
$\beta$ is the hyperparameter for controlling the regularization strength. The objective here is to
get large reward without deviating from the original reference LLM too much.</p>
<p>To optimize <a href="#eq3">eq 3</a> for human preference alignment, there are several approaches broadly categorized into reward-based methods like Proximal Policy Optimization (PPO) (<a href="#Ouyang2022">Ouyang, et al. 2022</a>) and reward-free methods like Direct Preference Optimization (DPO) (<a href="#Rafailov2024">Rafailov, et al. 2024</a>) and Self-Play Preference Optimization (SPPO) (<a href="#Wu2024">Wu, et al. 2024</a>).</p>
<h4 id="461-ppo">4.6.1 PPO<a hidden class="anchor" aria-hidden="true" href="#461-ppo">#</a></h4>
<p>After a reward model $r_\phi$ is constructed using the human-labeled comparison data, <a href="#eq3">eq 3</a>
can be explicitly optimized with online RL algorithms. PPO is a policy gradient method in RL
algorithm designed to stabilize training by using a clipped objective function (<a href="#Schulman2017">Schulman, et al.
2017</a>). Compared to other RL algorithms, the key idea for PPO is to update the policy
in small steps to prevent large deviations from the previous policy, which can lead to instability.</p>
<h4 id="462-dpo">4.6.2 DPO<a hidden class="anchor" aria-hidden="true" href="#462-dpo">#</a></h4>
<p>Direct Preference Optimization (DPO) simplifies RFHF by directly optimizing preference probabilities
without training a separate reward model (<a href="#Rafailov2024">Rafailov, et al. 2024</a>). It leverages the log-likelihood ratio of preferred
responses to non-preferred ones. The DPO loss function is:</p>
<p><a id="eq4"></a>
\begin{equation}
\begin{aligned}
\mathcal{L_{\text{DPO}}}(\pi_t; \pi_r) = -\mathbb{E}_{(x,y_w,y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \left( \frac{\pi_t(y_w | x)}{\pi_r(y_w | x)} \right) - \beta \log \left( \frac{\pi_t(y_l | x)}{\pi_r(y_l | x)} \right) \right) \right],
\end{aligned}
\end{equation}</p>
<p>where $y_w$ and $y_l$ are the preferred and less preferred responses given the input $x$,
respectively, $\sigma$ is the sigmoid function and $\beta$ is a scaling parameter. This method
directly targets the improvement of preference probabilities, which can lead to more stable and
efficient training compared to methods requiring a reward model.</p>
<p>DPO avoids the complexity and potential biases introduced by training a separate reward
model. It aligns the language model with human preferences through a straightforward
optimization process. However, DPO might struggle with data sparsity and non-transitive
preferences, which can limit its effectiveness in some scenarios.</p>
<h4 id="463-sppo">4.6.3 SPPO<a hidden class="anchor" aria-hidden="true" href="#463-sppo">#</a></h4>
<p>Self-Play Preference Optimization (SPPO) formulates the alignment problem as a two-player
constant-sum game, aiming to find the Nash equilibrium policy that consistently aligns with human
preferences (<a href="#Wu2024">Wu, et al. 2024</a>). SPPO uses a self-play mechanism to iteratively refine the
model by generating responses and evaluating them using a pre-trained preference model. The policy
update is given by:</p>
<p><a id="eq5"></a>
\begin{equation}
\begin{aligned}
\pi_{t+1}(y | x) \propto \pi_t(y | x) \exp \left( \eta P(y \succ \pi_t | x) \right),
\end{aligned}
\end{equation}</p>
<p>where $P(y \succ \pi_t | x) = \mathbb{E}_{y&rsquo; \sim \pi_t(\cdot | x)}[P(y \succ y&rsquo; | x)]$
is the winning probability.</p>
<p>The SPPO loss function ensures that the policy iteratively improves by fitting the
log-probabilities of the model’s responses to the empirical winning probabilities. This
approach effectively handles intransitive and irrational human preferences by directly
working with preference probabilities, which cannot be trivially achieved by symmetric
pairwise loss functions like DPO. SPPO was shown significant improvements
in alignment tasks, outperforming iterative DPO and other methods (e.g. Identity Preference
Optimization IPO) in various benchmarks without the need for external supervision.</p>
<h3 id="47-efficient-fine-tuning">4.7 Efficient Fine-Tuning<a hidden class="anchor" aria-hidden="true" href="#47-efficient-fine-tuning">#</a></h3>
<p>As the size of foundational LLMs (Large Language Models) today is typically quite large, fine-tuning
these models can be computationally challenging. Efficient fine-tuning techniques aim to reduce the
computational resources required for fine-tuning large models. This enables large language models to
be adapted to specific tasks with significantly reduced computational costs, making them more
accessible and practical for a broader range of applications.</p>
<p>A few common Parameter-Efficient Fine-Tuning (PEFT) methods are briefly described below.</p>
<figure id="fig2" 
     class="align-center ">
    <img loading="lazy" src="../../images/adapters.png#center"
         alt="Fig 2. Model architectures for different adaptation. (a) Prefix-Tuning, (b) LoRA,(c) Series Adapter, and (d) Parallel Adapter. The chart is the Fig. 1 from Hu, et al. 2023. Using this fig here for illustration purpose." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 2. Model architectures for different adaptation.</strong> (a) Prefix-Tuning, (b) LoRA,(c) Series Adapter, and (d) Parallel Adapter. The chart is the Fig. 1 from <a href="#Hu2023">Hu, et al. 2023</a>. Using this fig here for illustration purpose.</p>
        </figcaption>
</figure>

<h4 id="471-low-rank-adaptation-lora">4.7.1 Low-Rank Adaptation (LoRA)<a hidden class="anchor" aria-hidden="true" href="#471-low-rank-adaptation-lora">#</a></h4>
<p>Low-Rank Adaptation (LoRA) reduces the number of trainable parameters by learning low-rank updates
to the pre-trained weights. This method adapts only a small subset of the model&rsquo;s parameters,
allowing for quick and resource-efficient updates. LoRA is particularly useful in scenarios with
limited computational resources.</p>
<p>The key idea behind LoRA is to approximate the weight update matrix $\Delta W$ using two lower-rank
matrices $A$ and $B$:</p>
<p><a id="eq6"></a>
\begin{equation}
\begin{align*}
W        = W + \Delta W \\\
\Delta W = A \times B,
\end{align*}
\end{equation}</p>
<p>where $A \in \mathbb{R}^{d \times r}$ and $B \in \mathbb{R}^{r \times k}$, with $r$ being much
smaller than the original dimensions $d$ and $k$. This approximation reduces the number of
parameters from $d \times k$ to $d \times r + r \times k$.</p>
<figure id="fig2" 
     class="align-center ">
    <img loading="lazy" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/lora_animated.gif#center"
         alt="Fig 2. LoRA. Imagine a visual representation showing the original large matrix $W$ and its low-rank approximation through matrices $A$ and $B$. The GIF is from HuggingFace. I recommend checking the link for more information about hte adapters." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig 2. LoRA.</strong> Imagine a visual representation showing the original large matrix $W$ and its low-rank approximation through matrices $A$ and $B$. The GIF is from <a href="https://huggingface.co/docs/peft/conceptual_guides/adapter">HuggingFace</a>. I recommend checking the link for more information about hte adapters.</p>
        </figcaption>
</figure>

<h4 id="472-adapter-modules">4.7.2 Adapter Modules<a hidden class="anchor" aria-hidden="true" href="#472-adapter-modules">#</a></h4>
<p>Adapter modules involve adding small, trainable layers between the layers of a pre-trained model
<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>. These adapters can be fine-tuned for specific tasks without altering the original model
parameters. This approach significantly reduces the amount of training data and computational power
required compared to full fine-tuning.</p>
<p>Mathematically, if $h$ is the hidden state of a layer, an adapter module can be represented as:</p>
<p><a id="eq7"></a>
\begin{equation}
\begin{aligned}
h&rsquo; = h + f_{\text{adapter}}(h),
\end{aligned}
\end{equation}</p>
<p>where $f_{\text{adapter}}$ is a lightweight feed-forward network with a bottleneck architecture,
typically much smaller than the original layer. Please read <a href="#fig2">Fig 2 (c &amp; d)</a> for checking series abd parallel adapters, two common ways to place the adapters:</p>
<h4 id="473-prefix-tuning">4.7.3 Prefix-Tuning<a hidden class="anchor" aria-hidden="true" href="#473-prefix-tuning">#</a></h4>
<p>Prefix-tuning involves learning a small continuous vector, or &ldquo;prefix,&rdquo; that is prepended to the
input sequence (<a href="#fig2">Fig 2(a)</a>). This prefix is fine-tuned while the rest of the model&rsquo;s parameters remain fixed.
Prefix-tuning allows the model to adapt to new tasks efficiently, leveraging the pre-trained model&rsquo;s
capabilities with minimal additional computation.</p>
<p>Given an input sequence $x = [x_1, x_2, \ldots, x_n]$, prefix-tuning modifies it to:</p>
<p>$x&rsquo; = [p_1, p_2, \ldots, p_m, x_1, x_2, \ldots, x_n]$, where $[p_1, p_2, \ldots, p_m]$ are the
learned prefix vectors.</p>
<div style="background-color: #FFFFE0; padding: 10px; border-left: 6px solid #FFD700; border-radius: 5px; margin: 10px 0; color: #000000;">
  <strong>Example:</strong>
  <p>
    Consider a pre-trained language model being adapted to a specific task, such as
    sentiment analysis. The original input sentence is:
  </p>
  <p>
    "I had a great day at the park."
  </p>
  <p>
    With prefix-tuning, a learned prefix is prepended to this input: 
    <p>
    $[p_1, p_2, p_3, p_4, p_5]$ + "I had a great day at the park."
    </p>
  </p>
  <p>
    Here, $[p_1, p_2, p_3, p_4, p_5]$ are the learned prefix vectors. 
    During fine-tuning, only these prefix vectors are updated while the rest of the 
    model's parameters remain fixed. This allows the model to adapt to the
    sentiment analysis task using minimal additional computation.
  </p>
</div>
<h4 id="474-other-parameter-efficient-fine-tuning-techniques">4.7.4 Other Parameter-Efficient Fine-Tuning Techniques<a hidden class="anchor" aria-hidden="true" href="#474-other-parameter-efficient-fine-tuning-techniques">#</a></h4>
<p>Other PEFT techniques focus on modifying only a subset of the model&rsquo;s parameters or applying
quantization techniques to reduce computational overhead. These methods maintain model performance
while minimizing the resources needed for fine-tuning. Some examples are:</p>
<ul>
<li>
<p><strong>BitFit</strong> (<a href="#Zaken2021">Zaken, et al. 2021</a>): Only the bias terms of the model are fine-tuned.
Mathematically, for a weight matrix $W$ and bias $b$, only $b$ is updated:</p>
<p>$W&rsquo; = W$</p>
<p>$b&rsquo; = b + \Delta b$</p>
</li>
<li>
<p><strong>QAT (Quantization Aware Training)</strong> (<a href="#Liu2023">Liu, et al. 2023</a>): Quantization Aware Training
applies quantization during training, which reduces the precision of the model parameters, leading
to lower computational requirements.</p>
</li>
</ul>
<h2 id="5-plans-for-this-series">5. Plans for this Series<a hidden class="anchor" aria-hidden="true" href="#5-plans-for-this-series">#</a></h2>
<p>Subsequent posts in this series will delve into specific applications of LLMs in the biomedical
domain. The planned topics include:</p>
<ol>
<li><strong>Genomic/DNA Language Models</strong>: Covering general genomic applications, including DNA sequence
analysis, and introducing specialized models like Geneformer.</li>
<li><strong>Protein Language Models</strong>: Exploring the use of LLMs in understanding protein structures and
functions.</li>
<li><strong>RNA and Single-cell Language Models</strong>: Combining RNA language models with single-cell analysis
models like sc-GPT to explore their roles in gene expression and cellular heterogeneity.</li>
<li><strong>Chemistry Language Models</strong>: Highlighting the role of LLMs in chemical research, including drug
discovery and molecular analysis.</li>
<li><strong>Other Biomedical LLMs</strong>: Discussing models for medical imaging, generalized biological
understanding through multimodal data integration, and other emerging applications in the
biomedical field.</li>
</ol>
<p>While this intro post focuses on some general intro to LLMs, I look forward to more exploration of
how LLMs are transforming biomedical research and applications in the coming posts of this series.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>If you find this post helpful and are interested in referencing it in your write-up, you can cite it as</p>
<blockquote>
<p>Xiao, Jiajie. (May 2023). <em>Biomedical LLMs: Intro</em>. JX&rsquo;s log. Available
at: <a href="https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/">https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/</a>.</p>
</blockquote>
<p>or add the following to your BibTeX file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bib" data-lang="bib"><span style="display:flex;"><span><span style="color:#a6e22e">@article</span>{xiao2024_biollm_intro,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">title</span>   = <span style="color:#e6db74">&#34;Biomedical LLMs: Intro&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">author</span>  = <span style="color:#e6db74">&#34;Xiao, Jiajie&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">journal</span> = <span style="color:#e6db74">&#34;JX&#39;s log&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">year</span>    = <span style="color:#e6db74">&#34;2024&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">month</span>   = <span style="color:#e6db74">&#34;May&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">url</span>     = <span style="color:#e6db74">&#34;https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ul>
<li>
<p><a id="Vaswani2017"></a> Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &hellip; &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.</p>
</li>
<li>
<p><a id="Devlin2018"></a> Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
</li>
<li>
<p><a id="Radford2018"></a> Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <a href="https://openai.com/index/language-unsupervised/">Improving language understanding by generative pre-training</a>. <em>OpenAI</em>.</p>
</li>
<li>
<p><a id="Liu2019"></a> Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., &hellip; &amp; Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
</li>
<li>
<p><a id="Raffel2020"></a> Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., &hellip; &amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140), 1-67.</p>
</li>
<li>
<p><a id="Singh2023"></a> Mukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. CodeFusion: A Pre-trained Diffusion Model for Code Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11697–11708, Singapore. Association for Computational Linguistics.</p>
</li>
<li>
<p><a id="Wang2024"></a> Wang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., &amp; Gu, Q. (2024). Diffusion Language Models Are Versatile Protein Learners. arXiv preprint arXiv:2402.18567.</p>
</li>
<li>
<p><a id="Lewis2019"></a> Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., &hellip; &amp; Zettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.</p>
</li>
<li>
<p><a id="Radford2021"></a> Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., &hellip; &amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.</p>
</li>
<li>
<p><a id="Ziegler2019"></a> Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., &hellip; &amp; Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.</p>
</li>
<li>
<p><a id="Ouyang2022"></a> Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., &hellip; &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.</p>
</li>
<li>
<p><a id="Schulman2017"></a> Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
</li>
<li>
<p><a id="Rafailov2024"></a> Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., &amp; Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.</p>
</li>
<li>
<p><a id="Wu2024"></a> Wu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., &amp; Gu, Q. (2024). Self-Play Preference Optimization for Language Model Alignment. arXiv preprint arXiv:2405.00675.</p>
</li>
<li>
<p><a id="Hu2023"></a> Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E. P., Bing, L., &hellip; &amp; Lee, R. K. W. (2023). Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933</p>
</li>
<li>
<p><a id="Zaken2021"></a> Zaken, E. B., Ravfogel, S., &amp; Goldberg, Y. (2021). Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199.</p>
</li>
<li>
<p><a id="Liu2023"></a> Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., &hellip; &amp; Chandra, V. (2023). Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888.</p>
</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>LoRA is sometimes treated as a special case of an adapter module. In this post, I separated
them as two different approaches given the popularity of LoRA.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://jiajiexiao.github.io/tags/ai/ml/">AI/ML</a></li>
      <li><a href="https://jiajiexiao.github.io/tags/llms/">LLMs</a></li>
      <li><a href="https://jiajiexiao.github.io/tags/biomedical/">Biomedical</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/">
    <span class="title">Next »</span>
    <br>
    <span>What a large p for small n</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (1): Intro on x"
            href="https://x.com/intent/tweet/?text=Biomedical%20LLMs%20%281%29%3a%20Intro&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-05-10_biollm_intro%2f&amp;hashtags=AI%2fML%2cLLMs%2cBiomedical">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (1): Intro on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-05-10_biollm_intro%2f&amp;title=Biomedical%20LLMs%20%281%29%3a%20Intro&amp;summary=Biomedical%20LLMs%20%281%29%3a%20Intro&amp;source=https%3a%2f%2fjiajiexiao.github.io%2fposts%2f2024-05-10_biollm_intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
<div id="disqus_thread"></div>
    <script type="text/javascript">
    (function() {
      
      
      if (window.location.hostname === 'localhost') return;

      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      var disqus_shortname = 'jjxiao';
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2024 <a href="https://jiajiexiao.github.io/">JX&#39;s log</a> | <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a> | </span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
