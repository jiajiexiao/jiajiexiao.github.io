<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Biomedical LLMs (3): Protein | JX&#39;s log</title>
<meta name="keywords" content="">
<meta name="description" content="In the Biological LLMs series, we have so far visited the general intro to LLMs and LLMs for genomics. In this post, we will review protein LLMs.
Protein Language Models
ESMs
The ESM (Evolutionary Scale Modeling) models, developed by Meta AI, apply large language models (LLMs) to protein sequences, leveraging the transformer architecture to perform a variety of tasks related to protein structure and function. Here&rsquo;s an overview of the various ESM models:">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="http://localhost:1313/posts/2024-05-15_biollm_protein/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.c292a07dea08ffa7274e381a70305fb0723ab31bafbf10e470c03a04b23c11b6.css" integrity="sha256-wpKgfeoI/6cnTjgacDBfsHI6sxuvvxDkcMA6BLI8EbY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2024-05-15_biollm_protein/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Biomedical LLMs (3): Protein
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2024-02-17 22:34:35 -0800 PST'>2024-02-17</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#protein-language-models" aria-label="Protein Language Models">Protein Language Models</a><ul>
                            
                    <li>
                        <a href="#esms" aria-label="ESMs">ESMs</a></li>
                    <li>
                        <a href="#esm-aa" aria-label="ESM-AA">ESM-AA</a></li>
                    <li>
                        <a href="#progen-and-progen2" aria-label="ProGen and ProGen2">ProGen and ProGen2</a><ul>
                            
                    <li>
                        <a href="#progen-laying-the-groundwork" aria-label="ProGen: Laying the Groundwork">ProGen: Laying the Groundwork</a></li>
                    <li>
                        <a href="#progen2-scaling-up-for-superior-performance" aria-label="ProGen2: Scaling Up for Superior Performance">ProGen2: Scaling Up for Superior Performance</a></li>
                    <li>
                        <a href="#conditional-generation-and-fitness-prediction" aria-label="Conditional Generation and Fitness Prediction">Conditional Generation and Fitness Prediction</a></li></ul>
                    </li>
                    <li>
                        <a href="#protmpnn" aria-label="ProtMPNN">ProtMPNN</a></li>
                    <li>
                        <a href="#xtrimo" aria-label="xTrimo">xTrimo</a></li>
                    <li>
                        <a href="#clean" aria-label="CLEAN">CLEAN</a></li>
                    <li>
                        <a href="#dplm" aria-label="DPLM">DPLM</a></li>
                    <li>
                        <a href="#prollama" aria-label="ProLLaMA">ProLLaMA</a></li>
                    <li>
                        <a href="#prostt5" aria-label="ProstT5">ProstT5</a></li>
                    <li>
                        <a href="#other-plms" aria-label="Other pLMs">Other pLMs</a></li>
                    <li>
                        <a href="#lucaone" aria-label="LucaOne">LucaOne</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>In the Biological LLMs series, we have so far visited the <a href="https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/">general intro to LLMs</a> and <a href="">LLMs for genomics</a>. In this post, we will review protein LLMs.</p>
<h2 id="protein-language-models">Protein Language Models<a hidden class="anchor" aria-hidden="true" href="#protein-language-models">#</a></h2>
<h3 id="esms">ESMs<a hidden class="anchor" aria-hidden="true" href="#esms">#</a></h3>
<p>The ESM (Evolutionary Scale Modeling) models, developed by Meta AI, apply large language models (LLMs) to protein sequences, leveraging the transformer architecture to perform a variety of tasks related to protein structure and function. Here&rsquo;s an overview of the various ESM models:</p>
<table>
  <thead>
      <tr>
          <th>Shorthand</th>
          <th>Description</th>
          <th>Training Dataset</th>
          <th>Positional Embeddings</th>
          <th>Pretraining Method</th>
          <th>Architecture</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ESM-2</td>
          <td>SOTA general-purpose protein language model for predicting structure, function, and other properties.</td>
          <td>UR50 (sample UR90)</td>
          <td>Rotary Position Embedding (RoPE)</td>
          <td>Masked Language Modeling (MLM)</td>
          <td>6-48 transformer encoder layers, 8M~15B parameters</td>
      </tr>
      <tr>
          <td>ESMFold</td>
          <td>End-to-end single sequence 3D structure predictor.</td>
          <td>PDB + UR50</td>
          <td>Pairwise relative positional encoding as AlphaFold2</td>
          <td>MLM (for ESM-2)</td>
          <td>ESM-2 architecture + additional layers (called folding head) for 3D structure prediction (15B for best protein structure prediction)</td>
      </tr>
      <tr>
          <td>ESM-MSA-1b / MSA transformer</td>
          <td>MSA Transformer language model for extracting embeddings from MSA, enabling SOTA inference of structure.</td>
          <td>UR50 + MSA</td>
          <td>A learned MSA positional embedding + learned positional embedding</td>
          <td>MLM</td>
          <td>Modified Transformer for MSAs, 12 layers</td>
      </tr>
      <tr>
          <td>ESM-1v</td>
          <td>Specialized for prediction of variant effects, enabling SOTA zero-shot prediction of functional effects of sequence variations.</td>
          <td>UR90</td>
          <td>?</td>
          <td>MLM</td>
          <td>Same as ESM-1b, 33 layers, 650M parameters</td>
      </tr>
      <tr>
          <td>ESM-IF1</td>
          <td>Inverse folding model for designing sequences for given structures and predicting functional effects of sequence variation for given structures.</td>
          <td>CATH + UR50 + augmented by predicting structures for 12M protein sequences using AlphaFold2</td>
          <td>Learned positional embedding</td>
          <td>Autoregressive Language Modeling (ALM)</td>
          <td>Incorporates geometric vector perceptrons (GVPs), 16 layers, 142M parameters</td>
      </tr>
  </tbody>
</table>
<h3 id="esm-aa">ESM-AA<a hidden class="anchor" aria-hidden="true" href="#esm-aa">#</a></h3>
<p>ESM-AA (ESM All-Atom) is a recent protein language model (PLM) that enables atom-scale and residue-scale
unified molecular modeling. Traditional PLMs have predominantly operated at the residue scale <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>,
potentially limiting their utility for applications involving both proteins and small molecules. ESM-AA
addresses this limitation by pretraining on multi-scale code-switch protein sequences and leveraging
multi-scale position encoding to capture intricate relationships among residues and atoms.</p>
<p>ESM-AA employs a Transformer encoder architecture with multi-scale position encoding to handle both
residue and atom-level information. It consists of 12 stacked Transformer layers, each featuring 20
attention heads. The model has a dimension of 480 and a feedforward dimension of 1920, totaling
approximately 35 million parameters. This configuration allows ESM-AA to process sequences up to a
length of 2048 tokens, accommodating the additional length introduced by the unzipped residues.</p>
<p>To integrate atom-scale information, ESM-AA constructs multi-scale code-switch protein sequences by
randomly unzipping some residues into their constituent atoms. These sequences include both residue
and atomic coordinates, enhancing the model&rsquo;s structural knowledge. The positional encoding is split
into two components: residue scale position encoding (RSPE) $E^\text{R}$ and atom scale position encoding
$E^\text{A}$ using a spatial distance matrix. Rotary Position Embedding (RoPE) is adapted from ESM-2 to
handle residue sequences, while the spatial distance matrix encodes the 3D positions of atoms.</p>
<p>ESM-AA is pretrained using two primary tasks:</p>
<ol>
<li><strong>Multi-scale Masked Language Modeling (MLM):</strong> This task involves masking and predicting both
residues and atoms within the code-switch sequences, enabling the model to learn from diverse
structural contexts.</li>
<li><strong>Pair-wise Distance Recovery (PDR):</strong> In this task, the model learns to recover the accurate
Euclidean distances between corrupted atom coordinates, thus enhancing its understanding of
atomic structures.</li>
</ol>
<p>The pretraining dataset for ESM-AA includes both protein and small molecule data. The protein
dataset is sourced from AlphaFold DB, consisting of 8 million sequences with structural predictions,
while the molecular dataset includes 19 million molecules with 209 million conformations. ESM-AA was
trained on 16 NVIDIA A100 GPUs for three days, using Adam optimizer with a learning rate of 4e-4 and
a weight decay of 1e-2. The training involved 300,000 steps with a batch size of 256,000 tokens.</p>
<p>ESM-AA excels in protein-molecule interaction tasks, such as enzyme-substrate affinity regression
and drug-target affinity regression. It achieves state-of-the-art results on benchmarks by
effectively leveraging its multi-scale pretraining approach. Notably, ESM-AA outperforms models that
require separate pretraining for proteins and molecules, demonstrating the advantages of unified
molecular modeling.</p>
<p>ESM-AA offers several significant advantages that enhance its utility in biological research. Its
unified modeling capability allows it to handle both residue and atom scales effectively, making it
versatile for a wide range of biological applications, including protein engineering and drug
discovery. The innovative multi-scale pretraining and positional encoding strategies enable ESM-AA
to capture detailed structural relationships, improving its performance on complex tasks.
Additionally, ESM-AA&rsquo;s compatibility with existing ESM models ensures seamless integration with
prior advancements, facilitating easier adoption and further development in the field.</p>
<p>However, ESM-AA also has some limitations that need to be considered. The dual-scale pretraining
process is complex and requires sophisticated handling of positional encodings and data mixing,
which can be resource-intensive. Moreover, while ESM-AA efficiently manages partial residue
unzipping, modeling entire proteins at the atomic level remains computationally demanding, posing
challenges for scalability. Despite these limitations, ESM-AA represents a significant advancement
in protein language modeling, bridging the gap between residue and atom-scale information and
offering robust performance across multiple benchmarks.</p>
<p>In summary, ESM-AA represents a significant step forward in protein language modeling by bridging
the gap between residue and atom-scale information. Its innovative approach and robust performance
across multiple benchmarks highlight its potential for advancing research in protein engineering and
drug discovery.</p>
<p><a href="https://www.biorxiv.org/content/10.1101/2024.03.04.583284v2">https://www.biorxiv.org/content/10.1101/2024.03.04.583284v2</a></p>
<h3 id="progen-and-progen2">ProGen and ProGen2<a hidden class="anchor" aria-hidden="true" href="#progen-and-progen2">#</a></h3>
<p>ProGen and ProGen2 are pioneering examples of decoder-only protein language models (PLMs) trained
via Autoregressive Language Modeling (ALM), pushing the boundaries of protein generation and fitness
prediction. Developed by researchers from Salesforce Research, these models highlight the potential
of transformer architectures in synthetic biology, enabling the design of proteins with specific
properties through conditional generation.</p>
<h4 id="progen-laying-the-groundwork">ProGen: Laying the Groundwork<a hidden class="anchor" aria-hidden="true" href="#progen-laying-the-groundwork">#</a></h4>
<p>ProGen is a transformer-based language model with 1.2B parameters, trained on approximately
280 million protein sequences from diverse databases such as Uniparc, UniprotKB, SWISS-PROT, TrEMBL,
Pfam, and NCBI taxonomy. This extensive dataset provides ProGen with a broad evolutionary context,
which is crucial for generating realistic and functional protein sequences.</p>
<p>The model employs a standard amino acid vocabulary for tokenization and uses sinusoidal positional
embeddings to maintain sequence order. ProGen&rsquo;s autoregressive nature means it predicts the next
amino acid in a sequence based on the previous context, making it adept at generating coherent and
structurally viable protein sequences. The addition of conditioning tags—encoding information like
taxonomic data and molecular functions—enables ProGen to perform conditional generation, tailoring
protein outputs to desired specifications.</p>
<p>For example, if a user wants to generate a protein with a specific function such as &ldquo;oxidoreductase
activity,&rdquo; they can provide a sequence context along with a tag indicating this function. The model
uses this tag during generation to produce a sequence that aligns with the desired function. During
pre-training, tags included taxonomic classifications (e.g., species or genus), molecular functions
(e.g., enzyme activity), cellular components (e.g., membrane-bound), and biological processes (e.g.,
metabolic pathways).</p>
<p>Training ProGen involved substantial computational resources, using TensorFlow on Cloud TPU v3 Pods
over two weeks. The evaluation metrics included sequence similarity, secondary structure accuracy,
and conformational energy, ensuring that the generated proteins closely mimic natural ones in both
structure and function.</p>
<h4 id="progen2-scaling-up-for-superior-performance">ProGen2: Scaling Up for Superior Performance<a hidden class="anchor" aria-hidden="true" href="#progen2-scaling-up-for-superior-performance">#</a></h4>
<p>ProGen2 builds on the foundational principles of ProGen, scaling up both the model size and the
diversity of the training data. The largest ProGen2 model boasts 6.4B parameters, with other
versions at 151M, 764M, and 2.7B parameters. The training dataset encompasses over 1 billion protein
sequences from genomic, metagenomic, and immune repertoire databases, including Uniref90 and BFD30,
enhancing the model&rsquo;s ability to capture a wide spectrum of evolutionary diversity.</p>
<p>ProGen2 retains the autoregressive transformer framework but introduces innovations such as rotary
positional encodings and parallel processing of self-attention and feed-forward circuits. These
enhancements improve computational efficiency and model performance. ProGen2&rsquo;s training involved
extensive hyperparameter optimization, informed by research into balancing model size and
computational resources. ProGen2 was implemented using Jax, and training was
conducted on TPU v3 infrastructure over hundreds of thousands of steps.</p>
<h4 id="conditional-generation-and-fitness-prediction">Conditional Generation and Fitness Prediction<a hidden class="anchor" aria-hidden="true" href="#conditional-generation-and-fitness-prediction">#</a></h4>
<p>Both ProGen and ProGen2 excel in conditional generation, leveraging conditioning tags to generate
proteins with specific desired properties. For instance, to generate an antibody-binding protein,
the user can provide a context sequence along with a tag indicating &ldquo;antibody binding.&rdquo; The model
then produces a sequence that is more likely to have the desired binding properties.</p>
<p>An interesting aspect of ProGen2 is its ability to predict protein fitness through perplexity,
despite being a decoder-only model that can only sequentially decode with one-sided context rather
than bi-directional global receptive field. Traditionally, fitness prediction would be better to
rely on bi-directional encoder-only PLMs with extensive fine-tuning. However, ProGen2&rsquo;s
autoregressive nature and large-scale training enable it to assess the likelihood of sequences
effectively. Lower perplexity scores correlate with higher fitness, making ProGen2 a powerful tool
for evaluating protein viability without the need for additional fine-tuning.</p>
<p>Evaluation methods for both models include metrics such as sequence similarity, secondary structure
accuracy, and conformational energy. ProGen2 further leverages AlphaFold2 predictions and structural
and biophysical property comparisons, demonstrating superior performance in generating viable and
functional proteins. By using these advanced evaluation techniques, ProGen2 ensures that generated
proteins not only resemble natural sequences but also maintain their functional viability.</p>
<h3 id="protmpnn">ProtMPNN<a hidden class="anchor" aria-hidden="true" href="#protmpnn">#</a></h3>
<h3 id="xtrimo">xTrimo<a hidden class="anchor" aria-hidden="true" href="#xtrimo">#</a></h3>
<h3 id="clean">CLEAN<a hidden class="anchor" aria-hidden="true" href="#clean">#</a></h3>
<p><a href="https://github.com/tttianhao/CLEAN">https://github.com/tttianhao/CLEAN</a></p>
<h3 id="dplm">DPLM<a hidden class="anchor" aria-hidden="true" href="#dplm">#</a></h3>
<p>DPLM stands for a Diffusion Protein Language Model that is developed by ByteDance Research. DPLM leverages a discrete diffusion probabilistic framework to model evolutionary-scale protein sequences for generative and predictive capabilities.</p>
<p>Probably for comparison purpose, DPLM adopts the same network architecture as ESM2, which is a Transformer encoder using bidirectional multihead attention mechanism. The model comes in multiple versions, scaled to accommodate different sizes of data and computational resources. Specifically, DPLM is available in configurations with 150M, 650M, and up to 3B parameters. To manage exceptionally long protein sequences,  like what ESM2 model does, the extremely long input sequences are randomly truncated to a length of 1024 tokens.</p>
<p>Despite adopting an ESM2-like network architecture, DPLM utilizes a unique discrete diffusion process for sequence generation. This process involves a forward and reverse phase:</p>
<ol>
<li>
<p>Forward Process: In the forward diffusion process, noise is progressively added to the input protein sequence. This process is governed by a transition kernel that corrupts the sequence over multiple steps, transforming the original sequence into a fully noisy (masked) state.</p>
</li>
<li>
<p>Reverse Denoising Process: The reverse process aims to recover the original sequence by iteratively denoising the noisy sequence. At each denoising step, the model predicts a subset of masked tokens, gradually refining the sequence towards its original, clean state. The generative denoising process can be viewed as an iterative mask-predict approach. Initially, the sequence is fully masked, and at each step, the model predicts a portion of the masked tokens based on the current noisy sequence, re-masking the rest until the sequence is fully denoised. This iterative approach allows DPLM to capture complex dependencies within the sequence, resulting in high-quality protein generation.</p>
</li>
</ol>
<p>For structure-conditional generation, DPLM needs to integrate an additional structural adapter into its architecture. This adapter is typically inserted into the last layer of the network and works in conjunction with a pre-trained structure encoder such as the GVP-Transformer Encoder. This structural adapter allows DPLM to incorporate structural information into the generation process, enabling it to generate protein sequences that are consistent with given backbone structures.</p>
<p>Pre-training and Datasets
DPLM is pre-trained on the UniRef50 database, which comprises approximately 45 million protein sequences, totaling about 14 billion amino acid tokens. This extensive pre-training enables DPLM to learn the intricate patterns and relationships within protein sequences, providing a robust foundation for both generative and predictive tasks. The pre-training involves 100,000 updates, with batch sizes varying according to the model size, ranging from 320,000 tokens for the 150M model to 1 million tokens for the larger models.</p>
<p>Applications and Capabilities
DPLM excels in a variety of applications, from generating novel protein sequences to predicting protein functions and structures. The model supports unconditional generation, producing protein sequences that are novel, diverse, and structurally plausible. It also offers conditional generation capabilities, allowing for the generation of sequences based on partial sequences, structural constraints, or specific secondary structures. This versatility makes DPLM a valuable tool for protein engineering, therapeutic development, and fundamental biological research.</p>
<p>Training Infrastructure and Time
Training DPLM is a computationally intensive task, typically requiring high-performance infrastructure such as clusters of GPUs or TPUs. The extensive training dataset and the complexity of the model necessitate significant computational resources and time, with the exact duration varying depending on the specific configuration and the available hardware.</p>
<p>Advantages and Limitations
DPLM stands out for its ability to combine generative and predictive tasks in a single framework, outperforming traditional masked language models (e.g., ESM2) and autoregressive models in various benchmarks. However, its reliance on substantial computational resources for training and the need for extensive pre-training data are notable limitations. Additionally, while DPLM demonstrates superior performance in many areas, integrating explicit structural information could further enhance its capabilities, suggesting avenues for future research.</p>
<p>In summary, the Diffusion Protein Language Model (DPLM) offers a powerful and versatile approach to modeling protein sequences, leveraging the strengths of diffusion probabilistic frameworks and Transformer architectures. Its ability to perform both generative and predictive tasks makes it a cornerstone in the field of protein research, with potential applications spanning from fundamental biological studies to the development of new therapeutics.</p>
<h3 id="prollama">ProLLaMA<a hidden class="anchor" aria-hidden="true" href="#prollama">#</a></h3>
<p>ProLLaMA is another recent Protein Large Language Model (ProLLM) designed to handle multiple tasks
in Protein Language Processing (PLP). It uniquely leverages the architecture and pre-trained
strengths of LLaMA2, enabling it to incorporate robust natural language processing capabilities that
are absent in traditional ProLLMs. This integration allows ProLLaMA to perform tasks that extend
beyond the conventional boundaries of protein sequence generation.</p>
<p>ProLLaMA uses a transformer-based architecture, building upon the pre-trained LLaMA2 model. This
foundation is crucial as it allows ProLLaMA to inherit and further enhance the natural language
processing capabilities of LLaMA2 through specialized training. The model also incorporates Low-Rank
Adaptation (LoRA) in its multiple decoder layers to ensure efficient learning and scalability while
maintaining the integrity of the pre-trained parameters.</p>
<figure id="fig5" 
     class="align-center ">
    <img loading="lazy" src="../../images/ProLLaMA.jpg#center"
         alt="Fig . ProLLaMA. (a) Similarity between general LLM and ProLLaMA in their multitask ability. (b) Overall architecture of our ProLLaMA. (c) Overall training framework of ProLLaMA." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig . ProLLaMA.</strong> (a) Similarity between general LLM and ProLLaMA in their multitask ability. (b) Overall architecture of our ProLLaMA. (c) Overall training framework of ProLLaMA.</p>
        </figcaption>
</figure>

<p>The objective of training LLMs like ProLLaMA includes Causal Language Modeling, which focuses on
predicting the next token in a sequence given the previous tokens. This autoregressive approach is
vital for generating coherent and contextually relevant protein sequences. The training process of
ProLLaMA is divided into two stages:</p>
<ul>
<li>
<p>Continual Learning: ProLLaMA utilizes the pre-trained LLaMA2 to learn protein language. This stage
allows the model to acquire domain-specific knowledge from the UniRef50 dataset while retaining
the broad natural language capabilities of LLaMA2. This stage employs a high-rank LoRA
configuration to capture the complex dependencies within protein sequences effectively.</p>
</li>
<li>
<p>Instruction Tuning: The model is then trained on a multi-task dataset where each sample includes
an instruction and the corresponding output. This involves adjusting the model to process and
execute specific instructions within the PLP tasks, enhancing its applicability and performance in
practical scenarios. In other words, this stage enables the model to follow user instructions and
perform various PLP tasks, such as controllable protein generation and property prediction. The
instruction dataset includes protein sequences and property texts from InterPro, allowing the
model to learn both protein generation and property prediction.</p>
</li>
</ul>
<p>ProLLaMA uses a tokenization approach that incorporates specific prefixes and suffixes to
distinguish protein language from natural language. This ensures accurate context understanding
during the processing of protein sequences. ProLLaMA likely employs a direct tokenization approach
where each amino acid in a protein sequence is treated as an individual token. Positional embeddings
are likely added but it is not discussed in the manuscript. This method aligns with the biochemical
properties of proteins, where the sequence of amino acids determines their structure and function.
By tokenizing at the amino acid level, ProLLaMA can capture detailed information about the
sequence’s biochemical properties and the potential interactions between residues.</p>
<p>Training ProLLaMA used DeepSpeed ZeRO-2 for memory optimization. The continual learning phase
involves one epoch of training with a peak learning rate of 0.05 and a batch size of 4 per GPU. For
instruction tuning, the model undergoes two epochs with a batch size of 144 per GPU, ensuring
comprehensive coverage of multi-task capabilities.</p>
<p>ProLLaMA&rsquo;s versatility extends to multiple PLP tasks:</p>
<ul>
<li>Unconditional Protein Sequence Generation: Producing novel protein sequences with high structural
plausibility.</li>
<li>Controllable Protein Generation: Designing proteins with specific functionalities based on
user-provided instructions.</li>
<li>Protein Property Prediction: Achieving nearly 100% accuracy in predicting various protein
properties.</li>
</ul>
<p><a href="https://arxiv.org/abs/2402.16445">https://arxiv.org/abs/2402.16445</a></p>
<h3 id="prostt5">ProstT5<a hidden class="anchor" aria-hidden="true" href="#prostt5">#</a></h3>
<p>ProstT5 (Protein Structure-Sequence T5) is a bilingual language model designed to integrate
one-dimensional (1D) amino acid sequences with three-dimensional (3D) protein structures. Utilizing
the transformer-based architecture of the T5 model, ProstT5 is tailored specifically for biological
sequences, making it a powerful tool in protein modeling. The model encompasses 3B parameters and
leverages the transformer’s encoder-decoder structure, which is adept at handling translation tasks
between different data modalities.</p>
<figure id="fig5" 
     class="align-center ">
    <img loading="lazy" src="../../images/ProstT5.jpeg#center"
         alt="Fig . Sketch of ProstT5. Source image from Fig.1 from ." width="auto"/> <figcaption style="text-align: center; width: 100%;"> 
            <p style="text-align: left;"> <strong>Fig . Sketch of ProstT5.</strong> Source image from Fig.1 from <a href=""></a>.</p>
        </figcaption>
</figure>

<p>The model takes in sequences of amino acids (AAs) and 3D structural representations (3Di) encoded as
1D sequences. The tokenization scheme introduced by ProstT5 involves converting 3D coordinates into
1D sequences using the 3Di-alphabet from Foldseek. This innovative tokenization allows the model to
process and translate between sequences and structures effectively. The 3Di tokens are integrated
into the model&rsquo;s vocabulary, ensuring no collision with existing amino acid tokens by casting them
in lowercase.</p>
<p>The training dataset for ProstT5 is meticulously curated from the AlphaFold Protein Structure
Database (AFDB). This dataset includes high-quality, non-redundant protein structures clustered to
minimize bias. The final training set comprises 17 million proteins, providing a robust foundation
for the model’s training. Training ProstT5 involves two main stages:</p>
<ol>
<li>Expanding the ProtT5’s vocabulary to include 3Di tokens</li>
<li>Fine-tuning on a dataset with paired 1D and 3Di sequences.</li>
</ol>
<p>This dual-modal training helps the model learn the intricate relationships between sequences and
structures. The training process utilized 8 Nvidia A100 GPUs over approximately 30 days, employing
techniques such as mixed precision training and gradient accumulation to optimize resource use.</p>
<p>ProstT5 excels in several tasks, including remote homology detection, inverse folding, and
structure-based protein design. By translating between amino acid sequences and 3D structures, the
model can generate new protein sequences that adopt desired structural scaffolds, a process crucial
for protein design. The authours&rsquo; Benchmarks show that ProstT5 performs competitively against
state-of-the-art methods in these tasks. For instance, in remote homology detection,
ProstT5-generated 3Di sequences closely matched the performance of experimentally determined
structures when used with Foldseek.</p>
<p>One of ProstT5&rsquo;s significant advantages is its ability to handle both sequences and structures
within a single model, providing a flexible and powerful tool for various protein modeling tasks.
The model&rsquo;s bilingual nature enables it to perform bi-directional translations, enhancing its
utility in protein design and prediction tasks. However, limitations include a class imbalance in
the 3Di tokens, which can affect performance, and a slight decrease in function-related predictions
such as subcellular location and binding residue prediction. Addressing these limitations through
improved data balancing and further fine-tuning could enhance the model’s capabilities.</p>
<h3 id="other-plms">Other pLMs<a hidden class="anchor" aria-hidden="true" href="#other-plms">#</a></h3>
<p>It is not superised that there are many other pLMs that are not coverred in detail by this writeup.
These include ProtTrans, ProtBert, ProtAlbert, ProtXLNet, ProtElectra, ProtT5, ProtTXL, etc. They
were either slightly older or less popular.</p>
<p><a href="https://ieeexplore.ieee.org/document/9477085">https://ieeexplore.ieee.org/document/9477085</a></p>
<h3 id="lucaone">LucaOne<a hidden class="anchor" aria-hidden="true" href="#lucaone">#</a></h3>
<p><a href="https://mp.weixin.qq.com/s/HNOjm1RBte2A9_kB_uk50w">https://mp.weixin.qq.com/s/HNOjm1RBte2A9_kB_uk50w</a></p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For example, each residue is a token for ESMs.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2024-05-15_biollm_chemistry/">
    <span class="title">« Prev</span>
    <br>
    <span>Biomedical LLMs (3): Chemistry</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2024-01-06_how_robust_ai/">
    <span class="title">Next »</span>
    <br>
    <span>Toward Robust AI (2): How To Achieve Robust AI</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (3): Protein on x"
            href="https://x.com/intent/tweet/?text=Biomedical%20LLMs%20%283%29%3a%20Protein&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_protein%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (3): Protein on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_protein%2f&amp;title=Biomedical%20LLMs%20%283%29%3a%20Protein&amp;summary=Biomedical%20LLMs%20%283%29%3a%20Protein&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_protein%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>

</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2025 <a href="http://localhost:1313/">JX&#39;s log</a> | <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a> | </span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
