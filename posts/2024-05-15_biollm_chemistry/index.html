<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Biomedical LLMs (3): Chemistry | JX&#39;s log</title>
<meta name="keywords" content="">
<meta name="description" content="BERT-like encoder-only model: DNABERT, ESM
ESM-AA (ESM-all atom), Prott5nv
GPT-like decoder-only model: ProGen
pLM
dnaLM
rnaLM
https://docs.nvidia.com/bionemo-framework/latest/models/prott5nv.html
https://www.biorxiv.org/content/10.1101/2024.03.04.583284v2
Protein Language Models
ESMs
ESM-AA
Prott5nv
ProtGen
ProLLaMA
ProLLaMA is an innovative Protein Large Language Model (ProLLM) designed to handle multiple tasks in Protein Language Processing (PLP). ProLLaMA builds upon general large language models (LLMs) like LLaMA2, integrating the unique properties of protein sequences with natural language capabilities. This model was developed to address the limitations of existing ProLLMs, which typically focus on single-task applications such as protein sequence generation.">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="http://localhost:1313/posts/2024-05-15_biollm_chemistry/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.c292a07dea08ffa7274e381a70305fb0723ab31bafbf10e470c03a04b23c11b6.css" integrity="sha256-wpKgfeoI/6cnTjgacDBfsHI6sxuvvxDkcMA6BLI8EbY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2024-05-15_biollm_chemistry/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Biomedical LLMs (3): Chemistry
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2024-02-17 22:34:35 -0800 PST'>2024-02-17</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#protein-language-models" aria-label="Protein Language Models">Protein Language Models</a><ul>
                            
                    <li>
                        <a href="#esms" aria-label="ESMs">ESMs</a></li>
                    <li>
                        <a href="#esm-aa" aria-label="ESM-AA">ESM-AA</a></li>
                    <li>
                        <a href="#prott5nv" aria-label="Prott5nv">Prott5nv</a></li>
                    <li>
                        <a href="#protgen" aria-label="ProtGen">ProtGen</a></li>
                    <li>
                        <a href="#prollama" aria-label="ProLLaMA">ProLLaMA</a></li></ul>
                    </li>
                    <li>
                        <a href="#dna-language-models" aria-label="DNA Language Models">DNA Language Models</a><ul>
                            
                    <li>
                        <a href="#dnabert" aria-label="DNABERT">DNABERT</a></li>
                    <li>
                        <a href="#dnabert-2" aria-label="DNABERT-2">DNABERT-2</a></li>
                    <li>
                        <a href="#nucleotide-transformer" aria-label="Nucleotide Transformer">Nucleotide Transformer</a></li>
                    <li>
                        <a href="#enformer" aria-label="Enformer">Enformer</a></li>
                    <li>
                        <a href="#gpn" aria-label="GPN">GPN</a></li>
                    <li>
                        <a href="#hyenadna" aria-label="HyenaDNA">HyenaDNA</a></li>
                    <li>
                        <a href="#evo" aria-label="Evo">Evo</a></li></ul>
                    </li>
                    <li>
                        <a href="#rna-language-models" aria-label="RNA Language Models">RNA Language Models</a><ul>
                            
                    <li>
                        <a href="#rinalmo" aria-label="RiNALMo">RiNALMo</a></li>
                    <li>
                        <a href="#rnaernie" aria-label="RNAErnie">RNAErnie</a></li>
                    <li>
                        <a href="#bigrna" aria-label="BigRNA">BigRNA</a></li></ul>
                    </li>
                    <li>
                        <a href="#chemistry-llm" aria-label="Chemistry LLM">Chemistry LLM</a></li>
                    <li>
                        <a href="#image-llm-owkin-pathai" aria-label="Image LLM Owkin, pathAI">Image LLM Owkin, pathAI</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><p>BERT-like encoder-only model: DNABERT, ESM
ESM-AA (ESM-all atom), Prott5nv</p>
<p>GPT-like decoder-only model: ProGen</p>
<p>pLM
dnaLM
rnaLM</p>
<p><a href="https://docs.nvidia.com/bionemo-framework/latest/models/prott5nv.html">https://docs.nvidia.com/bionemo-framework/latest/models/prott5nv.html</a></p>
<p><a href="https://www.biorxiv.org/content/10.1101/2024.03.04.583284v2">https://www.biorxiv.org/content/10.1101/2024.03.04.583284v2</a></p>
<h2 id="protein-language-models">Protein Language Models<a hidden class="anchor" aria-hidden="true" href="#protein-language-models">#</a></h2>
<h3 id="esms">ESMs<a hidden class="anchor" aria-hidden="true" href="#esms">#</a></h3>
<h3 id="esm-aa">ESM-AA<a hidden class="anchor" aria-hidden="true" href="#esm-aa">#</a></h3>
<h3 id="prott5nv">Prott5nv<a hidden class="anchor" aria-hidden="true" href="#prott5nv">#</a></h3>
<h3 id="protgen">ProtGen<a hidden class="anchor" aria-hidden="true" href="#protgen">#</a></h3>
<h3 id="prollama">ProLLaMA<a hidden class="anchor" aria-hidden="true" href="#prollama">#</a></h3>
<p>ProLLaMA is an innovative Protein Large Language Model (ProLLM) designed to handle multiple tasks in Protein Language Processing (PLP). ProLLaMA builds upon general large language models (LLMs) like LLaMA2, integrating the unique properties of protein sequences with natural language capabilities. This model was developed to address the limitations of existing ProLLMs, which typically focus on single-task applications such as protein sequence generation.</p>
<p>Architecture and Specifications</p>
<p>ProLLaMA adopts a transformer-based architecture, leveraging the LLaMA2 model&rsquo;s foundational design. It utilizes a two-stage training framework incorporating Low-Rank Adaptation (LoRA) to facilitate efficient learning and scalability. The model consists of multiple decoder layers with specific LoRA modules added to weights for efficient downstream fine-tuning. These modifications ensure that while the core parameters of LLaMA2 are preserved, new protein language capabilities are seamlessly integrated.</p>
<p>Training Details</p>
<p>The model was published in February 2024, marking a significant advancement in multi-task PLP. The training process of ProLLaMA is divided into two stages:</p>
<p>Continual Learning: Using a dataset derived from UniRef50, ProLLaMA learns protein language without losing its natural language understanding. This stage employs a high-rank LoRA configuration to capture the complex dependencies within protein sequences effectively.</p>
<p>Instruction Tuning: The model is then trained on a multi-task dataset where each sample includes an instruction and the corresponding output. This enables the model to follow user instructions and perform various PLP tasks, such as controllable protein generation and property prediction.</p>
<p>Tokenization and Positional Embedding</p>
<p>ProLLaMA uses a tokenization approach that incorporates specific prefixes and suffixes to distinguish protein language from natural language. This ensures accurate context understanding during the processing of protein sequences. Positional embeddings are included to maintain the sequential order of amino acids, crucial for understanding protein structures. ProLLaMA likely employs a direct tokenization approach where each amino acid in a protein sequence is treated as an individual token. This method aligns with the biochemical properties of proteins, where the sequence of amino acids determines their structure and function. By tokenizing at the amino acid level, ProLLaMA can capture detailed information about the sequence’s biochemical properties and the potential interactions between residues.</p>
<p>Training Infrastructure and Time</p>
<p>Training ProLLaMA involves substantial computational resources, facilitated by using DeepSpeed ZeRO-2 for memory optimization. The continual learning phase involves one epoch of training with a peak learning rate of 0.05 and a batch size of 4 per GPU. For instruction tuning, the model undergoes two epochs with a batch size of 144 per GPU, ensuring comprehensive coverage of multi-task capabilities.</p>
<p>Datasets</p>
<p>ProLLaMA was trained on the UniRef50 dataset for continual learning and a constructed instruction dataset for tuning. The instruction dataset includes protein sequences and property texts from InterPro, allowing the model to learn both protein generation and property prediction.</p>
<p>Applications</p>
<p>ProLLaMA&rsquo;s versatility extends to multiple PLP tasks:</p>
<p>Unconditional Protein Sequence Generation: Producing novel protein sequences with high structural plausibility.
Controllable Protein Generation: Designing proteins with specific functionalities based on user-provided instructions.
Protein Property Prediction: Achieving nearly 100% accuracy in predicting various protein properties.
Advantages and Limitations</p>
<p>ProLLaMA demonstrates significant advantages over existing ProLLMs, including state-of-the-art performance in protein generation tasks and the ability to handle multi-task PLP applications. The model&rsquo;s design ensures scalability and efficiency, making it accessible for broader research applications.</p>
<p>However, there are limitations to consider:</p>
<p>Resource Intensity: Despite the efficiency improvements, training ProLLaMA requires substantial computational resources.
Natural Language Capabilities: While ProLLaMA retains some natural language understanding, it does not match the proficiency of specialized NLP models like LLaMA2.</p>
<p><a href="https://arxiv.org/abs/2402.16445">https://arxiv.org/abs/2402.16445</a></p>
<h2 id="dna-language-models">DNA Language Models<a hidden class="anchor" aria-hidden="true" href="#dna-language-models">#</a></h2>
<h3 id="dnabert">DNABERT<a hidden class="anchor" aria-hidden="true" href="#dnabert">#</a></h3>
<p>DNABERT ([2021]) is a specialized deep learning model designed to process and analyze genomic DNA sequences by adapting the Bidirectional Encoder Representations from Transformers (BERT) model, which was originally developed for natural language processing. DNABERT uses a transformer-based architecture characterized by attention mechanisms that effectively capture long-range dependencies in DNA sequences. The model architecture is identical to the BERT base model, comprising 12 transformer layers, each with 768 hidden units and 12 attention heads.</p>
<p>DNABERT employs a k-mer tokenization strategy to segment DNA sequences into <em>overlapping</em> k-mers, capturing richer contextual information than traditional base-by-base tokenization. The authors experimented with four k-mer sizes: 3, 4, 5, and 6. Each k-mer model (DNABERT-3, DNABERT-4, DNABERT-5, DNABERT-6) has a vocabulary size of 4^k+5 tokens, including special tokens such as [CLS] (classification), [PAD] (padding), [UNK] (unknown), [SEP] (separator), and [MASK] (masking). Each version captures different levels of contextual information, with DNABERT-6 generally performing the best due to its richer context representation.</p>
<p>The pretraining dataset for DNABERT was derived from the human genome. Sequences were generated through direct non-overlapping splitting and random sampling, with lengths varying between 5 and 510 bases. The pretraining process involved masking 15% of k-mers in the sequences for the first 100,000 steps and increasing this to 20% for the final 20,000 steps. The model was pretrained for 120,000 steps with a batch size of 2,000.</p>
<p>The training process for DNABERT was resource-intensive, taking approximately 25 days on 8 NVIDIA 2080Ti GPUs. This extensive computational effort underscores the complexity and depth of the model’s pretraining process. Meanwhile, the performance depends on the choice of k-mer size, which requires empirical testing for optimization. Limited to human genome data, which might not capture all nuances of other organisms&rsquo; genomes without additional fine-tuning.</p>
<p>Applications:
DNABERT exhibits versatility across various genomic sequence-related tasks. It demonstrated state-of-the-art performance in in 2021 for predicting promoters, splice sites, and transcription factor binding sites. The model’s ability to be fine-tuned with minimal task-specific labeled data makes it highly adaptable for different applications within genomic research. Furthermore, DNABERT provides insights into nucleotide-level importance and the semantic relationships within sequences, aiding in the identification of conserved sequence motifs and functional genetic variants.</p>
<h3 id="dnabert-2">DNABERT-2<a hidden class="anchor" aria-hidden="true" href="#dnabert-2">#</a></h3>
<p>DNABERT-2 ([Zhou2024]) is a refined genome foundation model designed to improve upon the limitations of its predecessor, DNABERT. DNABERT-2 utilizes a transformer-based architecture, specifically leveraging Byte Pair Encoding (BPE) for tokenization instead of the k-mer approach used in DNABERT. This switch addresses computational inefficiencies and sample inefficiencies associated with k-mer tokenization .</p>
<p>Primary Architecture
DNABERT-2 is built upon the transformer architecture, similar to BERT, but incorporates several key innovations. It replaces traditional k-mer tokenization with Byte Pair Encoding (BPE), a technique that improves tokenization efficiency and sequence representation. Additionally, DNABERT-2 integrates Attention with Linear Biases (ALiBi) to overcome input length limitations and employs Flash Attention and Low Precision Layer Normalization to boost computational efficiency. This model comprises 117 million parameters, making it substantially smaller yet more efficient than other models in its class .</p>
<p>Pertaining Methods and Tokenization
DNABERT-2 adopts Byte Pair Encoding (BPE), a data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segments. This method overcomes the limitations of k-mer tokenization, such as information leakage and poor computational efficiency, by providing a more streamlined and effective approach to sequence representation .</p>
<p>Training Datasets
The pre-training of DNABERT-2 involved two main datasets: a human genome dataset consisting of 2.75 billion nucleotide bases, and a multi-species genome dataset comprising genomes from 135 species, totaling 32.49 billion nucleotide bases. This multi-species dataset ensures a broader representation and diversity of genetic sequences</p>
<p>Training Time and Infrastructure
The pre-training of DNABERT-2 took approximately 14 days using eight Nvidia RTX 2080Ti GPUs. The model training utilized the Transformers library and the Composer library. The training process employed a batch size of 4096 and a maximum sequence length of 128, optimized using the AdamW optimizer with specific hyperparameters.</p>
<p>Applications and Benchmarking
DNABERT-2 has been evaluated on the Genome Understanding Evaluation (GUE) benchmark, which includes 36 datasets across nine genome analysis tasks from four species. This comprehensive benchmark enables a standardized assessment of model performance across various tasks such as promoter detection, transcription factor prediction, and species classification.</p>
<p>Versions and Performance
Compared to its predecessor DNABERT, DNABERT-2 achieves superior performance with significantly reduced computational cost and model size. It performs on par with state-of-the-art models while being 21 times smaller and requiring 92 times less GPU time for pre-training. This efficiency makes DNABERT-2 particularly suitable for fine-tuning on consumer GPUs</p>
<p>Advantages and Limitations
DNABERT-2 offers substantial improvements over DNABERT, including better tokenization efficiency, removal of input length limitations, and enhanced computational efficiency. However, the model&rsquo;s performance is highly dependent on the quality and diversity of the training data, and the absence of a comprehensive, standardized benchmark dataset has historically posed challenges in fair comparative analysis. The introduction of the GUE benchmark aims to address this issue, providing a more accurate reflection of model capabilities across diverse genome analysis tasks.</p>
<h3 id="nucleotide-transformer">Nucleotide Transformer<a hidden class="anchor" aria-hidden="true" href="#nucleotide-transformer">#</a></h3>
<p>The Nucleotide Transformer (NT) is a foundation model designed for encoding genomic sequences ([2023]). Developed by InstaDeep in collaboration with Nvidia, the Nucleotide Transformer leverages the transformer architecture, traditionally used in natural language processing, to advance genomic research. This model series, ranging from 50 million to 2.5 billion parameters, was published in recent years, with continuous improvements in training techniques and architectural enhancements.</p>
<p>Model Architecture and Versions
The Nucleotide Transformer employs an encoder-only transformer architecture. THey use non-overlapping 6-mer DNA tokens as a trade-off between
sequence length (up to 6kb) and embedding size, and because it achieved the highest performance
when compared with other token lengths. Positional encodings are then added to each embedding in the sequence to provide the model with positional information. We use a learnable positional encoding layer that accepts a maximum of 1000 tokens. Each model includes a stack of transformer layers, each comprising a layer normalization, multi-head self-attention mechanism, and a two-layer perceptron with GELU activations. The parameter sizes for NT-v1 models range from 500 million to 2.5 billion.</p>
<p>The updated NT-v2 models incorporate several architectural advancements, such as rotary embeddings and swiGLU activations, eliminating MLP biases and dropout mechanisms to improve efficiency. NT-v2 models extend the context length to 12kb and include variants with parameter sizes from 50 million to 500 million​​.</p>
<p>Training Datasets and Methods
The Nucleotide Transformer models are pre-trained on diverse datasets to ensure comprehensive genomic representation. Three primary datasets were used:</p>
<p>Human Reference Genome: Based on the GRCh38/hg38 assembly, encompassing 3.2 billion nucleotides.
1000 Genomes Project (1000G): Comprising 3,202 high-coverage human genomes, representing 20.5 trillion nucleotides from 27 geographically diverse populations.</p>
<p>Multispecies Dataset: Includes 850 genomes from various species, totaling 174 billion nucleotides, selected to maximize diversity and functional relevance across different phyla​​.
Tokenization and Training Process
The tokenization process converts nucleotide sequences into 6-mer tokens, with a vocabulary of 4104 tokens including special tokens for padding, masking, and sequence start. The models are trained using a masked language modeling approach similar to BERT, where 15% of the tokens in each sequence are masked and the model learns to predict these masked tokens. Training utilizes the Adam optimizer with a learning rate schedule, and gradient accumulation to handle large batch sizes effectively.</p>
<p>Training infrastructure involves the Cambridge-1 Nvidia supercomputer, employing 128 A100 GPUs across 16 nodes for large-scale models. The NT-v1 models require up to 28 days for training, while NT-v2 models are trained for extended durations, with the largest models processing up to 1 trillion tokens to understand the scaling laws better​​.</p>
<p>Applications and Performance
The Nucleotide Transformer models have been evaluated on 18 diverse genomic tasks, such as predicting epigenetic marks, promoter sequences, chromatin profiles, and splicing sites. The models demonstrate significant improvements over existing benchmarks like DNABERT and Enformer, particularly in tasks involving human genomic data.</p>
<p>Key applications include:</p>
<p>Epigenetic Marks Prediction: Utilizing datasets of yeast genome histone marks.
Splice Site Prediction: Benchmarking against SpliceAI with impressive performance on splice donor and acceptor site predictions.
Chromatin Profile Prediction: Simultaneous training on 919 chromatin features, outperforming DeepSEA in accuracy​​.
Advantages and Limitations
The Nucleotide Transformer models offer several advantages:</p>
<p>Scalability: Capability to handle sequences up to 12kb with NT-v2 models.
Performance: Superior accuracy across various genomic tasks compared to other foundational models.
Efficiency: Enhanced architectural designs in NT-v2 allow for significant parameter reduction without sacrificing performance.</p>
<h3 id="enformer">Enformer<a hidden class="anchor" aria-hidden="true" href="#enformer">#</a></h3>
<p>Enformer, introduced in the paper &ldquo;Effective gene expression prediction from sequence by integrating long-range interactions&rdquo; published in October 2021, represents a significant advancement in computational genomics. Developed by a team at DeepMind in collaboration with Calico Life Sciences and Google, Enformer leverages transformer architecture to predict gene expression and chromatin states from DNA sequences in humans and mice. Unlike other LLMs, enformer itself does not involve pre-training tasks.</p>
<p>Model Architecture
Enformer combines deep convolutional neural networks with transformer blocks, a technique that significantly extends its receptive field. The model processes 196,608 base pairs (bp) of DNA sequence input and predicts 5,313 genomic tracks for humans and 1,643 for mice. It consists of seven convolutional layers followed by eleven transformer layers, using attention mechanisms to integrate information from distal genomic elements up to 100 kb away. This setup contrasts with previous state-of-the-art models like Basenji2, which can only integrate information from up to 20 kb away.</p>
<p>Positional Embeddings and Attention Mechanisms
The model employs custom relative positional encodings to enhance its ability to distinguish between proximal and distal regulatory elements and to differentiate positions upstream and downstream of the transcription start site (TSS). This approach ensures effective integration of long-range genomic interactions, crucial for accurate gene expression prediction.</p>
<p>Training and Datasets
Enformer was trained using a multitask learning framework on a vast dataset encompassing most of the human and mouse genomes. The training involved 34,021 human and 29,295 mouse sequences, with additional validation and test sets. The dataset included various genomic assays such as transcription factor (TF) chromatin immunoprecipitation and sequencing (ChIP-seq), histone modification ChIP-seq, DNase-seq, and ATAC-seq, providing a comprehensive set of genomic tracks.</p>
<p>Training was conducted on 64 TPU v3 cores over approximately three days, with optimization handled by the Adam optimizer. The model&rsquo;s training and validation employed Poisson negative log-likelihood loss, and data augmentation techniques like random shifting and reverse-complementing the input sequences were used to enhance robustness.</p>
<p>Tokenization and Input Processing
Input DNA sequences are one-hot encoded, with each nucleotide represented by a unique vector (A = [1,0,0,0], C = [0,1,0,0], G = [0,0,1,0], T = [0,0,0,1]). This encoding feeds into the convolutional layers, which reduce the spatial dimension, allowing the transformer layers to capture long-range interactions effectively.</p>
<p>Performance and Applications
Enformer demonstrated superior performance in gene expression prediction compared to Basenji2, with mean correlation improvements from 0.81 to 0.85 in predicting RNA expression at TSSs of human protein-coding genes. The model also showed enhanced ability to predict tissue-specific gene expression and the effects of genetic mutations on gene expression, validated by CRISPR interference assays and population eQTL studies.</p>
<p>The applications of Enformer are vast, including fine-mapping of human disease associations, understanding cis-regulatory evolution, and potentially designing synthetic enhancers for specific cell types. Its ability to predict regulatory activity from DNA sequence alone, without relying on experimental data, presents a significant advantage in genomic research.</p>
<p>Limitations and Future Directions
Despite its advancements, Enformer is limited by its reliance on the cell types and assays present in its training data. It cannot generalize to new cell types or assays not included in its training set. Future improvements could involve integrating 3D genome organization data to better model genomic interactions and expanding training datasets to include more cell types and organisms. Additionally, advancements in computational efficiency and hardware could further enhance the model&rsquo;s scalability and performance.</p>
<h3 id="gpn">GPN<a hidden class="anchor" aria-hidden="true" href="#gpn">#</a></h3>
<p>The Genomic Pre-trained Network (GPN) is an advanced language model designed for genome-wide variant effect prediction, leveraging unsupervised pretraining on genomic DNA sequences. Published in October 2023, GPN represents a significant advancement in the field of computational biology by applying techniques from natural language processing to genomic data. The model was developed by researchers at the University of California, Berkeley, and it is particularly notable for its application to predicting the functional impact of genetic variants in Arabidopsis thaliana, a model organism for plant biology.</p>
<p>Model Architecture and Training</p>
<p>GPN is built on a convolutional neural network architecture rather than the more common transformer-based architectures used in many other language models. This choice is motivated by the efficiency of convolutional networks in handling local dependencies, which are prevalent in genomic sequences. The model processes input DNA sequences of 512 base pairs, where 15% of the positions are masked during training. The goal is to predict the nucleotides at these masked positions, facilitating the learning of complex genomic features and structures.</p>
<p>The core of GPN consists of 25 convolutional blocks, each incorporating a dilated convolutional layer followed by a feed-forward layer, with intermediate residual connections and layer normalization. The embedding dimension throughout these layers is fixed at 512. This architecture enables GPN to capture both local and long-range dependencies within the genomic sequences. The training of GPN involved 150,000 steps and took approximately four days using four NVIDIA A100 80 GB GPUs.</p>
<p>Tokenization and Positional Embeddings</p>
<p>Unlike some models that use k-mers or byte-pair encoding for tokenization, GPN employs single-nucleotide tokens. This simplifies the interpretation of model outputs, particularly important for variant effect prediction. The model does not use explicit positional embeddings; instead, it relies on the structure provided by the convolutional layers to capture positional information within the sequence.</p>
<p>Training Datasets and Methods</p>
<p>GPN was pretrained on unaligned reference genomes from Arabidopsis thaliana and seven related species within the Brassicales order. The training dataset included various genomic regions such as exons, promoters, and random genomic windows, ensuring a comprehensive representation of the genome. To address the overrepresentation of repetitive elements, the training loss was adjusted to down-weight these regions, improving the model&rsquo;s performance on non-repetitive, functionally significant regions.</p>
<p>Applications and Performance</p>
<p>GPN is designed to predict the effects of genetic variants across the genome, making it a powerful tool for genome-wide association studies (GWAS) and fine-mapping of causal variants. It outperforms traditional conservation scores like phyloP and phastCons in predicting variant effects in Arabidopsis thaliana. The model&rsquo;s predictions can be visualized as sequence logos in the UCSC Genome Browser, providing a user-friendly interface for researchers to explore variant effects.</p>
<p>One of the key strengths of GPN is its ability to learn and predict gene structures and DNA motifs without any supervision. This capability is crucial for identifying transcription factor binding sites and other regulatory elements in the genome. Additionally, GPN&rsquo;s predictions show a strong correlation with functional genomic regions, as evidenced by its high accuracy in distinguishing coding sequences, untranslated regions, and introns.</p>
<p>Advantages and Limitations</p>
<p>GPN&rsquo;s main advantage lies in its ability to provide accurate, genome-wide variant effect predictions using only DNA sequence data, making it applicable to a wide range of species without the need for extensive functional genomics data. This scalability is particularly beneficial for studying non-model organisms. The convolutional architecture ensures efficient training and inference, making it suitable for large-scale genomic analyses.</p>
<p>However, there are limitations to the current implementation of GPN. The model&rsquo;s performance on repetitive elements, despite improvements, still poses challenges. Additionally, while the convolutional network handles local dependencies well, incorporating more sophisticated architectures that can capture long-range interactions, such as state space models, could further enhance performance. Future versions of GPN could also benefit from integrating DNA-specific inductive biases and exploring larger model scales and more extensive training datasets.</p>
<h3 id="hyenadna">HyenaDNA<a hidden class="anchor" aria-hidden="true" href="#hyenadna">#</a></h3>
<p>HyenaDNA is a genomic foundation model designed for long-range genomic sequence modeling at single nucleotide resolution. Published on November 15, 2023, by a collaboration between researchers from Stanford University, Harvard University, SynTensor, Mila, and Université de Montréal, this model aims to overcome the limitations of previous Transformer-based genomic models by leveraging the Hyena architecture based on implicit convolutions.</p>
<p>Model Architecture
HyenaDNA utilizes the Hyena architecture, which incorporates long convolutions and element-wise gating layers to process sequences with lower time complexity compared to traditional attention mechanisms. The model scales sub-quadratically in sequence length, allowing it to process up to 1 million tokens. Each token represents a single nucleotide, maintaining the critical resolution necessary for understanding genomic data.
HyenaDNA offers various configurations, allowing flexibility in model size to suit different applications:</p>
<p>2-layer model with width 128: Approximately 436K parameters.
2-layer model with width 256: Approximately 1.6M parameters.
4-layer model with width 128: Approximately 870K parameters.
4-layer model with width 256: Approximately 3.3M parameters.
8-layer model with width 256: Approximately 6.6M parameters.</p>
<p>These configurations illustrate the model&rsquo;s scalability and efficiency, which are crucial for processing long genomic sequences.</p>
<p>The primary architectural components include:</p>
<p>Hyena Operator: Combines long convolutions with element-wise gating, parameterized via an MLP that generates convolutional filters.</p>
<p>Training Infrastructure: Models were trained on Nvidia A100 GPUs with significant use of gradient checkpointing to manage memory consumption for long sequences.</p>
<p>Training and Tokenization
Training Methods:</p>
<p>Sequence Length Warm-up: Introduced to stabilize training on ultralong sequences. Training begins with shorter sequences and gradually increases to longer ones, significantly improving training stability and reducing time.
Soft Prompting: A novel method where learnable tokens are injected into the input sequence to adapt the pretrained model to new tasks without updating the pretrained weights.
Tokenization Methods:</p>
<p>Uses a minimal vocabulary of four nucleotides (A, G, C, T) plus special tokens, avoiding the aggregation methods (k-mers) used by previous models, which compromise single nucleotide resolution.
Training Datasets
HyenaDNA was pretrained on the human reference genome, providing a robust foundation for various genomic tasks. This single-genome pretraining is a limitation, suggesting future work could benefit from incorporating multiple genomes to enhance generalizability.</p>
<p>Applications and Performance
Applications:</p>
<p>Regulatory Element Identification: HyenaDNA can predict the location and function of genes and regulatory elements.
Species Classification: Demonstrated its capability by classifying species based on genomic sequences with high accuracy, even for ultralong sequences.
Chromatin Profile Prediction: Efficiently predicts chromatin features, such as transcription factor binding sites and histone modifications.
Performance:</p>
<p>State-of-the-Art Results: Achieved state-of-the-art performance on 12 of 18 benchmarks compared to the Nucleotide Transformer, using significantly fewer parameters and pretraining data.
GenomicBenchmarks: Surpassed existing models on 7 of 8 datasets, showing a marked improvement in accuracy for enhancer identification and other tasks.</p>
<p>When comparing to other models like DNABERT (110M parameters) and Nucleotide Transformer (up to 2.5B parameters), HyenaDNA’s smaller size (ranging from 436K to 6.6M parameters) demonstrates its efficiency and effectiveness, achieving competitive or superior results with a fraction of the parameters and training data.</p>
<p>Technical Advantages and Limitations
Advantages:</p>
<p>Extended Context Length: Capable of processing sequences up to 1 million tokens, enabling it to model long-range interactions in genomic data.
Training Efficiency: Sub-quadratic scaling in sequence length results in faster training times compared to traditional attention-based models.
Single Nucleotide Resolution: Maintains the necessary resolution to capture subtle genetic variations critical for understanding genomic functions.
Limitations:</p>
<p>Pretraining Data: Limited to a single human reference genome, which may affect the model&rsquo;s ability to generalize across diverse genetic backgrounds.
Hardware Requirements: Requires substantial computational resources for training, although the efficiency gains partially offset this.</p>
<h3 id="evo">Evo<a hidden class="anchor" aria-hidden="true" href="#evo">#</a></h3>
<p>Evo is a cutting-edge genomic foundation model designed for biological sequence modeling and generative tasks at scales ranging from molecular to whole genomes. Developed through collaboration between Stanford University, Arc Institute, TogetherAI, and other institutions, Evo represents a significant advancement in leveraging machine learning for genomic data.</p>
<p>Model Architecture and Parameters
Evo is built on the StripedHyena architecture, a hybrid model combining 29 layers of data-controlled convolutional operators with 3 layers of multi-head attention equipped with rotary position embeddings (RoPE). This structure is designed to efficiently process long DNA sequences while maintaining single-nucleotide resolution. Evo scales up to 7 billion parameters and utilizes a context length of 131 kilobases (kb) at single-nucleotide resolution.</p>
<p>Publication and Training
The model was first introduced in a preprint published on March 6, 2024. Evo was pretrained on a vast dataset of 300 billion nucleotide tokens, encompassing 2.7 million prokaryotic and phage genomes from databases such as GTDB, IMG/PR, and IMG/VR. The training followed a two-stage process: an initial phase with an 8k token context length, followed by a context extension phase to 131k tokens. Evo was trained using an autoregressive modeling approach, where the model predicts the likelihood of the next token given a sequence of tokens, which is a fundamental technique for capturing complex patterns in DNA sequences. Evo employs a decoder-only framework, enabling it to efficiently handle long context lengths and maintain single-nucleotide resolution.</p>
<p>The training data for Evo includes a vast collection of 2.7 million prokaryotic and phage genomes, amounting to 300 billion nucleotide tokens. This data was sourced from comprehensive genomic databases such as the Genome Taxonomy Database (GTDB), Integrated Microbial Genomes/Virus (IMG/VR), and Integrated Microbial Genomes/Plasmid (IMG/PR) databases.</p>
<p>Tokenization and Positional Embedding
Evo employs byte-level, single-nucleotide tokenization, allowing it to model sequences with fine granularity. The use of rotary position embeddings (RoPE) aids in maintaining positional information over long contexts, crucial for genomic sequences where the relative position of nucleotides can impact biological function.</p>
<p>Training Infrastructure and Time
Training Evo required substantial computational resources, including high-performance GPUs and TPUs. The exact duration of the training process is not specified, but it involved extensive compute hours to process and learn from the vast genomic datasets.</p>
<p>Applications and Performance
Evo excels in both predictive and generative tasks across multiple biological modalities:</p>
<p>Zero-shot Function Prediction: Evo outperforms domain-specific models in predicting the effects of mutations on protein and ncRNA functions without task-specific finetuning.
Generative Design: It can generate synthetic CRISPR-Cas systems, including coherent protein and non-coding RNA sequences, and multi-component biological systems such as transposable elements.
Gene Essentiality Prediction: Using long genomic contexts, Evo accurately predicts essential genes in bacteria and phages, providing insights into organismal fitness and potential targets for drug discovery.
Whole-genome Sequence Generation: Evo generates sequences up to 650 kb, demonstrating high coding density and plausible genomic organization, though it occasionally lacks key marker genes.
Advantages and Limitations
Evo&rsquo;s primary advantage lies in its ability to handle long genomic sequences at single-nucleotide resolution, enabling comprehensive modeling of biological systems. Its hybrid architecture ensures efficient processing and scalability, outperforming traditional Transformer models on genomic data.</p>
<p>However, Evo also has limitations:</p>
<p>Training Data Scope: Currently trained only on prokaryotic data, Evo&rsquo;s predictions for eukaryotic sequences, including human genomes, are limited.
Generation Quality: While Evo generates high-level genomic organization, the finer details and completeness of generated sequences can be improved.
Computational Demand: The model&rsquo;s training and inference require significant computational resources, which may limit accessibility for some research groups.</p>
<h2 id="rna-language-models">RNA Language Models<a hidden class="anchor" aria-hidden="true" href="#rna-language-models">#</a></h2>
<h3 id="rinalmo">RiNALMo<a hidden class="anchor" aria-hidden="true" href="#rinalmo">#</a></h3>
<h3 id="rnaernie">RNAErnie<a hidden class="anchor" aria-hidden="true" href="#rnaernie">#</a></h3>
<h3 id="bigrna">BigRNA<a hidden class="anchor" aria-hidden="true" href="#bigrna">#</a></h3>
<h2 id="chemistry-llm">Chemistry LLM<a hidden class="anchor" aria-hidden="true" href="#chemistry-llm">#</a></h2>
<p><a href="https://portal.valencelabs.com/blogs/post/from-molecules-to-materials-pre-training-large-generalizable-models-for-Q4afm0EdqUEmrqN?utm_source=substack&amp;utm_medium=email">https://portal.valencelabs.com/blogs/post/from-molecules-to-materials-pre-training-large-generalizable-models-for-Q4afm0EdqUEmrqN?utm_source=substack&amp;utm_medium=email</a></p>
<h2 id="image-llm-owkin-pathai">Image LLM Owkin, pathAI<a hidden class="anchor" aria-hidden="true" href="#image-llm-owkin-pathai">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/2024-04-29_large_p_small_n/">
    <span class="title">« Prev</span>
    <br>
    <span>What a large p for small n</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2024-01-06_how_robust_ai/">
    <span class="title">Next »</span>
    <br>
    <span>Toward Robust AI (2): How To Achieve Robust AI</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (3): Chemistry on x"
            href="https://x.com/intent/tweet/?text=Biomedical%20LLMs%20%283%29%3a%20Chemistry&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_chemistry%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Biomedical LLMs (3): Chemistry on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_chemistry%2f&amp;title=Biomedical%20LLMs%20%283%29%3a%20Chemistry&amp;summary=Biomedical%20LLMs%20%283%29%3a%20Chemistry&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2024-05-15_biollm_chemistry%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>

</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2025 <a href="http://localhost:1313/">JX&#39;s log</a> | <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a> | </span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
