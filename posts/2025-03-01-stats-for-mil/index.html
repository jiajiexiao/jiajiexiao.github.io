<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Estimating Statistical Properties in Grouped Measurements | JX&#39;s log</title>
<meta name="keywords" content="">
<meta name="description" content="Intro
In this post, we&rsquo;ll explore a statistical problem: estimating population means
and their uncertainties from hierarchically structured data or so called grouped
measurements. While averaging measurements may seem straightforward, the
presence of natural groupings in data introduces important statistical
considerations that require careful treatment.
To illustrate the practical significance of this problem, let&rsquo;s examine how
hierarchically structured measurements - where individual observations are
naturally clustered into groups - arise across diverse real-world applications.
Multiple-Instance Learning (MIL) represents an important machine learning
paradigm specifically designed for analyzing such grouped or clustered data
structures. The following scenarios showcase a few situations where measurements
naturally organize into clusters of varying sizes:">
<meta name="author" content="Jiajie Xiao">
<link rel="canonical" href="http://localhost:1313/posts/2025-03-01-stats-for-mil/">
<link crossorigin="anonymous" href="../../assets/css/stylesheet.c292a07dea08ffa7274e381a70305fb0723ab31bafbf10e470c03a04b23c11b6.css" integrity="sha256-wpKgfeoI/6cnTjgacDBfsHI6sxuvvxDkcMA6BLI8EbY=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/2025-03-01-stats-for-mil/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<!DOCTYPE html>

<html>
  <head>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>

    
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload='renderMathInElement(
          document.body, 
          {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false},
                {left: "\\[", right: "\\]", display: true},
                {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                {left: "\\begin{align}", right: "\\end{align}", display: true},
                {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                {left: "\\begin{CD}", right: "\\end{CD}", display: true},
            ]
          }
        );'></script>
  </head>
</html>



</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="JX&#39;s log (Alt + H)">JX&#39;s log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tag">
                    <span>Tag</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Estimating Statistical Properties in Grouped Measurements
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-03-01 22:32:12 -0800 PST'>2025-03-01</span>&nbsp;·&nbsp;18 min&nbsp;·&nbsp;Jiajie Xiao

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">Table of Contents</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#intro" aria-label="Intro">Intro</a></li>
                    <li>
                        <a href="#problem-statement" aria-label="Problem Statement">Problem Statement</a></li>
                    <li>
                        <a href="#complete-pooling-analyzing-identically-distributed-measurements-as-a-single-dataset" aria-label="Complete Pooling: Analyzing Identically Distributed Measurements as a Single Dataset">Complete Pooling: Analyzing Identically Distributed Measurements as a Single Dataset</a></li>
                    <li>
                        <a href="#macro-averaging-equal-weighting-of-samples" aria-label="Macro-Averaging: Equal Weighting of Samples">Macro-Averaging: Equal Weighting of Samples</a></li>
                    <li>
                        <a href="#meta-analytic-approaches-a-framework-for-combining-multiple-samples" aria-label="Meta-Analytic Approaches: A Framework for Combining Multiple Samples">Meta-Analytic Approaches: A Framework for Combining Multiple Samples</a><ul>
                            
                    <li>
                        <a href="#fixed-effects-model-fe-combining-estimates-under-homogeneity" aria-label="Fixed-Effects Model (FE): Combining Estimates Under Homogeneity">Fixed-Effects Model (FE): Combining Estimates Under Homogeneity</a></li>
                    <li>
                        <a href="#random-effects-model-re-accounting-for-both-within-sample-and-between-sample-heterogeneity" aria-label="Random-Effects Model (RE): Accounting for both Within-sample and Between-Sample Heterogeneity">Random-Effects Model (RE): Accounting for both Within-sample and Between-Sample Heterogeneity</a></li></ul>
                    </li>
                    <li>
                        <a href="#bayesian-hierarchical-modeling-a-probabilistic-framework" aria-label="Bayesian Hierarchical Modeling: A Probabilistic Framework">Bayesian Hierarchical Modeling: A Probabilistic Framework</a><ul>
                            
                    <li>
                        <a href="#the-hierarchical-structure" aria-label="The Hierarchical Structure">The Hierarchical Structure</a></li></ul>
                    </li>
                    <li>
                        <a href="#summary" aria-label="Summary">Summary</a></li>
                    <li>
                        <a href="#citation" aria-label="Citation">Citation</a></li>
                    <li>
                        <a href="#references" aria-label="References">References</a>
                    </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>

  <div class="post-content"><h2 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h2>
<p>In this post, we&rsquo;ll explore a statistical problem: estimating population means
and their uncertainties from hierarchically structured data or so called grouped
measurements. While averaging measurements may seem straightforward, the
presence of natural groupings in data introduces important statistical
considerations that require careful treatment.</p>
<p>To illustrate the practical significance of this problem, let&rsquo;s examine how
hierarchically structured measurements - where individual observations are
naturally clustered into groups - arise across diverse real-world applications.
Multiple-Instance Learning (MIL) represents an important machine learning
paradigm specifically designed for analyzing such grouped or clustered data
structures. The following scenarios showcase a few situations where measurements
naturally organize into clusters of varying sizes:</p>
<ol>
<li>
<p>Product Defect Rates Across Factories: A manufacturer tracks product quality
across multiple factories. Each factory tests a different number of units
daily, with each unit providing a pass/fail measurement. These measurements
are naturally grouped by factory, with potential batch effects from different
equipments or quality control processes.</p>
</li>
<li>
<p>Survey Response Across Regions: A nationwide survey collects responses from
different geographical regions. Each region contributes a different number of
responses based on population size, creating natural groups where responses
are clustered by region. Regional cultural or demographic factors may
introduce biases in how people respond.</p>
</li>
<li>
<p>Disease Prevalence Across Clinics: Medical researchers study disease
occurrence patterns across multiple clinics. Each clinic reports data from
their patient population, with varying sample sizes due to clinic size and
local demographics.</p>
</li>
<li>
<p>Digital Pathology: Pathologists analyze tissue samples by examining multiple
regions within each sample. Each region provides specific measurements about
cell characteristics or tissue features. These measurements naturally group
together under the same patient sample, with the number of regions varying
based on the tissue sample size.</p>
</li>
<li>
<p>Liquid Biopsy Analysis: When analyzing blood samples from cancer patients,
researchers examine millions of DNA molecules, including both normal
cell-free DNA and tumor-derived DNA. Each molecule provides a distinct
measurement, and these measurements are naturally grouped by patient sample,
with molecule counts varying between samples.</p>
</li>
</ol>
<p>These examples share a common pattern: the data consists of groups (referred to
as samples), where each group contains a varying number of individual
measurements (referred to as instances). This dual-level variation in the
hierarchical structure presents an interesting statistical problem as seen in
the following section. Our focus will be on developing robust methods for
analyzing data where measurements naturally cluster into groups of different
sizes, considering both the variations that occur within groups and those that
emerge between different groups.</p>
<h2 id="problem-statement">Problem Statement<a hidden class="anchor" aria-hidden="true" href="#problem-statement">#</a></h2>
<div style="background-color: #FFFFE0; padding: 10px; border-left: 6px solid #FFD700; border-radius: 5px; margin: 10px 0; color: #000000;">
<strong>Problem Statement</strong>
<p>Given a dataset consisting of $N$ samples $S = {S_1, &hellip;, S_N}$ with
hierarchical structure:</p>
<ul>
<li><strong>Sample Level</strong>: Each sample $S_i$ represents a distinct group or cluster</li>
<li><strong>Instance Level</strong>: Within each sample $S_i$, there are $n_i$ individual
measurements ${x_{i,1}, &hellip;, x_{i,n_i}}$</li>
<li><strong>Measurement Properties</strong>:
<ul>
<li>Each instance measurement $x_{i,j} ∈ ℝ$ follows some distribution $D_i$
specific to sample $i$</li>
<li>Key statistical properties include:
<ul>
<li>Population parameters:
<ul>
<li>Mean: $μ_i = \mathbb{E}_j[x_{i,j}]$</li>
<li>Variance: $\sigma_i^2 = \mathbb{Var}_j[x_{i,j}]$</li>
</ul>
</li>
<li>Sample statistics:
<ul>
<li>Mean: $\overline{x}_i = \frac{1}{n_i}\sum_j x_{i,j}$</li>
<li>Variance: $s_i^2 = \frac{1}{n_i-1}\sum_j (x_{i,j}-\overline{x}_i)^2$</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ol>
<li>How do we estimate a reliable overall mean $μ = \mathbb{E}[x_{i,j}]$ across all
samples $S_i$ and instances $x_{i,j}$?</li>
<li>How can we properly quantify the
uncertainty (e.g. standard error $σ_{\overline{μ}}$) in this estimate?</li>
</ol>
</div>
<p>This problem requires attention to how the data is generated and structured,
particularly when dealing with varying group sizes and potential batch effects.
We need to derive an unbiased formula for the mean that accounts for samples of
different sizes, while providing uncertainty estimates (e.g. standard error,
confidence intervals) that consider both within-sample and between-sample
variability.</p>
<h2 id="complete-pooling-analyzing-identically-distributed-measurements-as-a-single-dataset">Complete Pooling: Analyzing Identically Distributed Measurements as a Single Dataset<a hidden class="anchor" aria-hidden="true" href="#complete-pooling-analyzing-identically-distributed-measurements-as-a-single-dataset">#</a></h2>
<p>Consider the ideal scenario where all measurements are independently and
identically distributed (i.i.d.) from a single underlying distribution. In this
case, we can disregard group structure and estimate population parameters
directly through simple aggregation.</p>
<p>For finite samples, we can estimate the population mean μ optimally through the
pooled sample mean:</p>
<p><a id="eq1"></a> \begin{equation} \overline{x} =
\frac{\sum_{i=1}^{N}  \sum_{j=1}^{n_i} x_{i,j}}{ \sum_{i=1}^{N}n_i }
\end{equation}</p>
<p>This estimator corresponds to both the arithmetic mean of all observations and
the maximum likelihood estimate under i.i.d. assumptions.</p>
<p>The standard error of this estimate can be derived from the Central Limit
Theorem as:</p>
<p><a id="eq2"></a>
\begin{equation}
\mathrm{SE} = \frac{s}{\sqrt{\sum_{i=1}^{N} n_i}}
= \sqrt{\frac{\sum_{i=1}^{N} \sum_{j=1}^{n_i} (x_{i,j} - \overline{x})^2}
{\left( \sum_{i=1}^{N} n_i \right) \left( \sum_{i=1}^{N} n_i - 1 \right) }}
\end{equation}</p>
<p>where $s$ represents the pooled standard deviation. Here we use
Student approximation to use the sample standard deviation $s$ instead
of the unknown true standard deviation $σ$.</p>
<p>This approach, known as micro-averaging or sample size-weighted averaging,
achieves statistical optimality under the i.i.d. assumption. When measurements
are truly i.i.d., the group structure becomes irrelevant since each individual
measurement contributes equally valid information about the underlying
distribution. However, this method&rsquo;s effectiveness critically depends on the
i.i.d. assumption holding true.</p>
<p>In practice, the i.i.d. assumption often fails due to several interrelated
factors. For example, sampling biases frequently emerge within individual
groups, while systematic differences manifest between different groups. The data
collection process itself can introduce batch effects that create artificial
patterns or variations. Additionally, natural heterogeneity across different
populations can lead to inherent variations that violate the assumption of
identical distributions.</p>
<p>When i.i.d doesn&rsquo;t hold, complete pooling can produce misleading results. Large
samples with systematic biases will dominate both the mean estimate and
uncertainty calculations, potentially masking important patterns in smaller
samples. For example, if a large sample systematically oversamples a particular
subpopulation or measurement range, this bias will disproportionately influence
the overall estimates.</p>
<p>To address these limitations, researchers typically employ more sophisticated
approaches that explicitly model the clustered nature of the data rather than
treating all measurements as independent observations from a single
distribution.</p>
<h2 id="macro-averaging-equal-weighting-of-samples">Macro-Averaging: Equal Weighting of Samples<a hidden class="anchor" aria-hidden="true" href="#macro-averaging-equal-weighting-of-samples">#</a></h2>
<p>The macro-averaging approach provides an alternative method for estimating the
overall mean by first calculating individual sample means and then averaging
them equally across all samples. This two-step process ensures each sample
contributes equally to the final estimate, regardless of its size:</p>
<p><a id="eq3"></a>
\begin{equation}
\overline{x}_{macro} = \frac{1}{N} \sum_{i=1}^{N} \overline{x}_i,  \quad \text{where} \quad \overline{x}_i = \frac{1}{n_i} \sum_{j=1}^{n_i} {x_{i,j}}
\end{equation}</p>
<p>This approach enables independent computation of sample-level means
$\overline{x}_i$, ensuring that each sample&rsquo;s statistics are calculated without
interference from other samples, which preserves the integrity of individual
cluster information. The approach implements a democratic weighting scheme,
where each sample contributes exactly $1/N$ to the final estimate, regardless of
its size or variance characteristics. The method prevents larger clusters from
overwhelming smaller ones, maintains accuracy even in the presence of strong
within-cluster correlations, and handles substantial variations in sample sizes
with stability.</p>
<p>A basic approximation of the standard error uses the between-sample variance:</p>
<p><a id="eq4"></a>
\begin{equation}
\mathrm{SE}_{\text{macro}} = \sqrt{\frac{1}{N(N-1)} \sum_{i=1}^{N} (\overline{x}_i - \overline{x}_{\text{macro}})^2}
\end{equation}</p>
<p>However, this simple approximation obviously neglects both within-sample
variability and the effects of varying cluster sizes. Assuming minimal
within-sample variation but significant cluster size differences, we can derive
a more comprehensive variance decomposition using the law of total variance:</p>
<p><a id="eq5"></a>
\begin{equation}
\mathrm{Var}(\overline{X}) = \underbrace{\mathbb{E}[N]\mathrm{Var}(X)}_{\text{between-sample}} + \underbrace{\mathrm{Var}(N)(\mathbb{E}[X])^2}_{\text{cluster-size effect}}
\end{equation}</p>
<p>This leads to a more accurate standard error estimation:</p>
<p>\begin{equation}
\mathrm{SE}_{\text{total}} = \sqrt{\frac{\mathbb{E}[N]s^2 + s_N^2(\overline{x}_{\text{macro}})^2}{N}}
\end{equation}</p>
<p>where:</p>
<ul>
<li>$\mathbb{E}[N] = \overline{n} = \frac{1}{N}\sum_{i=1}^N n_i$ is the average
cluster size</li>
<li>$s^2 = \frac{1}{N-1}\sum_{i=1}^N (\overline{x}_i - \overline{x}_{\text{macro}})^2$ is the pooled between-sample variance</li>
<li>$s_N^2 = \frac{1}{N-1}\sum_{i=1}^N (n_i - \overline{n})^2$ captures the
variance in cluster sizes</li>
</ul>
<p>While this approach provides a more comprehensive treatment of cluster size
effects and helps mitigate potential biases from grouped measurements, it
becomes increasingly complex when incorporating within-sample variability in
addition. This complexity motivates the need for a more elegant analytical
framework, which we&rsquo;ll explore in the following section on meta-analytic
approaches.</p>
<h2 id="meta-analytic-approaches-a-framework-for-combining-multiple-samples">Meta-Analytic Approaches: A Framework for Combining Multiple Samples<a hidden class="anchor" aria-hidden="true" href="#meta-analytic-approaches-a-framework-for-combining-multiple-samples">#</a></h2>
<p>Meta-analysis, first introduced by Gene Glass in 1976 for the &ldquo;analysis of
analyses&rdquo; (<a href="#Glass1976">Glass1976</a>), has become an important methodology in
evidence synthesis across many fields. Originally developed to combine results
from multiple clinical trials, it has evolved into a sophisticated statistical
framework used in diverse areas from social sciences to engineering. The method
arose from the need to make sense of sometimes conflicting results across
multiple studies and to increase statistical power by pooling data
systematically.</p>
<p>In our context of multiple-instance learning, meta-analysis provides powerful
frameworks for combining estimates from multiple samples while accounting for
different variance components. While meta-analysis can handle various types of
estimates (like correlations or odds ratios), we&rsquo;ll focus specifically on
combining mean estimates across samples, which is our key quantity of interest
from each sample.</p>
<p>When combining mean estimates from multiple samples, two critical types of
variance need consideration:</p>
<ul>
<li>Within-sample variance: Captures the uncertainty in individual sample mean
estimates, expressed through standard errors that depend on both the sample
size and the spread of measurements within each sample</li>
<li>Between-sample variance: Reflects true heterogeneity in means across
different samples, which could arise from batch effects or systematic
differences between samples</li>
</ul>
<p>As we will see in details later, meta-analytical approaches are particularly
valuable for analyzing hierarchical data structures because they:</p>
<ul>
<li>Combine sample means while appropriately weighting by their precision
(inverse variance)</li>
<li>Model both within-sample and between-sample variability simultaneously</li>
<li>Account for varying sample sizes and potential dependencies within groups</li>
<li>Provide frameworks for testing whether samples are homogeneous or
heterogeneous</li>
</ul>
<h3 id="fixed-effects-model-fe-combining-estimates-under-homogeneity">Fixed-Effects Model (FE): Combining Estimates Under Homogeneity<a hidden class="anchor" aria-hidden="true" href="#fixed-effects-model-fe-combining-estimates-under-homogeneity">#</a></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Fixed_effects_model">fixed-effects model</a> assumes:</p>
<ol>
<li>All samples share a single true mean μ</li>
<li>Observed between-sample differences in means arise solely from:
<ul>
<li>Random sampling variation within samples (quantified by $s_i^2/n_i$)</li>
<li>No systematic between-sample heterogeneity</li>
</ul>
</li>
</ol>
<p>This is equivalent to assuming perfect transportability <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> across samples - any
apparent differences in their estimates would disappear given infinite
within-sample measurements ($\lim_{n_i \to \infty} \overline{x}_i = \mu,
\forall i $).</p>
<p>Under this model, we employ an inverse-variance weighted estimator that combines estimates by assigning greater weight to more precise measurements <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>:</p>
<p>\begin{equation}
\overline{x}_{\text{FE}} = \frac{\sum_{i=1}^{N} w_i
\overline{x}_i}{\sum_{i=1}^{N} w_i}, \quad w_i =
\frac{1}{\mathrm{Var}(\overline{x}_i)} = \frac{n_i}{s_i^2}
\end{equation}</p>
<p>Where:</p>
<ul>
<li>Variance of the sample mean (standard error squared): $\mathrm{Var}(\overline{x}_i) = \mathrm{SE}^2(\overline{x}_i) =
{s_i^2}/{n_i}$</li>
<li>Sample variance of individual instances within sample $i$: $s_i^2 =
\frac{1}{n_i-1}\sum_{j=1}^{n_i}(x_{i,j}-\overline{x}_i)^2$</li>
</ul>
<p>The uncertainty estimate on $\overline{x}_{\text{FE}}$ can be measured by
standard error:
\begin{equation}
\mathrm{SE}_{\text{FE}} = \frac{1} {\sqrt{\sum_{i=1}^{N} w_i}} = \sqrt{\frac{1}
{\sum_{i=1}^{N} \frac{n_i} {s_i^2} }}
\end{equation}</p>
<!-- 

**Key assumptions**:
1. Homogeneity of true means ($\mu_i = \mu,\ \forall i$)
2. Within-sample variances estimated accurately
3. Observations independent both within and between samples -->
<h3 id="random-effects-model-re-accounting-for-both-within-sample-and-between-sample-heterogeneity">Random-Effects Model (RE): Accounting for both Within-sample and Between-Sample Heterogeneity<a hidden class="anchor" aria-hidden="true" href="#random-effects-model-re-accounting-for-both-within-sample-and-between-sample-heterogeneity">#</a></h3>
<p>While the fixed-effects model efficiently combines estimates when samples are
homogeneous, its fundamental assumption of a single true underlying mean
(perfect transportability) often proves unrealistic in practice. Different
samples frequently exhibit systematically different means due to various
factors:</p>
<ul>
<li>Data collection procedures and protocols</li>
<li>Population characteristics and selection biases</li>
<li>Environmental conditions and temporal variations</li>
<li>Batch effects and instrumental drift</li>
<li>Laboratory-specific practices</li>
<li>Geographic variations</li>
<li>Other unobserved confounding factors</li>
</ul>
<p>To account for this between-sample heterogeneity, DerSimonian and Laird (1986)
proposed the random-effects model (<a href="#DerSimonian1986">DerSimonian1986</a>). This
approach extends the fixed-effects framework by introducing an additional
variance component $\tau^2$ that captures true differences between sample means.
The model takes a hierarchical form:</p>
<p>\begin{equation}
\overline{x}_i \sim \mathcal{N}(\mu_i, \sigma_i^2/n_i)
\end{equation}</p>
<p>\begin{equation}
\mu_i \sim \mathcal{N}(\mu, \tau^2)
\end{equation}</p>
<p>This hierarchical structure acknowledges two distinct sources of variation:</p>
<ol>
<li>Within-sample variation ($\sigma_i^2/n_i$): Captures measurement uncertainty</li>
<li>Between-sample variation ($\tau^2$): Models true heterogeneity between samples</li>
</ol>
<p>The DerSimonian-Laird estimator combines sample means using modified weights
$w^*$ that incorporate both variance components:</p>
<p>\begin{equation}
\overline{x}_{\text{RE}} = \frac{\sum w_i^* \overline{x}_i}{\sum w_i^*}, \quad w_i^* = \frac{1}{s_i^2/n_i + \tau^2}
\end{equation}</p>
<p>Similar to FE model, within-sample variance of the mean is estimated by
$s_i^2/n_i = \frac{1}{n_i(n_i-1)}\sum_{j=1}^{n_i}(x_{i,j}-\overline{x}_i)^2$.</p>
<p>Between-sample variance term $\tau^2$ is estimated using the <em>DerSimonian-Laird</em>
method:</p>
<p>\begin{equation}
\tau^2 = \max\left(0, \frac{Q - (N-1)}{\sum_{i=1}^N w_i - \sum_{i=1}^N w_i^2/\sum_{i=1}^N w_i}\right)
\end{equation}</p>
<p>where $w_i$ is the weight in fixed-effects model and Cochran&rsquo;s $Q$ statistic
quantifies heterogeneity:</p>
<p>\begin{equation} Q = \sum_{i=1}^N w_i(\overline{x}_i - \overline{x}_{\text{FE}})^2 \end{equation}</p>
<p>The Q statistic follows a chi-square distribution under the null hypothesis of
homogeneity, providing a formal test for between-sample heterogeneity. Large Q
values suggest significant heterogeneity and support using the random-effects
model over fixed-effects.</p>
<p>Uncertainty for $\overline{x}_{\text{RE}}$ can be measured by standard error
with the modified weights $w^*$. The standard variance estimator is:</p>
<p>\begin{equation}
\mathrm{SE}_{\text{RE,naive}} = \frac{1} {\sqrt{\sum_{i=1}^{N} w_i^*}} = \sqrt{\frac{1}
{\sum_{i=1}^{N} \frac{1}{s_i^2/n_i + \tau^2} }}
\end{equation}</p>
<p>However, this naive variance estimation tends to underestimate uncertainty when</p>
<ol>
<li>working with a small number of samples (N), which limits the precision of
heterogeneity estimates</li>
<li>substantial heterogeneity exists between samples, making the simple pooled
variance inadequate</li>
<li>sample sizes or weights are unbalanced across different groups</li>
</ol>
<p>To address these limitations, the Hartung-Knapp-Sidik-Jonkman (HKSJ) method (see
original references 4-6, 11 and 12 in this
<a href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-14-25">article</a>)
provides a more robust variance estimator that better accounts for uncertainty
in the estimation of between-study heterogeneity:</p>
<p>\begin{equation}
\mathrm{SE}_{\text{RE,HKSJ}} = \sqrt{\frac{\sum_{i=1}^N w_i^{*}(\overline{x}_i - \overline{x}_{\text{RE}})^2}{(N-1)\sum_{i=1}^N w_i^{*}}}
\end{equation}</p>
<p>As seen, the random-effects model has a more comprehensive characterization than
FE. When the between-sample heterogeneity is significant, the random-effects
model captures both within-sample and between-sample variability. When the
between-sample heterogeneity is not significant, the random-effects model
reduces to the fixed-effects model as $\tau^2 \rightarrow 0$.</p>
<!-- 
Beyond DerSimonian-Laird, several alternative estimation approaches exist,
including Restricted Maximum Likelihood (REML) for small samples [](),
Paule-Mandel for non-normal data [](), Empirical Bayes for data-prior fusion
[](), and Profile likelihood for improved confidence intervals [](). More
details please refer to the literature. -->
<h2 id="bayesian-hierarchical-modeling-a-probabilistic-framework">Bayesian Hierarchical Modeling: A Probabilistic Framework<a hidden class="anchor" aria-hidden="true" href="#bayesian-hierarchical-modeling-a-probabilistic-framework">#</a></h2>
<p>While the frequentist approaches discussed above provide practical solutions for
combining estimates across samples, they rely on asymptotic approximations<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> and
point estimates that may not fully capture parameter uncertainty, especially
with small sample sizes or complex hierarchical structures. A more comprehensive
approach is offered by Bayesian hierarchical modeling, which naturally handles
multi-level data structures by explicitly modeling the full data generation
process. This framework treats all unknown parameters as random variables with
associated probability distributions, which enables us to:</p>
<ul>
<li>Directly model the data generation process</li>
<li>Incorporate prior knowledge about parameters</li>
<li>Obtain full posterior distributions for uncertainty quantification</li>
<li>Account for different sources of variation simultaneously</li>
</ul>
<h3 id="the-hierarchical-structure">The Hierarchical Structure<a hidden class="anchor" aria-hidden="true" href="#the-hierarchical-structure">#</a></h3>
<p>The beauty of Bayesian hierarchical modeling lies in how it breaks down complex
data into interconnected, understandable layers - like a recipe where each step
builds upon the previous ones. The Bayesian hierarchical model provides a
natural framework for describing how information flows from raw measurements
through connected levels to arrive at population-level insights. By examining
these relationships between adjacent layers, we can better understand the
complete data generation process:</p>
<ol>
<li><strong>Population Level</strong>: An overall mean μ represents the true population-wide
average we want to estimate</li>
<li><strong>Sample Level</strong>: Each sample i has its own true mean μᵢ that varies around μ</li>
<li><strong>Instance Level</strong>: Individual measurements within each sample vary around
their sample-specific mean μᵢ</li>
</ol>
<p>This naturally maps to an example hierarchical model below:</p>
<ul>
<li>Instance Level (Likelihood):</li>
</ul>
<p><a id="eq_bayes_likelihood"></a>
\begin{equation}
\overline{x}_i \sim \mathcal{N}(\mu_i, \sigma_i^2/n_i)
\end{equation}</p>
<p>This level models how individual sample means $\overline{x}_i$ vary around their
true sample-specific means $\mu_i$. The Central Limit Theorem justifies using a
normal distribution when sample sizes are sufficiently large. Here, $\sigma_i^2$
represents the within-sample variance for sample i, capturing the spread of
individual measurements within that sample. The term $n_i$ is the number of
instances in sample i - dividing by $n_i$ reflects how the variance of a sample
mean decreases with larger sample sizes, a key result from sampling theory. This
variance structure $\sigma_i^2/n_i$ directly incorporates both the inherent
variability of measurements ($\sigma_i^2$) and the precision gained from larger
samples ($n_i$).</p>
<ul>
<li>Sample Level (Prior):
<a id="eq_bayes_prior1"></a>
\begin{equation}
\mu_i \sim \mathcal{N}(\mu, \tau^2)
\end{equation}</li>
</ul>
<p>This level captures how true sample means $\mu_i$ vary around the population
mean $\mu$. The normal distribution assumption reflects our belief that
sample-level effects are symmetric and unbounded. The parameter $\tau^2$
represents the between-sample variance, quantifying how much the true sample
means tend to differ from each other. A larger $\tau^2$ indicates greater
heterogeneity between samples, while a smaller $\tau^2$ suggests more
homogeneous samples. When $\tau^2$ approaches zero, the model effectively
reduces to a fixed-effects model where all samples share nearly identical true
means.</p>
<ul>
<li>Population Level (Hyperpriors):</li>
</ul>
<p><a id="eq_bayes_hyper1"></a>
\begin{equation}
\mu \sim \mathcal{N}(\mu_0, \sigma_0)
\end{equation}</p>
<p>This level specifies our prior beliefs about the overall population mean $\mu$.
The hyperparameters $\mu_0$ and $\sigma_0$ should be chosen based on domain
knowledge or standardization of the data. For illustration, we might use $\mu_0
= 0$ and $\sigma_0 = 10$ as a weakly informative prior that can accommodate a
broad range of possible values. In practice, these values should be adjusted
based on the scale of your measurements and any prior knowledge about plausible
parameter ranges. The choice of these parameters also influences the degree of
regularization in the model - smaller values of $\sigma_0$ lead to stronger
regularization while larger values allow for more flexibility in the estimates.
For example, if working with standardized data (mean 0, variance 1), $\mu_0 = 0,
\sigma_0 = 1$ might be more appropriate. For proportion data bounded between 0
and 1, using $\mu_0 = 0.5, \sigma_0 = 0.25$ could better reflect the constrained
parameter space.</p>
<ul>
<li>Between-Sample Variation:</li>
</ul>
<p><a id="eq_bayes_hyper2"></a>
\begin{equation}
\tau \sim \mathrm{HalfCauchy}(\mu_\tau, \sigma_\tau)
\end{equation}</p>
<p>This parameter models the scale of variation between different samples. The
<a href="https://distribution-explorer.github.io/continuous/halfcauchy.html">HalfCauchy
prior</a> is
chosen for its heavy tail that allows for potentially large between-sample
variation while still maintaining some regularization near zero. The location
parameter $\mu_\tau$ is typically set to 0 to ensure non-negative scale
parameters, while the scale parameter $\sigma_\tau$ controls the spread of the
distribution. Common choices include:</p>
<ul>
<li>$\sigma_\tau = 2$ for moderate regularization (recommended by <a href="#Gelman2006">Gelman</a>)</li>
<li>$\sigma_\tau = 1$ for stronger regularization when sample sizes are small</li>
<li>$\sigma_\tau = 5$ for weaker regularization when strong heterogeneity is expected</li>
</ul>
<p>The choice of $\sigma_\tau$ should be guided by the scale of your data and prior
knowledge about between-sample variation.</p>
<ul>
<li>Within-Sample Variation:
<a id="eq_bayes_hyper3"></a>
\begin{equation}
\sigma_i \sim \mathrm{Exponential}(\lambda)
\end{equation}</li>
</ul>
<p>This parameter captures the scale of variation within each sample. The
<a href="https://en.wikipedia.org/wiki/Exponential_distribution">Exponential prior</a> with rate parameter $\lambda$ encourages smaller values while
still allowing for larger variations when supported by the data. The choice of
$\lambda$ depends on the scale of your measurements:</p>
<ul>
<li>$\lambda = 1$ is common for standardized data (variance ≈ 1) such as z-scores
or normalized features</li>
<li>$\lambda = 0.1$ for data with larger variance (≈ 10) such as raw pixel
intensities in medical imaging</li>
<li>$\lambda = 10$ for small-scale measurements (variance ≈ 0.1) such as
proportions or probabilities near 0 or 1</li>
</ul>
<p>The rate parameter $\lambda$ effectively sets the prior expectation of the
standard deviation as $\mathbb{E}[\sigma_i] = 1/\lambda$. This hierarchical
structure automatically implements &ldquo;partial pooling&rdquo; - estimates for individual
samples are shrunk toward the overall mean, with the degree of shrinkage
determined by the relative magnitudes of within-sample and between-sample
variation. This helps prevent extreme estimates from samples with limited data
while still preserving meaningful differences between samples.</p>
<p>After specifying the model structure through likelihood, prior, and hyperprior
distributions, we can estimate the joint posterior distribution of all
parameters conditional on the observed data:</p>
<p>\begin{equation}
p(\mu, \tau, {\mu_i}, {\sigma_i} | {\overline{x}_i}, {n_i}) \propto p({\overline{x}_i}|{\mu_i},{\sigma_i},{n_i}) \cdot p({\mu_i}|\mu,\tau) \cdot p(\mu) \cdot p(\tau) \cdot p({\sigma_i})
\end{equation}</p>
<p>where each term corresponds to components defined in equations earlier. This
posterior can be estimated using either:</p>
<ol>
<li>
<p><strong>Markov Chain Monte Carlo (MCMC)</strong>: Provides exact posterior sampling
through various algorithms:</p>
<ul>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings</a></strong>:
Uses proposal distributions to explore the parameter space:</p>
<p>\begin{equation}
\alpha = \min\left(1, \frac{p(\theta&rsquo;|\text{data})q(\theta|\theta&rsquo;)}{p(\theta|\text{data})q(\theta&rsquo;|\theta)}\right)
\end{equation}</p>
<p>where $q(\theta&rsquo;|\theta)$ is the proposal distribution and $\alpha$ is the
acceptance probability</p>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs Sampling</a></strong>: Samples
each parameter conditional on others:</p>
<p>\begin{equation}
\theta_i^{(t+1)} \sim p(\theta_i|\theta_{-i}^{(t)}, \text{data})
\end{equation}</p>
</li>
<li>
<p><strong>Hamiltonian Monte Carlo (HMC)</strong>: Leverages gradient information for
efficient exploration:</p>
<p>\begin{equation}
\theta^{(t+1)} \sim K(\theta^{(t)}, \theta) \cdot p(\theta|\text{data})
\end{equation}</p>
<p>where $K$ is a transition kernel incorporating Hamiltonian dynamics</p>
</li>
</ul>
</li>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational
Bayes</a> (VB)</strong>:
Approximates the posterior by optimizing a simpler distribution
$q_\phi(\theta)$ to minimize KL-divergence:</p>
<p>\begin{equation}
\phi^* = \arg\min_\phi \text{KL}(q_\phi(\theta) || p(\theta|\text{data}))
\end{equation}</p>
<p>VB is computationally more efficient than MCMC, making it suitable for large
datasets, though typically less accurate in estimating the true posterior.</p>
</li>
</ol>
<p>Modern probabilistic programming frameworks like Stan, PyMC, and TensorFlow
Probability make these Bayesian inference accessible and computationally
tractable. Although these Bayesian methods are often computationally more
complex than the frequentist approaches described earlier, they offer several
key advantages: (1) more accurate uncertainty quantification through full
posterior distributions (2) natural incorporation of prior knowledge and domain
expertise (3) better handling of small sample sizes through partial pooling and
(4) flexibility to extend models with additional structure or covariates.</p>
<h2 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h2>
<p>We&rsquo;ve examined various approaches for estimating population means from
hierarchically structured data, each with distinct assumptions and applications:</p>
<!-- 
1. **Complete Pooling/Micro-Averaging**: Assumes i.i.d. measurements across all samples
2. **Macro-Averaging**: Treats each sample equally regardless of size
3. **Meta-Analytic Methods**:
   - Fixed-Effects (FE): Assumes homogeneous true means
   - Random-Effects (RE): Accounts for between-sample heterogeneity
4. **Bayesian Hierarchical Modeling**: Full probabilistic treatment of uncertainty -->
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Assumptions</th>
          <th>Weighting</th>
          <th>Uncertainty Handling</th>
          <th>Use Cases</th>
          <th>Complexity</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Complete Pooling (Micro-Averaging)</td>
          <td>IID measurements</td>
          <td>Sample size</td>
          <td>Variance in all instances in all samples</td>
          <td>Homogeneous groups</td>
          <td>Low</td>
      </tr>
      <tr>
          <td>Macro-Averaging</td>
          <td>Equal sample importance</td>
          <td>Equal per sample</td>
          <td>Between-sample variance</td>
          <td>Small N, balanced groups</td>
          <td>Low</td>
      </tr>
      <tr>
          <td>Fixed-Effects (FE)</td>
          <td>Homogeneous true means</td>
          <td>Inverse within-var</td>
          <td>Within-sample variance</td>
          <td>Controlled experiments</td>
          <td>Medium</td>
      </tr>
      <tr>
          <td>Random-Effects (RE)</td>
          <td>Heterogeneous true means</td>
          <td>Inverse total var</td>
          <td>Within + between variance</td>
          <td>Observational studies</td>
          <td>High</td>
      </tr>
      <tr>
          <td>Bayesian Hierarchical</td>
          <td>Full probability model</td>
          <td>Adaptive shrinkage</td>
          <td>Full posterior distribution</td>
          <td>Small samples, complex hierarchies</td>
          <td>Typically Very High</td>
      </tr>
  </tbody>
</table>
<p>The choice of method depends on specific data distributions and your needs. In
practice, Complete Pooling/Micro-Averaging works best when samples are
homogeneous and efficiency is key, while Macro-Averaging is ideal when each
sample should have equal influence. Random-Effects modeling offer a good balance
of flexibility and interpretability, though may be less efficient than directly
building an equivalent Fixed-Effects model when samples are similar. Bayesian
approaches provide the most thorough uncertainty quantification but require more
inductive biases and computational resources. For very large datasets (e.g. &gt;1M
instances) with limited computing power, consider using Variational Bayes or
simplified meta-analytic methods. When unsure, the DerSimonian-Laird
random-effects model with HKSJ variance adjustment can be a good default choice.</p>
<h2 id="citation">Citation<a hidden class="anchor" aria-hidden="true" href="#citation">#</a></h2>
<p>If you find this post helpful and are interested in referencing it in your write-up, you can cite it as</p>
<blockquote>
<p>Xiao, Jiajie. (March 2025). <em>Estimating Statistical Properties in Grouped
Measurements</em>. JX&rsquo;s log. Available at:
<a href="https://jiajiexiao.github.io/posts/2023-03-01_stats-for-mil/">https://jiajiexiao.github.io/posts/2023-03-01_stats-for-mil/</a>.</p>
</blockquote>
<p>or add the following to your BibTeX file.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bib" data-lang="bib"><span style="display:flex;"><span><span style="color:#a6e22e">@article</span>{xiao2025stats-for-mil,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">title</span>   = <span style="color:#e6db74">&#34;Estimating Statistical Properties in Grouped Measurements&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">author</span>  = <span style="color:#e6db74">&#34;Xiao, Jiajie&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">journal</span> = <span style="color:#e6db74">&#34;JX&#39;s log&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">year</span>    = <span style="color:#e6db74">&#34;2025&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">month</span>   = <span style="color:#e6db74">&#34;March&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">url</span>     = <span style="color:#e6db74">&#34;https://jiajiexiao.github.io/posts/2023-03-01_stats-for-mil/&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li><a id="Glass1976"></a>  Glass, G. V. (1976). Primary, secondary, and
meta-analysis of research. Educational researcher, 5(10), 3-8.</li>
<li><a id="DerSimonian1986"></a> DerSimonian, R., &amp; Laird, N. (1986). Meta-analysis in clinical trials. Controlled clinical trials, 7(3), 177-188.</li>
<li><a id="Gelman2006"></a> Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper).</li>
</ul>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p><strong>Perfect Transportability</strong> describes a strong assumption that estimation
results from any sample can be directly generalized to all other samples
without adjustments $$ \forall i,k\ \lim_{n_i,n_k \to \infty}
(\overline{x}_i - \overline{x}_k) = 0. $$ This implies sample collection
mechanisms and subpopulation characteristics do not systematically influence
measurements. For example, you will see perfect transportability if all
clinics draw from identical patient populations, factories use identical
processes/materials during Manufacturing.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The inverse-variance weighting scheme (w = 1/SE²) naturally assigns higher weights to more precise estimates - those with smaller within-sample variance and/or larger sample sizes. This approach combines heterogeneous estimates by giving more influence to more reliable measurements while downweighting less precise ones. For example, in clinical trials, larger studies with tighter confidence intervals would receive proportionally more weight than smaller studies with wider intervals.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><strong>Asymptotic approximations</strong> in this context refer to statistical
properties that only hold as sample size approaches infinity. For example,
the Central Limit Theorem guarantees sample means are normally distributed
only for large enough samples. With small samples, these approximations
become less reliable - normal distributions may not adequately describe
uncertainty, and confidence intervals based on standard errors may not
achieve their nominal coverage rates (e.g., a &ldquo;95%&rdquo; confidence interval may
contain the true parameter less than 95% of the time).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/test/">
    <span class="title">« Prev</span>
    <br>
    <span>Basic Markdown Syntax</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/2025-03-01-stats-for-mil-copy/">
    <span class="title">Next »</span>
    <br>
    <span>Stats for multiple-instance learning</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Estimating Statistical Properties in Grouped Measurements on x"
            href="https://x.com/intent/tweet/?text=Estimating%20Statistical%20Properties%20in%20Grouped%20Measurements&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2025-03-01-stats-for-mil%2f&amp;hashtags=">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Estimating Statistical Properties in Grouped Measurements on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2f2025-03-01-stats-for-mil%2f&amp;title=Estimating%20Statistical%20Properties%20in%20Grouped%20Measurements&amp;summary=Estimating%20Statistical%20Properties%20in%20Grouped%20Measurements&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2f2025-03-01-stats-for-mil%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>

</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023-2025 <a href="http://localhost:1313/">JX&#39;s log</a> | <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1" 
        target="_blank" rel="license noopener noreferrer">CC BY-NC-SA 4.0 </a> | </span>

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>

    

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
