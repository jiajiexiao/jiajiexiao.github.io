[{"content":"Intro In this post, we\u0026rsquo;ll explore a statistical problem: estimating population means and their uncertainties from hierarchically structured data or so called grouped measurements. While averaging measurements may seem straightforward, the presence of natural groupings in data introduces important statistical considerations that require careful treatment.\nTo illustrate the practical significance of this problem, let\u0026rsquo;s examine how hierarchically structured measurements - where individual observations are naturally clustered into groups - arise across diverse real-world applications. Multiple-Instance Learning (MIL) represents an important machine learning paradigm specifically designed for analyzing such grouped or clustered data structures. The following scenarios showcase a few situations where measurements naturally organize into clusters of varying sizes:\nProduct Defect Rates Across Factories: A manufacturer tracks product quality across multiple factories. Each factory tests a different number of units daily, with each unit providing a pass/fail measurement. These measurements are naturally grouped by factory, with potential batch effects from different equipments or quality control processes.\nSurvey Response Across Regions: A nationwide survey collects responses from different geographical regions. Each region contributes a different number of responses based on population size, creating natural groups where responses are clustered by region. Regional cultural or demographic factors may introduce biases in how people respond.\nDisease Prevalence Across Clinics: Medical researchers study disease occurrence patterns across multiple clinics. Each clinic reports data from their patient population, with varying sample sizes due to clinic size and local demographics.\nDigital Pathology: Pathologists analyze tissue samples by examining multiple regions within each sample. Each region provides specific measurements about cell characteristics or tissue features. These measurements naturally group together under the same patient sample, with the number of regions varying based on the tissue sample size.\nLiquid Biopsy Analysis: When analyzing blood samples from cancer patients, researchers examine millions of DNA molecules, including both normal cell-free DNA and tumor-derived DNA. Each molecule provides a distinct measurement, and these measurements are naturally grouped by patient sample, with molecule counts varying between samples.\nThese examples share a common pattern: the data consists of groups (referred to as samples), where each group contains a varying number of individual measurements (referred to as instances). This dual-level variation in the hierarchical structure presents an interesting statistical problem as seen in the following section. Our focus will be on developing robust methods for analyzing data where measurements naturally cluster into groups of different sizes, considering both the variations that occur within groups and those that emerge between different groups.\nProblem Statement Problem Statement Given a dataset consisting of $N$ samples $S = {S_1, \u0026hellip;, S_N}$ with hierarchical structure:\nSample Level: Each sample $S_i$ represents a distinct group or cluster Instance Level: Within each sample $S_i$, there are $n_i$ individual measurements ${x_{i,1}, \u0026hellip;, x_{i,n_i}}$ Measurement Properties: Each instance measurement $x_{i,j} ∈ ℝ$ follows some distribution $D_i$ specific to sample $i$ Key statistical properties include: Population parameters: Mean: $μ_i = \\mathbb{E}_j[x_{i,j}]$ Variance: $\\sigma_i^2 = \\mathbb{Var}_j[x_{i,j}]$ Sample statistics: Mean: $\\overline{x}_i = \\frac{1}{n_i}\\sum_j x_{i,j}$ Variance: $s_i^2 = \\frac{1}{n_i-1}\\sum_j (x_{i,j}-\\overline{x}_i)^2$ How do we estimate a reliable overall mean $μ = \\mathbb{E}[x_{i,j}]$ across all samples $S_i$ and instances $x_{i,j}$? How can we properly quantify the uncertainty (e.g. standard error $σ_{\\overline{μ}}$) in this estimate? This problem requires attention to how the data is generated and structured, particularly when dealing with varying group sizes and potential batch effects. We need to derive an unbiased formula for the mean that accounts for samples of different sizes, while providing uncertainty estimates (e.g. standard error, confidence intervals) that consider both within-sample and between-sample variability.\nComplete Pooling: Analyzing Identically Distributed Measurements as a Single Dataset Consider the ideal scenario where all measurements are independently and identically distributed (i.i.d.) from a single underlying distribution. In this case, we can disregard group structure and estimate population parameters directly through simple aggregation.\nFor finite samples, we can estimate the population mean μ optimally through the pooled sample mean:\n\\begin{equation} \\overline{x} = \\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{n_i} x_{i,j}}{ \\sum_{i=1}^{N}n_i } \\end{equation}\nThis estimator corresponds to both the arithmetic mean of all observations and the maximum likelihood estimate under i.i.d. assumptions.\nThe standard error of this estimate can be derived from the Central Limit Theorem as:\n\\begin{equation} \\mathrm{SE} = \\frac{s}{\\sqrt{\\sum_{i=1}^{N} n_i}} = \\sqrt{\\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{n_i} (x_{i,j} - \\overline{x})^2} {\\left( \\sum_{i=1}^{N} n_i \\right) \\left( \\sum_{i=1}^{N} n_i - 1 \\right) }} \\end{equation}\nwhere $s$ represents the pooled standard deviation. Here we use Student approximation to use the sample standard deviation $s$ instead of the unknown true standard deviation $σ$.\nThis approach, known as micro-averaging or sample size-weighted averaging, achieves statistical optimality under the i.i.d. assumption. When measurements are truly i.i.d., the group structure becomes irrelevant since each individual measurement contributes equally valid information about the underlying distribution. However, this method\u0026rsquo;s effectiveness critically depends on the i.i.d. assumption holding true.\nIn practice, the i.i.d. assumption often fails due to several interrelated factors. For example, sampling biases frequently emerge within individual groups, while systematic differences manifest between different groups. The data collection process itself can introduce batch effects that create artificial patterns or variations. Additionally, natural heterogeneity across different populations can lead to inherent variations that violate the assumption of identical distributions.\nWhen i.i.d doesn\u0026rsquo;t hold, complete pooling can produce misleading results. Large samples with systematic biases will dominate both the mean estimate and uncertainty calculations, potentially masking important patterns in smaller samples. For example, if a large sample systematically oversamples a particular subpopulation or measurement range, this bias will disproportionately influence the overall estimates.\nTo address these limitations, researchers typically employ more sophisticated approaches that explicitly model the clustered nature of the data rather than treating all measurements as independent observations from a single distribution.\nMacro-Averaging: Equal Weighting of Samples The macro-averaging approach provides an alternative method for estimating the overall mean by first calculating individual sample means and then averaging them equally across all samples. This two-step process ensures each sample contributes equally to the final estimate, regardless of its size:\n\\begin{equation} \\overline{x}_{macro} = \\frac{1}{N} \\sum_{i=1}^{N} \\overline{x}_i, \\quad \\text{where} \\quad \\overline{x}_i = \\frac{1}{n_i} \\sum_{j=1}^{n_i} {x_{i,j}} \\end{equation}\nThis approach enables independent computation of sample-level means $\\overline{x}_i$, ensuring that each sample\u0026rsquo;s statistics are calculated without interference from other samples, which preserves the integrity of individual cluster information. The approach implements a democratic weighting scheme, where each sample contributes exactly $1/N$ to the final estimate, regardless of its size or variance characteristics. The method prevents larger clusters from overwhelming smaller ones, maintains accuracy even in the presence of strong within-cluster correlations, and handles substantial variations in sample sizes with stability.\nA basic approximation of the standard error uses the between-sample variance:\n\\begin{equation} \\mathrm{SE}_{\\text{macro}} = \\sqrt{\\frac{1}{N(N-1)} \\sum_{i=1}^{N} (\\overline{x}_i - \\overline{x}_{\\text{macro}})^2} \\end{equation}\nHowever, this simple approximation obviously neglects both within-sample variability and the effects of varying cluster sizes. Assuming minimal within-sample variation but significant cluster size differences, we can derive a more comprehensive variance decomposition using the law of total variance:\n\\begin{equation} \\mathrm{Var}(\\overline{X}) = \\underbrace{\\mathbb{E}[N]\\mathrm{Var}(X)}_{\\text{between-sample}} + \\underbrace{\\mathrm{Var}(N)(\\mathbb{E}[X])^2}_{\\text{cluster-size effect}} \\end{equation}\nThis leads to a more accurate standard error estimation:\n\\begin{equation} \\mathrm{SE}_{\\text{total}} = \\sqrt{\\frac{\\mathbb{E}[N]s^2 + s_N^2(\\overline{x}_{\\text{macro}})^2}{N}} \\end{equation}\nwhere:\n$\\mathbb{E}[N] = \\overline{n} = \\frac{1}{N}\\sum_{i=1}^N n_i$ is the average cluster size $s^2 = \\frac{1}{N-1}\\sum_{i=1}^N (\\overline{x}_i - \\overline{x}_{\\text{macro}})^2$ is the pooled between-sample variance $s_N^2 = \\frac{1}{N-1}\\sum_{i=1}^N (n_i - \\overline{n})^2$ captures the variance in cluster sizes While this approach provides a more comprehensive treatment of cluster size effects and helps mitigate potential biases from grouped measurements, it becomes increasingly complex when incorporating within-sample variability in addition. This complexity motivates the need for a more elegant analytical framework, which we\u0026rsquo;ll explore in the following section on meta-analytic approaches.\nMeta-Analytic Approaches: A Framework for Combining Multiple Samples Meta-analysis, first introduced by Gene Glass in 1976 for the \u0026ldquo;analysis of analyses\u0026rdquo; (Glass1976), has become an important methodology in evidence synthesis across many fields. Originally developed to combine results from multiple clinical trials, it has evolved into a sophisticated statistical framework used in diverse areas from social sciences to engineering. The method arose from the need to make sense of sometimes conflicting results across multiple studies and to increase statistical power by pooling data systematically.\nIn our context of multiple-instance learning, meta-analysis provides powerful frameworks for combining estimates from multiple samples while accounting for different variance components. While meta-analysis can handle various types of estimates (like correlations or odds ratios), we\u0026rsquo;ll focus specifically on combining mean estimates across samples, which is our key quantity of interest from each sample.\nWhen combining mean estimates from multiple samples, two critical types of variance need consideration:\nWithin-sample variance: Captures the uncertainty in individual sample mean estimates, expressed through standard errors that depend on both the sample size and the spread of measurements within each sample Between-sample variance: Reflects true heterogeneity in means across different samples, which could arise from batch effects or systematic differences between samples As we will see in details later, meta-analytical approaches are particularly valuable for analyzing hierarchical data structures because they:\nCombine sample means while appropriately weighting by their precision (inverse variance) Model both within-sample and between-sample variability simultaneously Account for varying sample sizes and potential dependencies within groups Provide frameworks for testing whether samples are homogeneous or heterogeneous Fixed-Effects Model (FE): Combining Estimates Under Homogeneity The fixed-effects model assumes:\nAll samples share a single true mean μ Observed between-sample differences in means arise solely from: Random sampling variation within samples (quantified by $s_i^2/n_i$) No systematic between-sample heterogeneity This is equivalent to assuming perfect transportability 1 across samples - any apparent differences in their estimates would disappear given infinite within-sample measurements ($\\lim_{n_i \\to \\infty} \\overline{x}_i = \\mu, \\forall i $).\nUnder this model, we employ an inverse-variance weighted estimator that combines estimates by assigning greater weight to more precise measurements 2:\n\\begin{equation} \\overline{x}_{\\text{FE}} = \\frac{\\sum_{i=1}^{N} w_i \\overline{x}_i}{\\sum_{i=1}^{N} w_i}, \\quad w_i = \\frac{1}{\\mathrm{Var}(\\overline{x}_i)} = \\frac{n_i}{s_i^2} \\end{equation}\nWhere:\nVariance of the sample mean (standard error squared): $\\mathrm{Var}(\\overline{x}_i) = \\mathrm{SE}^2(\\overline{x}_i) = {s_i^2}/{n_i}$ Sample variance of individual instances within sample $i$: $s_i^2 = \\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(x_{i,j}-\\overline{x}_i)^2$ The uncertainty estimate on $\\overline{x}_{\\text{FE}}$ can be measured by standard error: \\begin{equation} \\mathrm{SE}_{\\text{FE}} = \\frac{1} {\\sqrt{\\sum_{i=1}^{N} w_i}} = \\sqrt{\\frac{1} {\\sum_{i=1}^{N} \\frac{n_i} {s_i^2} }} \\end{equation}\nRandom-Effects Model (RE): Accounting for both Within-sample and Between-Sample Heterogeneity While the fixed-effects model efficiently combines estimates when samples are homogeneous, its fundamental assumption of a single true underlying mean (perfect transportability) often proves unrealistic in practice. Different samples frequently exhibit systematically different means due to various factors:\nData collection procedures and protocols Population characteristics and selection biases Environmental conditions and temporal variations Batch effects and instrumental drift Laboratory-specific practices Geographic variations Other unobserved confounding factors To account for this between-sample heterogeneity, DerSimonian and Laird (1986) proposed the random-effects model (DerSimonian1986). This approach extends the fixed-effects framework by introducing an additional variance component $\\tau^2$ that captures true differences between sample means. The model takes a hierarchical form:\n\\begin{equation} \\overline{x}_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2/n_i) \\end{equation}\n\\begin{equation} \\mu_i \\sim \\mathcal{N}(\\mu, \\tau^2) \\end{equation}\nThis hierarchical structure acknowledges two distinct sources of variation:\nWithin-sample variation ($\\sigma_i^2/n_i$): Captures measurement uncertainty Between-sample variation ($\\tau^2$): Models true heterogeneity between samples The DerSimonian-Laird estimator combines sample means using modified weights $w^*$ that incorporate both variance components:\n\\begin{equation} \\overline{x}_{\\text{RE}} = \\frac{\\sum w_i^* \\overline{x}_i}{\\sum w_i^*}, \\quad w_i^* = \\frac{1}{s_i^2/n_i + \\tau^2} \\end{equation}\nSimilar to FE model, within-sample variance of the mean is estimated by $s_i^2/n_i = \\frac{1}{n_i(n_i-1)}\\sum_{j=1}^{n_i}(x_{i,j}-\\overline{x}_i)^2$.\nBetween-sample variance term $\\tau^2$ is estimated using the DerSimonian-Laird method:\n\\begin{equation} \\tau^2 = \\max\\left(0, \\frac{Q - (N-1)}{\\sum_{i=1}^N w_i - \\sum_{i=1}^N w_i^2/\\sum_{i=1}^N w_i}\\right) \\end{equation}\nwhere $w_i$ is the weight in fixed-effects model and Cochran\u0026rsquo;s $Q$ statistic quantifies heterogeneity:\n\\begin{equation} Q = \\sum_{i=1}^N w_i(\\overline{x}_i - \\overline{x}_{\\text{FE}})^2 \\end{equation}\nThe Q statistic follows a chi-square distribution under the null hypothesis of homogeneity, providing a formal test for between-sample heterogeneity. Large Q values suggest significant heterogeneity and support using the random-effects model over fixed-effects.\nUncertainty for $\\overline{x}_{\\text{RE}}$ can be measured by standard error with the modified weights $w^*$. The standard variance estimator is:\n\\begin{equation} \\mathrm{SE}_{\\text{RE,naive}} = \\frac{1} {\\sqrt{\\sum_{i=1}^{N} w_i^*}} = \\sqrt{\\frac{1} {\\sum_{i=1}^{N} \\frac{1}{s_i^2/n_i + \\tau^2} }} \\end{equation}\nHowever, this naive variance estimation tends to underestimate uncertainty when\nworking with a small number of samples (N), which limits the precision of heterogeneity estimates substantial heterogeneity exists between samples, making the simple pooled variance inadequate sample sizes or weights are unbalanced across different groups To address these limitations, the Hartung-Knapp-Sidik-Jonkman (HKSJ) method (see original references 4-6, 11 and 12 in this article) provides a more robust variance estimator that better accounts for uncertainty in the estimation of between-study heterogeneity:\n\\begin{equation} \\mathrm{SE}_{\\text{RE,HKSJ}} = \\sqrt{\\frac{\\sum_{i=1}^N w_i^{*}(\\overline{x}_i - \\overline{x}_{\\text{RE}})^2}{(N-1)\\sum_{i=1}^N w_i^{*}}} \\end{equation}\nAs seen, the random-effects model has a more comprehensive characterization than FE. When the between-sample heterogeneity is significant, the random-effects model captures both within-sample and between-sample variability. When the between-sample heterogeneity is not significant, the random-effects model reduces to the fixed-effects model as $\\tau^2 \\rightarrow 0$.\nBayesian Hierarchical Modeling: A Probabilistic Framework While the frequentist approaches discussed above provide practical solutions for combining estimates across samples, they rely on asymptotic approximations3 and point estimates that may not fully capture parameter uncertainty, especially with small sample sizes or complex hierarchical structures. A more comprehensive approach is offered by Bayesian hierarchical modeling, which naturally handles multi-level data structures by explicitly modeling the full data generation process. This Bayesian inference framework treats all unknown parameters as random variables with associated probability distributions, which enables us to:\nDirectly model the data generation process Incorporate prior knowledge about parameters Obtain full posterior distributions for uncertainty quantification Account for different sources of variation simultaneously The Hierarchical Structure The beauty of Bayesian hierarchical modeling lies in how it breaks down complex data into interconnected, understandable layers - like a recipe where each step builds upon the previous ones. The Bayesian hierarchical model provides a natural framework for describing how information flows from raw measurements through connected levels to arrive at population-level insights. By examining these relationships between adjacent layers, we can better understand the complete data generation process:\nPopulation Level: An overall mean μ represents the true population-wide average we want to estimate Sample Level: Each sample i has its own true mean μᵢ that varies around μ Instance Level: Individual measurements within each sample vary around their sample-specific mean μᵢ This naturally maps to an example hierarchical model below:\nInstance Level (Likelihood): \\begin{equation} \\overline{x}_i \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2/n_i) \\end{equation}\nThis level models how individual sample means $\\overline{x}_i$ vary around their true sample-specific means $\\mu_i$. The Central Limit Theorem justifies using a normal distribution when sample sizes are sufficiently large. Here, $\\sigma_i^2$ represents the within-sample variance for sample i, capturing the spread of individual measurements within that sample. The term $n_i$ is the number of instances in sample i - dividing by $n_i$ reflects how the variance of a sample mean decreases with larger sample sizes, a key result from sampling theory. This variance structure $\\sigma_i^2/n_i$ directly incorporates both the inherent variability of measurements ($\\sigma_i^2$) and the precision gained from larger samples ($n_i$).\nSample Level (Prior): \\begin{equation} \\mu_i \\sim \\mathcal{N}(\\mu, \\tau^2) \\end{equation} This level captures how true sample means $\\mu_i$ vary around the population mean $\\mu$. The normal distribution assumption reflects our belief that sample-level effects are symmetric and unbounded. The parameter $\\tau^2$ represents the between-sample variance, quantifying how much the true sample means tend to differ from each other. A larger $\\tau^2$ indicates greater heterogeneity between samples, while a smaller $\\tau^2$ suggests more homogeneous samples. When $\\tau^2$ approaches zero, the model effectively reduces to a fixed-effects model where all samples share nearly identical true means.\nPopulation Level (Hyperpriors): \\begin{equation} \\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0) \\end{equation}\nThis level specifies our prior beliefs about the overall population mean $\\mu$. The hyperparameters $\\mu_0$ and $\\sigma_0$ should be chosen based on domain knowledge or standardization of the data. For illustration, we might use $\\mu_0 = 0$ and $\\sigma_0 = 10$ as a weakly informative prior that can accommodate a broad range of possible values. In practice, these values should be adjusted based on the scale of your measurements and any prior knowledge about plausible parameter ranges. The choice of these parameters also influences the degree of regularization in the model - smaller values of $\\sigma_0$ lead to stronger regularization while larger values allow for more flexibility in the estimates. For example, if working with standardized data (mean 0, variance 1), $\\mu_0 = 0, \\sigma_0 = 1$ might be more appropriate. For proportion data bounded between 0 and 1, using $\\mu_0 = 0.5, \\sigma_0 = 0.25$ could better reflect the constrained parameter space.\nBetween-Sample Variation: \\begin{equation} \\tau \\sim \\mathrm{HalfCauchy}(\\mu_\\tau, \\sigma_\\tau) \\end{equation}\nThis parameter models the scale of variation between different samples. The HalfCauchy prior is chosen for its heavy tail that allows for potentially large between-sample variation while still maintaining some regularization near zero. The location parameter $\\mu_\\tau$ is typically set to 0 to ensure non-negative scale parameters, while the scale parameter $\\sigma_\\tau$ controls the spread of the distribution. Common choices include:\n$\\sigma_\\tau = 2$ for moderate regularization (recommended by Gelman) $\\sigma_\\tau = 1$ for stronger regularization when sample sizes are small $\\sigma_\\tau = 5$ for weaker regularization when strong heterogeneity is expected The choice of $\\sigma_\\tau$ should be guided by the scale of your data and prior knowledge about between-sample variation.\nWithin-Sample Variation: \\begin{equation} \\sigma_i \\sim \\mathrm{Exponential}(\\lambda) \\end{equation} This parameter captures the scale of variation within each sample. The Exponential prior with rate parameter $\\lambda$ encourages smaller values while still allowing for larger variations when supported by the data. The choice of $\\lambda$ depends on the scale of your measurements:\n$\\lambda = 1$ is common for standardized data (variance ≈ 1) such as z-scores or normalized features $\\lambda = 0.1$ for data with larger variance (≈ 10) such as raw pixel intensities in medical imaging $\\lambda = 10$ for small-scale measurements (variance ≈ 0.1) such as proportions or probabilities near 0 or 1 The rate parameter $\\lambda$ effectively sets the prior expectation of the standard deviation as $\\mathbb{E}[\\sigma_i] = 1/\\lambda$. This hierarchical structure automatically implements \u0026ldquo;partial pooling\u0026rdquo; - estimates for individual samples are shrunk toward the overall mean, with the degree of shrinkage determined by the relative magnitudes of within-sample and between-sample variation. This helps prevent extreme estimates from samples with limited data while still preserving meaningful differences between samples.\nAfter specifying the model structure through likelihood, prior, and hyperprior distributions, we can estimate the joint posterior distribution of all parameters conditional on the observed data:\n\\begin{equation} p(\\mu, \\tau, {\\mu_i}, {\\sigma_i} | {\\overline{x}_i}, {n_i}) \\propto p({\\overline{x}_i}|{\\mu_i},{\\sigma_i},{n_i}) \\cdot p({\\mu_i}|\\mu,\\tau) \\cdot p(\\mu) \\cdot p(\\tau) \\cdot p({\\sigma_i}) \\end{equation}\nwhere each term corresponds to components defined in equations earlier. This posterior can be estimated using either:\nMarkov Chain Monte Carlo (MCMC): Provides exact posterior sampling through various algorithms:\nMetropolis-Hastings: Uses proposal distributions to explore the parameter space:\n\\begin{equation} \\alpha = \\min\\left(1, \\frac{p(\\theta\u0026rsquo;|\\text{data})q(\\theta|\\theta\u0026rsquo;)}{p(\\theta|\\text{data})q(\\theta\u0026rsquo;|\\theta)}\\right) \\end{equation}\nwhere $q(\\theta\u0026rsquo;|\\theta)$ is the proposal distribution and $\\alpha$ is the acceptance probability\nGibbs Sampling: Samples each parameter conditional on others:\n\\begin{equation} \\theta_i^{(t+1)} \\sim p(\\theta_i|\\theta_{-i}^{(t)}, \\text{data}) \\end{equation}\nHamiltonian Monte Carlo (HMC): Leverages gradient information for efficient exploration:\n\\begin{equation} \\theta^{(t+1)} \\sim K(\\theta^{(t)}, \\theta) \\cdot p(\\theta|\\text{data}) \\end{equation}\nwhere $K$ is a transition kernel incorporating Hamiltonian dynamics\nVariational Bayes (VB): Approximates the posterior by optimizing a simpler distribution $q_\\phi(\\theta)$ to minimize KL-divergence:\n\\begin{equation} \\phi^* = \\arg\\min_\\phi \\text{KL}(q_\\phi(\\theta) || p(\\theta|\\text{data})) \\end{equation}\nVB is computationally more efficient than MCMC, making it suitable for large datasets, though typically less accurate in estimating the true posterior.\nModern probabilistic programming frameworks like Stan, PyMC, and TensorFlow Probability make these Bayesian inference accessible and computationally tractable. Although these Bayesian methods are often computationally more complex than the frequentist approaches described earlier, they offer several key advantages: (1) more accurate uncertainty quantification through full posterior distributions (2) natural incorporation of prior knowledge and domain expertise (3) better handling of small sample sizes through partial pooling and (4) flexibility to extend models with additional structure or covariates.\nSummary We\u0026rsquo;ve examined various approaches for estimating population means from hierarchically structured data, each with distinct assumptions and applications:\nMethod Assumptions Weighting Uncertainty Handling Use Cases Complexity Complete Pooling (Micro-Averaging) IID measurements Sample size Variance in all instances in all samples Homogeneous groups Low Macro-Averaging Equal sample importance Equal per sample Between-sample variance Small N, balanced groups Low Fixed-Effects (FE) Homogeneous true means Inverse within-var Within-sample variance Controlled experiments Medium Random-Effects (RE) Heterogeneous true means Inverse total var Within + between variance Observational studies High Bayesian Hierarchical Inductive biases for likelihood, prior, and hyperprior Adaptive shrinkage Full posterior distribution Small samples, complex hierarchies Typically Very High The choice of method depends on specific data distributions and your needs. In practice, Complete Pooling/Micro-Averaging works best when samples are homogeneous and efficiency is key, while Macro-Averaging is ideal when each sample should have equal influence. Random-Effects modeling offer a good balance of flexibility and interpretability, though may be less efficient than directly building an equivalent Fixed-Effects model when samples are similar. Bayesian approaches provide the most thorough uncertainty quantification but require more inductive biases and computational resources. For very large datasets (e.g. \u0026gt;1M instances) with limited computing power, consider using Variational Bayes or simplified meta-analytic methods. When unsure, the DerSimonian-Laird random-effects model with HKSJ variance adjustment can be a good default choice.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (March 2025). Estimating Statistical Properties in Grouped Measurements. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2023-03-01_stats-for-mil/.\nor add the following to your BibTeX file.\n@article{xiao2025stats-for-mil, title = \u0026#34;Estimating Statistical Properties in Grouped Measurements\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2025\u0026#34;, month = \u0026#34;March\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2023-03-01_stats-for-mil/\u0026#34; } References Glass, G. V. (1976). Primary, secondary, and meta-analysis of research. Educational researcher, 5(10), 3-8. DerSimonian, R., \u0026amp; Laird, N. (1986). Meta-analysis in clinical trials. Controlled clinical trials, 7(3), 177-188. Gelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Perfect Transportability describes a strong assumption that estimation results from any sample can be directly generalized to all other samples without adjustments $$ \\forall i,k\\ \\lim_{n_i,n_k \\to \\infty} (\\overline{x}_i - \\overline{x}_k) = 0. $$ This implies sample collection mechanisms and subpopulation characteristics do not systematically influence measurements. For example, you will see perfect transportability if all clinics draw from identical patient populations, factories use identical processes/materials during Manufacturing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe inverse-variance weighting scheme (w = 1/SE²) naturally assigns higher weights to more precise estimates - those with smaller within-sample variance and/or larger sample sizes. This approach combines heterogeneous estimates by giving more influence to more reliable measurements while downweighting less precise ones. For example, in clinical trials, larger studies with tighter confidence intervals would receive proportionally more weight than smaller studies with wider intervals.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAsymptotic approximations in this context refer to statistical properties that only hold as sample size approaches infinity. For example, the Central Limit Theorem guarantees sample means are normally distributed only for large enough samples. With small samples, these approximations become less reliable - normal distributions may not adequately describe uncertainty, and confidence intervals based on standard errors may not achieve their nominal coverage rates (e.g., a \u0026ldquo;95%\u0026rdquo; confidence interval may contain the true parameter less than 95% of the time).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jiajiexiao.github.io/posts/2025-03-01-stats-for-mil/","summary":"\u003ch2 id=\"intro\"\u003eIntro\u003c/h2\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;ll explore a statistical problem: estimating population means\nand their uncertainties from hierarchically structured data or so called grouped\nmeasurements. While averaging measurements may seem straightforward, the\npresence of natural groupings in data introduces important statistical\nconsiderations that require careful treatment.\u003c/p\u003e\n\u003cp\u003eTo illustrate the practical significance of this problem, let\u0026rsquo;s examine how\nhierarchically structured measurements - where individual observations are\nnaturally clustered into groups - arise across diverse real-world applications.\nMultiple-Instance Learning (MIL) represents an important machine learning\nparadigm specifically designed for analyzing such grouped or clustered data\nstructures. The following scenarios showcase a few situations where measurements\nnaturally organize into clusters of varying sizes:\u003c/p\u003e","title":"Estimating Statistical Properties in Grouped Measurements"},{"content":" 时间等不了人。\n大同，多好的一个人，看到了心目里少年自己希望成为的那样一个人：谦逊，幽 默，乐观，专情，坚韧，豁达，有才华，有愿景，聊到热爱的人和事眼睛会有光……\n这个世界每天都还在变化，好的坏的。桌上堆着的一份份一月进ER的账单，屏幕上一段一段 的代码，脑袋里chain of thought的推导。我们在老去，孩子在长大，还有很多很多想去却 没去尝试的想法。\n不早，晚安。让音乐延展记忆和生命，永远在一起。\nWell it’s useless to say Anymore It’s ok Nothing to say\nLet\u0026rsquo;s go back Let\u0026rsquo;s go back to the place Where I first saw your face\n昨天变成了今天 过去变成了现在 未来刚才来\n","permalink":"https://jiajiexiao.github.io/posts/2025-02-28-revisted-khalil-fong/","summary":"\u003cblockquote\u003e\n\u003cp\u003e时间等不了人。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e大同，多好的一个人，看到了心目里少年自己希望成为的那样一个人：谦逊，幽\n默，乐观，专情，坚韧，豁达，有才华，有愿景，聊到热爱的人和事眼睛会有光……\u003c/p\u003e","title":"回留 Revisted Khalil Fong"},{"content":" [Updated in Jan 2025]: Added HELM. [Updated in Dec 2024]: Added MethylGPT. In previous post, we discussed some of the introduction to Large Language Models (LLMs) and how they are constructed, trained, and utilized. Beginning with this post in the Biomedical LLMs series, we will explore their applications in biomedical domains. This post will concentrate on a few LLMs for genomics (e.g. DNA and RNA).\nDNA Language Models DNABERT DNABERT (Ji et al., 2021) is designed to encoder genomic DNA sequences by adapting the Bidirectional Encoder Representations from Transformers (BERT) model. DNABERT utilizes a Transformer\u0026rsquo;s encoder architecture characterized by attention mechanisms, which effectively capture both local and long-range dependencies in DNA sequences and offer contextual representation of the input DNA sequences. The encoder-only architecture is identical to the BERT base model, comprising 12 transformer layers, each with 768 hidden units and 12 attention heads.\nDNABERT employs a k-mer tokenization strategy to segment DNA sequences into overlapping k-mers, which provides a more coarse-grained and computationally cheaper way to capture the contextual information than base-by-base tokenization. The authors experimented with four k-mer sizes: 3, 4, 5, and 6. Each k-mer model (DNABERT-3, DNABERT-4, DNABERT-5, DNABERT-6) has a vocabulary size of 4^k+5 tokens, including special tokens such as [CLS] (classification), [PAD] (padding), [UNK] (unknown), [SEP] (separator), and [MASK] (masking). Each version captures different levels of contextual information, with DNABERT-6 generally performing the best due to its richer context representation. Moreover, learned positional embeddings (likely learns a simple lookup table via a linear layer) were used to add to the token embeddings.\nFig 1. Architecture and Key Features of the DNABERT Model. (a) Contextual understanding in RNN, CNN, and Transformer models: Comparison of how each model processes input tokens (T1-T5) to generate hidden states. RNNs use sequential information flow, CNNs focus on local context, while Transformers employ global self-attention mechanisms. (b) DNABERT\u0026rsquo;s structure: The model inputs tokenized k-mer sequences, including special tokens (CLS, SEP, MASK). This data passes through an embedding layer and 12 Transformer blocks. The model produces sentence-level classifications from the first output and token-level classifications from individual masked token outputs. (c) DNABERT\u0026rsquo;s versatility: The model uses general pre-training, allowing fine-tuning for various specific tasks with appropriate data. (d) DNABERT\u0026rsquo;s attention visualization: An example showing global attention patterns across 12 attention heads, demonstrating the model\u0026rsquo;s ability to focus on significant regions, such as known binding sites within a sequence.. The figure is Fig.1 from Ji et al., 2021.\nThe pretraining dataset for DNABERT was derived from the human genome, which has 3 billion bases. Sequences were generated through direct non-overlapping splitting and random sampling, with lengths varying between 5 and 510 bases. The pretraining process involved masking 15% of k-mers in the sequences for the first 100,000 steps and increasing this to 20% for the final 20,000 steps. The model was pretrained for 120,000 steps with a batch size of 2,000. The training process for DNABERT took approximately 25 days on 8 NVIDIA 2080Ti GPUs.\nDNABERT exhibits versatility across various genomic sequence-related tasks. It demonstrated state-of-the-art (SOTA) performance in 2021 for predicting promoters, splice sites, and transcription factor binding sites. By comparing the fine-tuned DNABERT pre-trained on the human genome with 78 mouse ENCODE ChIP-seq datasets against CNN, CNN+LSTM, CNN+GRU, and randomly initialized DNABERT models on cross-genome sequence prediction tasks, the authors show that the pre-trained DNABERT significantly outperforms all baselines. This suggests that pre-training not only enhances model performance by providing a better starting point for training but also enables robust transfer across species, even with considerable dissimilarities in non-coding regions, highlighting the model\u0026rsquo;s ability to capture shared deep semantic features across genomes. Furthermore, by visualizing the attention maps, DNABERT provides insights into the relative importance of different nucleotides and their semantic relationships within sequences. This interpretability demonstrates that deep learning models need not be \u0026ldquo;black boxes\u0026rdquo; - the model\u0026rsquo;s attention patterns can help identify conserved sequence motifs and predict functional genetic variants by highlighting biologically meaningful patterns in the data.\nDNABERT-2 DNABERT-2 (Zhou et al., 2023) is a refined genome foundation model designed to improve upon the limitations of its predecessor, DNABERT. DNABERT-2 is built upon the Transformer\u0026rsquo;s encoder architecture, similar to BERT and DNABERT, but incorporates a few key changes:\nDNABERT-2 adapts SentencePiece 1 (Kudo, 2018) with Byte Pair Encoding (BPE) 2 (Sennrich et al., 2015) to tokenize DNA sequences instead of using k-mer tokenization. BPE iteratively merges the most frequent co-occurring genome segments. This method overcomes the limitations of k-mer tokenization, such as information leakage 3 and poor computational efficiency, by providing a more streamlined and effective approach to sequence representation. This switch addresses computational inefficiencies and sample inefficiencies associated with k-mer tokenization. 8 vocabularies with target sizes ranging from 2^8 to 2^15 were tested for the BPE tokenization strategy. The best performing model was trained with a vocabulary size of 2^15, where larger vocabulary size means longer sequences but more sparse updates to the embedding layer. Evaluating different vocabulary sizes on a multi-species genome dataset on the Genome Understanding Evaluation (GUE) benchmark led the authors to use a vocabulary size of 2^12 = 4096 for training the final DNABERT-2 model. Except for replacing k-mer tokenization with BPE for better tokenization efficiency and sequence representation, DNABERT-2 abandons learned positional embeddings by integrating Attention with Linear Biases (ALiBi) (Press, et al, 2021) to overcome input length limitations. Unlike approaches that add positional embeddings to word embeddings, ALiBi biases query-key attention scores with a penalty proportional to the distance between tokens, enabling the model to extrapolate efficiently to sequences longer than those encountered during training. Additionally, DNABERT-2 also employs Flash Attention (Dao, et al., 2022) and Low Precision Layer Normalization to boost computational efficiency. This model comprises 117M parameters, making it substantially smaller yet more efficient than other models in its class. The pre-training of DNABERT-2 involved two main datasets: a human genome dataset consisting of 2.75 billion nucleotide bases, and a multi-species genome dataset comprising genomes from 135 species, totaling 32.49 billion nucleotide bases. This multi-species dataset ensures a broader representation and diversity of genetic sequences. Maximum sequence length was set to 510bps4 for DNABERT.\nThe pre-training of DNABERT-2 took approximately 14 days using eight Nvidia RTX 2080Ti GPUs. The training process employed a batch size of 4096 and a maximum sequence length of 128, optimized using the AdamW optimizer with specific hyperparameters.\nAs briefly mentioned earlier, DNABERT-2 introduces the Genome Understanding Evaluation (GUE) benchmark to evaluate DNEBERT-2. GUE includes 36 datasets across nine genome analysis tasks from four species. This benchmark enables a standardized assessment of model performance across various tasks such as promoter detection, transcription factor prediction, and species classification. While the input size of datasets in GUE ranges from 5000 to 10000, the authors splitted the input into 512bps segments and use the averaged embedding of each segment as the input to the model during their benchmarking. Also, the DNABERT-2 model is pre-trained purely on 700-bps sequences. Thanks to the extrapolation capability offered by ALiBi, the authors showed that such a setting still performs well even on 10000-bps sequences with a few epochs of fine-tuning.\nCompared to its predecessor DNABERT, DNABERT-2 achieves superior performance with significantly reduced computational cost and model size. In the authors evaluation, it performs on par with \u0026ldquo;SOTA\u0026rdquo; Nucleotide Transformers in 2023 while being 21 times smaller and requiring 92 times less GPU time for pre-training. This efficiency makes DNABERT-2 particularly suitable for fine-tuning on consumer GPUs.\nNucleotide Transformer The Nucleotide Transformer (NT) is a foundation model developed by InstaDeep in collaboration with Nvidia, designed for encoding genomic sequences (Dalla-Torre, et al., 2023). This model series, ranging from 50M to 2.5B parameters, has been developed over recent years with continuous improvements in training techniques and architectural enhancements.\nFig 2. Overview of the Nucleotide Transformer: pre-training, fine-tuning, analysis, and comparison of foundational models for genomics. (a,b) Illustration of the Nucleotide Transformer’s training process (a) and its application to downstream genomic prediction tasks via fine-tuning (b). Probing for downstream tasks follows a similar approach but excludes rescaling weights. (c) Comparative analysis of the Nucleotide Transformer models against other foundational genomics models, focusing on receptive field size, number of parameters, and performance across a benchmark comprising 18 curated downstream tasks. (d) Visualization of genomic features considered in downstream tasks. The figure is Fig.1 from Dalla-Torre, et al., 2023.\nNTs employ an encoder-only transformer architecture similar to BERT. They use non-overlapping 6-mer DNA tokens as a trade-off between sequence length (up to 6kb) and embedding size, achieving the highest performance compared to other token lengths. NT-v1 models use a learnable positional encoding layer that accepts a maximum of 1000 tokens 5. Each model includes transformer layers with layer normalization, multi-head self-attention, and a two-layer perceptron with GELU activations. The parameter sizes for NT-v1 models range from 500M to 2.5B.\nThe updated NT-v2 models feature architectural advancements such as rotary embeddings (Su, et al., 2024)6 and swiGLU activation (Shazeer, 2020). They eliminate MLP biases and dropout mechanisms to improve efficiency. NT-v2 models extend the context length to 12kb by accepting up to 2048 tokens and include variants with parameter sizes from 50M to 500M.\nNT models are pre-trained on diverse datasets to ensure comprehensive genomic representation:\nHuman Reference Genome: Based on the GRCh38/hg38 assembly, encompassing 3.2 billion nucleotides. 1000 Genomes Project (1000G): Comprising 3,202 high-coverage human genomes, representing 20.5 trillion nucleotides from 27 geographically diverse populations. Multispecies Dataset: Includes 850 genomes from various species, totaling 174 billion nucleotides, selected to maximize diversity and functional relevance across different phyla​​. The tokenization process converts nucleotide sequences into 6-mer tokens, with a vocabulary of 4104 tokens, including special tokens for padding [pad], masking [mask], and sequence start [CLS]. All nucleotides other than A, T, C, G were replaced by N before tokenization. The models are trained using a masked language modeling (MLM) approach similar to BERT, where 15% of the tokens in each sequence are masked, and the model learns to predict these masked tokens. Training utilizes the Adam optimizer with a learning rate schedule and gradient accumulation to handle large batch sizes effectively, with an effective batch size of 1M tokens per batch. Training is conducted on the Cambridge-1 Nvidia supercomputer, using 128 A100 GPUs across 16 nodes. NT-v1 models require up to 28 days for training, while NT-v2 models are trained for extended durations, with the largest models processing up to 1 trillion tokens to better understand the scaling laws (Kaplan, et al., 2020).\nNT models are evaluated based on their performance across various genomic tasks:\nScaling Laws: NT models, ranging from 50M to 2.5B parameters, were trained on extensive datasets. Larger models captured more complex genomic patterns, resulting in better generalization and accuracy. Architectural advancements in NT-v2 allowed smaller models to achieve results comparable to larger ones. These findings emphasize the importance of optimizing training techniques and extending context lengths to enhance model performance.\nBenchmarking: NT models were tested on 18 diverse genomic tasks, such as predicting epigenetic marks, chromatin profiles, and splice sites. Larger models consistently outperformed smaller ones, with the 2.5B parameter NT-v2 model showing superior accuracy in most tasks. The NT-v2 500M parameter model achieved similar performance to the 2.5B parameter model due to architectural improvements and longer training durations. Smaller models, like the 50M and 100M parameter versions, also performed well in less complex tasks or resource-limited environments. This benchmarking highlighted the trade-off between model size, computational efficiency, and task complexity.\nModel Selection: Selecting NT models depends on the specific downstream tasks. For tasks like predicting epigenetic marks and chromatin profiles, larger models (500M-2.5B parameters) are recommended due to their higher accuracy and ability to capture intricate dependencies. For splice site prediction, the NT-v2 500M model offers excellent performance comparable to SOTA models. For simpler tasks or when computational resources are limited, smaller models (50M-100M parameters) provide sufficient performance and efficiency. Understanding the specific requirements and constraints of the application is crucial in selecting the most appropriate NT model.\nThe NT models have shown significant improvements over existing benchmarks like DNABERT and Enformer (see below), particularly in tasks involving human genomic data.\nEnformer Enformer (Avsec, et al., 2021), developed by a team at DeepMind in collaboration with Calico Life Sciences and Google, leverages transformer architecture to predict gene expression and chromatin states from DNA sequences in humans and mice. While Enformer does not involve pre-training tasks like other large language models (LLMs), its adoption of a BERT-like architecture makes it highly relevant to this post.\nFig 3. Overview of the Enformer architecture. The model is trained to predict human and mouse genomic tracks at a 128-bp resolution using 200 kb of input DNA sequence. It replaces dilated convolutions with transformer modules, expanding its receptive field fivefold to detect elements up to 100 kb away, compared to 20 kb in Basenji2. Detailed architecture settings can be found in Extended Data Fig. 1 of the original paper (Avsec, et al., 2021), with a comparison to Basenji2. The figure was originally Fig. 1a but is linked through DeepMind’s corresponding blog post.\nEnformer combines deep convolutional neural networks with transformer blocks, significantly extending its receptive field. The model processes 196,608 bps of DNA sequence input and predicts 5,313 genomic tracks for humans and 1,643 for mice. It consists of seven convolutional layers followed by eleven transformer layers, using attention mechanisms to integrate information from distal genomic elements up to 100 kb away. This setup contrasts with previous SOTA models like Basenji2, which can only integrate information from up to 20 kb away.\nRegarding tokenization, input DNA sequences are one-hot encoded, with each nucleotide represented by a unique vector (A = [1,0,0,0], C = [0,1,0,0], G = [0,0,1,0], T = [0,0,0,1]). This encoding feeds into the convolutional layers, which reduce the spatial dimension, allowing the transformer layers to capture long-range interactions effectively. Moreover, the model employs custom relative positional encodings like Transformer-XL paper does (in short that is to add relative positional encodings $R_{ij}$ to the $q_i k_j^T$ ) to enhance its ability to distinguish between proximal and distal regulatory elements and to differentiate positions upstream and downstream of the transcription start site (TSS). This approach ensures effective integration of long-range genomic interactions, crucial for accurate gene expression prediction.\nEnformer was trained using a multitask learning framework on a vast dataset encompassing most of the human and mouse genomes. The training involved 34,021 human and 29,295 mouse sequences, with additional validation and test sets. The dataset included various genomic assays such as transcription factor (TF) chromatin immunoprecipitation and sequencing (ChIP-seq), histone modification ChIP-seq, DNase-seq, and ATAC-seq, providing a comprehensive set of genomic tracks.\nTraining was conducted on 64 TPU v3 cores over approximately three days, with optimization handled by the Adam optimizer. The model\u0026rsquo;s training and validation employed Poisson negative log-likelihood loss (same as Basenji2 did) 7, and data augmentation techniques like random shifting and reverse-complementing the input sequences were used to enhance robustness.\nEnformer demonstrated superior performance in gene expression prediction compared to Basenji2, with mean correlation improvements from 0.81 to 0.85 in predicting RNA expression at TSSs of human protein-coding genes. The model also showed enhanced ability to predict tissue-specific gene expression and the effects of genetic mutations on gene expression, validated by CRISPR interference assays and population eQTL studies.\nAlthough Enformer is not an LLM, its generic architecture and trained model make it versatile for various applications, including fine-mapping of human disease associations, understanding cis-regulatory evolution, and potentially designing synthetic enhancers for specific cell types. Its ability to predict regulatory activity from DNA sequence alone presents a significant advantage in genomic research.\nDespite its advancements, the authors acknowledged in their paper that Enformer is limited by its reliance on the cell types and assays present in its training data. It cannot generalize to new cell types or assays not included in its training set. Future improvements could involve integrating 3D genome organization data to better model genomic interactions and expanding training datasets to include more cell types and organisms. Additionally, advancements in computational efficiency and hardware could further enhance the model\u0026rsquo;s scalability and performance. Moreover, other studies suggest that Enformer may not present SOTA results in benchmarking. Methods like nucleotide transformers we mentioned previously can outperform Enformer although Enformer has a much larger receptive field.\nGPN The Genomic Pre-trained Network (GPN) is a language model designed for genome-wide variant effect prediction, leveraging unsupervised pretraining on genomic DNA sequences (Benegas, et al., 2023). It is particularly notable for its application to predicting the functional impact of genetic variants in Arabidopsis thaliana, a model organism for plant biology.\nGPN is based on a customized transformer-encoder where the traditional multi-head attention layers are replaced by convolutions. This design leverages the efficiency of convolutional networks in capturing local dependencies, which are prevalent in genomic sequences. The core of GPN consists of 25 convolutional blocks, each incorporating a dilated convolutional layer followed by a feed-forward layer, with intermediate residual connections and layer normalization. The model maintains a fixed embedding dimension of 512 across all layers. This architecture allows GPN to effectively model both local and long-range dependencies within genomic sequences. GPN was trained for 150,000 steps over four days using four NVIDIA A100 80 GB GPUs.\nFig 4. Overview of the Genomic Pre-trained Network. GPN predicts nucleotides at masked positions in a 512-bp DNA sequence. During training, 15% of positions are masked, while only the variant position is masked during variant effect prediction. The model uses a convolutional neural network to generate embeddings for each position and outputs nucleotide probabilities for masked positions. Training is performed with cross-entropy loss on the reference sequence, and the variant effect prediction score is the log-likelihood ratio between the alternate and reference alleles. Here, L represents the window length in base pairs, D denotes the embedding dimension, REF is the reference allele, and ALT is the alternate allele. The figure was Fig. 1 from Benegas, et al.,2023.\nUnlike models that utilize k-mers or byte-pair encoding for tokenization, GPN uses single-nucleotide tokens. This approach simplifies the interpretation of model outputs, which is crucial for variant effect prediction. The model does not incorporate explicit positional embeddings; instead, it leverages the convolutional layers\u0026rsquo; structure to capture positional information within the sequence. This design introduces translational equivariance as an inductive bias through the convolutional operations. While this assumption can be advantageous, it may also limit generalization when this bias does not align with the data.\nGPN was pretrained on unaligned reference genomes from Arabidopsis thaliana and seven related species within the Brassicales order. The training dataset included various genomic regions such as exons, promoters, and random genomic windows, ensuring a comprehensive representation of the genome. The model processes input DNA sequences of 512 base pairs, where 15% of the positions are masked during training. The goal is to predict the nucleotides at these masked positions, facilitating the learning of complex genomic features and structures. To address the overrepresentation of repetitive elements, the training loss was adjusted to down-weight these regions, improving the model\u0026rsquo;s performance on non-repetitive, functionally significant regions.\nGPN is designed to predict the effects of genetic variants across the genome, making it a powerful tool for genome-wide association studies (GWAS) and fine-mapping of causal variants. It outperforms traditional conservation scores like phyloP and phastCons in predicting variant effects in Arabidopsis thaliana.\nThe authors highlight that GPN\u0026rsquo;s ability to learn and predict gene structures and DNA motifs without any supervision. This capability is crucial for identifying transcription factor binding sites and other regulatory elements in the genome. Additionally, GPN\u0026rsquo;s predictions show a strong correlation with functional genomic regions, as evidenced by its high accuracy in distinguishing coding sequences, untranslated regions, and introns. This work should remind us that conventional CNN-based models can also work well sometimes.\nHyenaDNA Fig 5. Overview of HyenaDNA. The figure was from the GitHub profile page of the author\u0026rsquo;s repo for this work (link).\nHyenaDNA is a genomic foundation model designed to handle long-range dependencies in DNA sequences at single nucleotide resolution (Nguyen, et al., 2024). HyenaDNA addresses the limitations of previous Transformer-based genomic models that were constrained by the quadratic scaling of attention mechanisms. These earlier models could only handle contexts of up to 4k tokens, significantly limiting their ability to model long-range interactions in genomic sequences.\nHyenaDNA is based on the Hyena operator (Fig. 6), a convolutional model that can process long contexts with sub-quadratic time complexity. This architecture allows HyenaDNA to scale linearly with sequence length, enabling the modeling of up to 1 million tokens in a single context. The key components of the HyenaDNA architecture include:\nImplicit Convolutions: The Hyena operator uses long convolutions parameterized by a neural network, which are evaluated using Fast Fourier Transform (FFT) for efficient computation.\nElement-wise Gating: These gates modulate the input based on learned parameters, allowing the model to apply context-specific operations at each token position.\nSingle Nucleotide Tokenization: Unlike previous models that relied on k-mers or other aggregation techniques, HyenaDNA tokenizes DNA sequences at the single nucleotide level, preserving fine-grained information crucial for understanding genetic variations.\nFig 6. HyenaDNA Block. (left) The HyenaDNA block resembles a Transformer-decoder block, but with the attention mechanism replaced by a Hyena Operator. (middle) The Hyena Operator integrates long convolutions with element-wise gates, where the gates are derived from projections of the input using dense and short convolutional layers. (right) The long convolutions are parameterized implicitly through an MLP, which generates the weights for the long Hyena filters. The figure was from (https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna).\nHyenaDNA is a decoder-only model that was pretrained on the human reference genome using next-nucleotide prediction. This autoregressive training employed a sequence length warm-up technique, gradually increasing the context length to stabilize training and improve efficiency. The model demonstrated significant improvements in perplexity with longer contexts, indicating better prediction accuracy. The pretraining used a minimal DNA vocabulary consisting of \u0026lsquo;A\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, \u0026lsquo;G\u0026rsquo;, \u0026lsquo;T\u0026rsquo;, and special tokens.\nHyenaDNA was pretrained using Nvidia A100 GPUs (exact number was not disclosed by authors), with training times varying based on sequence length and model size. For example, training a model with a context length of 1 million tokens took approximately 4 weeks. The model sizes used in training ranged from 400k to 6.6M parameters, and it employed gradient checkpointing to manage memory usage efficiently during training.\nDespite being as an auto-regressive, decoder-only model for predicting the next nucleotide, HyenaDNA can be used in discriminative tasks such as classification. This adaptability is achieved through techniques like sequence-level pooling and soft prompting.\nIn sequence-level pooling, the outputs of HyenaDNA are aggregated across the genomic sequence to form a cohesive representation, which can then be funneled through a classification head to predict labels for the entire sequence. This method leverages the contextual information encoded by the model to classify sequences based on their entire compositional makeup.\nFor discriminative tasks, HyenaDNA can also leverage soft prompting (check relevant information in the first post of this series), which involves integrating trainable tokens directly into the input sequence. These tokens are optimized during training to adapt the model\u0026rsquo;s focus towards relevant features for the classification task, enabling effective in-context learning without extensive retraining of the model. This method seems elegant as it allows for flexible adaptation to new tasks by modifying only a small part of the model’s input, making HyenaDNA a powerful tool for genomic classifications and other discriminative tasks, while preserving computational efficiency and model integrity.\nIn authors\u0026rsquo; evaluation, HyenaDNA excels in various genomic tasks, including species classification and regulatory element identification. HyenaDNA achieved SOTA performance on 12 out of 18 benchmarks from the Nucleotide Transformer dataset, despite using significantly fewer parameters and pretraining data. On the GenomicBenchmarks dataset, HyenaDNA surpassed the SOTA on 7 out of 8 datasets, with notable improvements in enhancer identification.\nFig 7. Why single nuceotide resolution and long context are important. (a) DNA motifs play a crucial role in gene regulation. Variants in regulatory regions such as promoters, silencers, and enhancers influence the binding of transcription factors and RNA polymerases to DNA, ultimately affecting gene expression. (b) Single-nucleotide resolution is essential for precise variant analysis, but not all DNA sequences are relevant to gene expression. Relevant sequences may be located far apart within the genome, necessitating a long contextual span to capture their interactions effectively. The figure was adapted from (https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna).\nHyenaDNA offers significant advancements in genomic sequence modeling due to its ability to handle long-range contexts of up to 1 million tokens, which allows it to capture intricate dependencies across vast genomic sequences. This capability is particularly beneficial for tasks that require understanding long-range interactions, such as regulatory element identification and species classification. Additionally, the model\u0026rsquo;s use of single nucleotide tokenization ensures that every subtle genetic variation is preserved, enabling precise detection of single nucleotide polymorphisms (SNPs) and mutations. Check Fig.7 to better understand such biological significance further. Moreoever, the architecture, based on implicit convolutions, provides efficient computation with sub-quadratic scaling, making it more effective and faster than traditional Transformer-based models. This efficiency is further enhanced by techniques like gradient checkpointing and sequence length warm-up during training.\nDespite its innovations, the pretraining of HyenaDNA was conducted on a single human reference genome, which may limit the model\u0026rsquo;s generalizability to broader genomic datasets or to genomes from different species. Incorporating multiple genomes in the training process could enhance its robustness and applicability. Expanding the model\u0026rsquo;s application beyond DNA sequences to other biological sequences, such as proteins and RNA, also presents an area for future research and potential enhancement.\nEvo Evo is a cutting-edge DNA foundation model that generalizes to prediction tasks and generative design at scales ranging from molecular to whole genomes (Nguyen, et al., 2024). Developed by key contributors of HyenaDNA in collaboration with Stanford University, Arc Institute, and TogetherAI, Evo represents a significant advancement in leveraging machine learning for genomic data.\nFig 8. The Evo model architecture, based on StripedHyena. The figure was adapted from https://hazyresearch.stanford.edu/blog/2023-06-29-hyena-dna.\nEvo is built on the StripedHyena architecture, a hybrid model combining 29 layers of data-controlled convolutional operators with 3 layers of multi-head attention equipped with rotary position embeddings (RoPE). (Check Fig. 8 and reference about StripedHyena for detailed information about the archtecture.) This design enables efficient processing of long DNA sequences while maintaining single-nucleotide resolution. Evo scales up to 7B parameters and utilizes a context length of 131 kilobases (kb) at single-nucleotide resolution. Evo employs byte-level, single-nucleotide tokenization for input DNA sequences, allowing it to model sequences with fine granularity. During pretraining, Evo uses an effective vocabulary of four tokens (A, T, C, G) from a total vocabulary of 512 characters. The additional 508 characters beyond ATCG enable prompting with special tokens during generation with finetuned models. Meanwhile, RoPE aids in maintaining positional information over long contexts, crucial for genomic sequences where the relative position of nucleotides can impact biological function.\nEvo was pretrained on a vast dataset of 300 billion nucleotide tokens, encompassing 2.7 million prokaryotic and phage genomes from databases such as the Genome Taxonomy Database (GTDB), Integrated Microbial Genomes/Virus (IMG/VR), and Integrated Microbial Genomes/Plasmid (IMG/PR). The authors introduced OpenGenome in this work, compiling over 80,000 bacterial and archaeal genomes, and millions of predicted prokaryotic phage and plasmid sequences. The training followed a two-stage process: an initial phase with an 8k token context length, followed by a context extension phase to 131k tokens. Evo was trained using an autoregressive modeling approach, predicting the likelihood of the next token given a sequence of tokens. This next-token prediction task was purely on raw genome sequences with no explicit supervision or annotations, which is fundamental for learning how to capture complex patterns in DNA sequences. Evo employs a decoder-only framework, enabling it to efficiently handle long context lengths and maintain single-nucleotide resolution.\nFig 9. Evo models the fundamental modalities of biology as seen in the centra dogma of molecular biology. The figure was from https://arcinstitute.org/news/blog/evo, which is also Fig. 1a from Evo\u0026rsquo;s paper.\nEvo supports RNA and protein sequence modeling through its DNA outputs by leveraging the central dogma of molecular biology (Fig. 9). By modeling genome sequences at single-nucleotide resolution, Evo captures the information encoded in regulatory DNA, protein-coding DNA, coding RNA, and non-coding RNA, etc. It can learn covariation involving multiple genes and regulatory elements, making DNA a productive modality for developing a biological foundation model. However, it is important to note that Evo\u0026rsquo;s tokenization approach focuses on DNA sequences, and while it can model RNA-related functionality through their DNA counterparts, it does not directly accept RNA sequences as inputs. The training data primarily consists of DNA sequences, and any RNA-related functionality is derived from the model\u0026rsquo;s understanding of DNA sequences. This approach is also applied to protein sequences.\nTo support structure-related predictions, Evo\u0026rsquo;s generated DNA sequences can be processed using tools like ESMFold, AlphaFold2, and RNAMultiFold. These tools help predict the 3D structures of proteins and RNA sequences that Evo generates through its DNA-based learning.\nEvo excels in various applications:\nZero-shot Function Prediction: Evo outperforms domain-specific models (e.g. ESMs, NTs, RNA-FM) in predicting the effects of mutations on protein and ncRNA functions without task-specific finetuning. This capability is derived from its understanding of the DNA sequences encoding these molecules. Generative Design: Evo can generate synthetic CRISPR-Cas systems, including coherent protein and non-coding RNA sequences. This is achieved by generating DNA sequences that code for these molecules, which can then be transcribed and translated into functional RNA and proteins. Gene Essentiality Prediction: Using long genomic contexts, Evo accurately predicts essential genes in bacteria and phages. By analyzing the DNA sequences, Evo can determine which genes are crucial for an organism\u0026rsquo;s survival, providing insights into organismal fitness and potential targets for drug discovery. Whole-genome Sequence Generation: Evo generates sequences up to 650 kb, demonstrating high coding density and plausible genomic organization. These sequences can include coding regions for proteins and RNAs, enabling further studies on their structure and function using tools like ESMFold, AlphaFold2, and RNAMultiFold. The authors conducted a comprehensive scaling laws analysis, comparing over 300 models across four architectures: Transformer++, Mamba, Hyena, and StripedHyena. They found that StripedHyena demonstrates favorable scaling laws on DNA sequence data, outperforming other architectures, including state-of-the-art Transformers and modern data-controlled state-space models like Mamba. However, training Evo required substantial computational resources, including 64 Nvidia H100 GPUs and 128 Nvidia A100 GPUs. It was trained on ~340B tokens using ~$2 \\times 10^{22}$ FLOPS.\nEvo\u0026rsquo;s primary advantage lies in its ability to handle long genomic sequences at single-nucleotide resolution, enabling comprehensive modeling of biological systems. Its hybrid architecture ensures efficient processing and scalability, outperforming traditional Transformer models on genomic data. Currently trained exclusively on prokaryotic data, Evo’s predictions for eukaryotic sequences, including human genomes, remain limited. Expanding Evo to include eukaryotic genomes will require addressing the much higher complexity of these genomes, along with significant investments in engineering, computational resources, and safety-related model alignment. While Evo is capable of generating broad genomic organization, further improvements are needed in the detail and completeness of generated sequences. Additionally, the model’s substantial computational demands during both training and inference could limit accessibility for smaller research groups.\nMethylation-awared Language Models MethylBert MethylBERT (Jeong, et al. 2023) is a deep learning model designed for read-level DNA methylation pattern classification and tumour purity estimation. Built on the DNABert architecture, it processes overlapping 3-mer DNA sequences (comprising 69 unique tokens, including DNA bases A, T, C, G and special tokens like [MASK], [UNK]) combined with methylation states to encode read-level methylomes. Unlike traditional methods that rely on microarray-based beta-values, MethylBERT directly analyzes sequencing-based methylation data, preserving single-molecule signals and improving sensitivity for detecting rare cell types. The model\u0026rsquo;s primary goal is to classify sequencing reads as either tumor or normal cells, facilitating tumor fraction estimation in bulk DNA methylation samples.\nFig 10. Overview of MethylBERT. The figure is Fig. 1 from its paper.\nMethylBERT\u0026rsquo;s training process consists of two major stages (Fig. 10). During the pre-training stage, the model employs an unsupervised Masked Language Model (MLM) objective on the hg19 genome (or mouse mm10 for cross-species applications). It learns to predict missing 3-mer DNA sequences using contextual embeddings, processing DNA sequences in 510 bp segments. Through this process, the model develops a deep understanding of sequence dependencies, CpG distributions, and genome-wide structures.\nIn the fine-tuning stage, MethylBERT focuses on specialized read-level methylation classification using supervised learning with tissue-derived data, such as Diffuse Large B-cell Lymphoma and Non-neoplastic B-cells. The model classifies reads based on CpG methylation patterns and genomic context, processing methylation states in a categorical format where 0 represents unmethylated CpG (unmodified cytosine), 1 indicates methylated CpG (cytosine with methyl group), and 2 denotes non-CpG sites (other nucleotides, not used in classification). The model converts these methylation states into embedding vectors and combines them with DNA sequence embeddings for comprehensive representation.\nFor final predictions, MethylBERT follows a three-step process (Fig. 10): first, it classifies each read as tumor-derived or normal using model embeddings; second, it incorporates additional genomic region information such as DMR labels; and finally, it estimates tumor purity in bulk samples using Bayesian probability inversion and maximum likelihood estimation.\nThe model\u0026rsquo;s architecture mirrors DNABert, featuring 12 Transformer encoder layers with 768 hidden units and 12 attention heads. Additionally, the authors developed a smaller variant with 6 encoder layers optimized for longer read sequences (500 bp), which achieved comparable performance to the larger model.\nThe computational requirements for MethylBERT varied significantly between training stages. The pre-training phase required several days using 4 NVIDIA V100 GPUs, while the subsequent fine-tuning phase completed much more quickly.\nMethylBERT exhibited remarkable capabilities in analyzing complex methylation patterns, significantly outperforming traditional statistical approaches like CancerDetector and Hidden Markov Models (HMM). Its particular strength lies in read-level DNA methylation analysis, demonstrating high accuracy and sensitivity. These results highlight MethylBERT\u0026rsquo;s potential as a powerful tool for liquid biopsy applications, where precise methylation pattern detection is crucial for early disease detection and monitoring.\nMethylGPT Unlike MethylBert, which is a BERT-like encoder model that embeds full sequences with methylation status, MethylGPT is a decoder-only generative model designed to analyze DNA methylation patterns across large numbers of CpG sites (Ying, et al. 2024). The model was trained on a large-scale dataset of 154,063 human methylation profiles, capturing 49,156 physiologically relevant CpG sites and processing a total of 7.6 billion training tokens. Unlike conventional linear models, MethylGPT leverages deep learning to recognize both local and higher-order genomic features, enabling robust predictions in various epigenetic applications.\nFig 11. Overview of MethylGPT. The figure is Fig. 1a from its paper.\nMethylGPT employs a transformer architecture consisting of six transformer blocks, with each block containing four multi-head self-attention layers followed by a feed-forward network. The model\u0026rsquo;s tokenization strategy involves two key components: CpG site positions and methylation states. Each CpG site is assigned a unique integer identifier for position encoding, while methylation states are embedded separately. These two types of embeddings are combined through an element-wise operation before being processed by the transformer layers. This dual embedding approach enables the model to simultaneously capture both the genomic context of CpG sites and their methylation states through the attention mechanism. The model uses a 64-dimensional embedding space for representation learning, and includes a special [CLS] token at the beginning of each sequence to generate sample-level representations for downstream tasks.\nDuring pretraining, the model was optimized using two complementary loss functions:\nMasked Language Modeling (MLM): Predicts methylation levels for 30% randomly masked CpG sites. Profile Reconstruction Loss: Uses the [CLS] token to reconstruct full DNA methylation profiles. The training dataset was curated from the EWAS Data Hub and Clockbase, covering over 20 tissue types. The selection of 49,156 CpG sites was based on their association with Epigenome-Wide Association Study (EWAS) traits and their prevalence across datasets. Methylation values were normalized, and missing values were handled through masking during training.\nThe training process proceeded for 10 epochs using the AdamW optimizer, starting with a 0.001 learning rate and implementing a 10% per-epoch decay. The implementation utilized NVIDIA A100 GPUs with a batch size of 16 and incorporated FlashAttention for efficient memory usage.\nMethylGPT was shown to generalizes well across epigenetic tasks, including methylation imputation, age prediction, missing data handling, and intervention analysis. It excels in biological interpretability, scalability, and resilience to missing data, outperforming traditional models in accuracy. However, its high computational cost and training on bulk methylation data may limit generalization to single-cell methylation data. Furthermore, the model only considers CpG sites rather than incorporating the full sequence context, which may limit its ability to efficiently capture sequence-dependent patterns during both training and inference.\nRNA Language Models RiNALMo RiNALMo (RiboNucleic Acid Language Model) (Penić, et al. 2024) is a RNA language model designed to understand and predict RNA structures and functions. It employs a BERT-style Transformer encoder architecture, which consists of 33 Transformer blocks with an embedding dimension of 1280. Each block includes multi-head attention (20 heads) and feed-forward networks, utilizing advanced techniques such as rotary positional embeddings (RoPE) and the SwiGLU activation function. RiNALMo has a total of 650M parameters.\nThe model tokenizes RNA sequences by treating each nucleotide as a single token. During preprocessing, all instances of uracil (U) in the sequences are replaced with thymine (T). The resulting vocabulary includes primary nucleotides (A, C, G, T) and various ambiguous nucleotide combinations and special tokens. These ambiguous combinations cover multiple nucleotide possibilities and are used to handle sequences where the exact nucleotide is not known or where multiple nucleotides are possible due to sequencing uncertainties or biological variability:\nSymbol Nucleotides Represented R A or G Y C or U K G or U M A or C S G or C W A or U B C, G, or U D A, G, or U H A, C, or U V A, C, or G N Any nucleotide (A, C, G, U) Additional special tokens used in the model include [CLS], [EOS], [PAD], and [MASK]. The positional information of tokens is encoded using RoPE, which captures both relative and absolute positional data, enhancing the model\u0026rsquo;s ability to learn sequence relationships effectively.\nRiNALMo was pre-trained on a dataset of 36 million non-coding RNA (ncRNA) sequences curated from multiple sources including RNAcentral, Rfam, and Ensembl. The model\u0026rsquo;s pre-training involved a masked language modeling (MLM) task, where 15% of the tokens in the input sequences were masked and the model was trained to predict these masked tokens. This approach helps the model learn the underlying structure and function of RNA sequences from vast amounts of unannotated data.\nThe pre-training of RiNALMo was conducted over six epochs on a cluster of seven A100 GPUs, each with 80 GB of memory. The training utilized a batch size of 192 per GPU and employed a cosine annealing learning rate schedule with a linear warm-up. The training process involved intensive computational resources to handle the large-scale data and model parameters efficiently.\nRiNALMo has demonstrated good performance across various RNA-related tasks. Key applications include:\nSecondary Structure Prediction: RiNALMo\u0026rsquo;s embeddings were fine-tuned for predicting RNA secondary structures, exhibiting superior generalization capabilities compared to existing deep learning methods. It was able to generalize well on unseen RNA families, a significant advancement over traditional models.\nSplice-Site Prediction: The model was fine-tuned to predict splice sites in RNA sequences, outperforming other RNA language models (e.g. RNA-FM) and established methods. RiNALMo\u0026rsquo;s ability to generalize across different species\u0026rsquo; RNA sequences underscores its robustness and versatility.\nMean Ribosome Loading (MRL) Prediction: RiNALMo was also fine-tuned to predict MRL values for mRNA sequences, showing superior performance and generalization on human UTRs despite being trained on random sequences.\nRiNALMo\u0026rsquo;s primary advantage lies in its large-scale pre-training on diverse RNA sequences, which equips it with powerful representations that can be effectively utilized across various downstream tasks.\nWhile RiNALMo excels in secondary structure prediction, it struggles with specific RNA families, such as telomerase RNAs. Overall, RiNALMo is a RNA embedding modeling, providing a powerful tool for RNA structure and function prediction tasks. Its ability to generalize across different RNA families and its superior performance on a range of downstream tasks highlight its potential to drive forward our understanding of RNA biology and its applications in biomedical research.\nRNAErnie RNAErnie (Wang, et al. 2024) is a RNA language model that integrates biological knowledge through RNA motifs (Fig. 11). The model employs a transformer-based architecture consisting of 12 layers of multihead transformer blocks, with each layer maintaining a hidden state dimension of 768. At its core, RNAErnie builds upon the Enhanced Representation through Knowledge Integration (ERNIE) framework, which systematically incorporates external domain knowledge during the pretraining process. This knowledge integration enables the model to capture and represent intricate biological relationships within RNA sequences more effectively. Specifically, RNAErnie leverages RNA motifs and RNA type information as biological priors, significantly enhancing its capabilities across various RNA analysis tasks.\nFig 11.Illustration of RNAErnie. The model uses 12 transformer encoder layers and employs motif-aware pretraining on 23M RNAcentral sequences. It then performs type-guided fine-tuning by predicting RNA types and using them as auxiliary information for downstream tasks. Fig. 1 from original paper.\nDuring pretraining, RNAErnie employs a sophisticated motif-aware multilevel masking strategy that incorporates biological knowledge (Fig. 12a). This hierarchical masking approach consists of:\nBase-level Masking: Randomly masks 15% of nucleobases within an RNA sequence, aiding in learning fundamental token representations. Subsequence-level Masking: Masks contiguous segments of nucleobases, ranging from 4 to 8 bases, to capture deeper biological information. Motif-level Masking: Incorporates biologically significant motifs from databases like ATtRACT and SpliceAid, masking these motifs to embed complex structural and functional elements within the model. Fig 12. RNAErnie\u0026rsquo;s training strategies. a, Motif-aware masking with three levels (base, subsequence, motif) during pretraining. b, Type-guided fine-tuning predicts RNA types first, then uses ensemble learning with shared parameters for downstream tasks.\nRNAErnie utilizes a type-guided fine-tuning approach, leveraging predicted RNA types as auxiliary information (Fig. 12b). This strategy employs three neural architectures:\nFBTH (Frozen Backbone with Trainable Head): Extracts embeddings from the pretrained RNAErnie block and uses them to train a separate task-specific head. TBTH (Trainable Backbone with Trainable Head): Combines the RNAErnie block with task-specific heads into an end-to-end neural network for supervised learning tasks. STACK: Uses the RNAErnie block to predict the top-K possible RNA types, followed by ensemble learning through multiple downstream modules. RNAErnie has approximately 105M trainable parameters and processes RNA sequences by tokenizing the bases \u0026lsquo;A\u0026rsquo;, \u0026lsquo;U\u0026rsquo;, \u0026lsquo;C\u0026rsquo;, and \u0026lsquo;G\u0026rsquo;. Each sequence is appended with an initial classification embedding ([CLS]) and an indication embedding ([IND]), which helps cluster similar RNA sequences in a latent space for more effective retrieval-based learning. The model accepts input sequences up to 512 nucleotides in length, though this limitation may pose challenges when analyzing complex three-dimensional RNA structural motifs.\nThe model was pretrained on approximately 23 million RNA sequences from RNAcentral. Training was conducted on four Nvidia Tesla V100 32GB GPUs over approximately 250 hours, using the AdamW optimizer with a learning rate scheduler that incorporates anneal warm-up and decay techniques. The initial learning rate was set to 1×10^-4.\nRNAErnie demonstrates robust performance across various RNA analytical tasks in both supervised and unsupervised learning scenarios. The model has been successfully evaluated on RNA sequence classification, RNA-RNA interaction prediction, and RNA secondary structure prediction tasks. Its effectiveness is particularly evident in RNA sequence classification on the nRC dataset, where RNAErnie+ achieved an impressive 96.88% accuracy, significantly outperforming baseline models (e.g. RNABERT, RNA-MSM and RNA-FM).\nHELM HELM (Hierarchical Encoding for mRNA Language Modeling) (Yazdani-Jahromi, et al. 2024) is a new LLM pretraining method specifically designed for analyzing messenger RNA (mRNA) sequences. HELM introduces a pretraining strategy that explicitly incorporates the biological hierarchy of mRNA sequences, particularly at the codon level. This approach better reflects the inherent structure of genetic information, where three nucleotides form a codon that codes for a specific amino acid. By aligning the model architecture and training process with these fundamental biological principles, HELM aims to achieve more accurate and biologically meaningful sequence analysis.\nFig 13.Hierarchical codon-aware tokenization and loss function in HELM. Left: The hierarchical structure of codons used in HELM for tokenization and modeling. Codons are categorized into Start, Coding (grouped by amino acids), and Stop codons. This biologically informed hierarchy influences the training loss function by prioritizing synonymous relationships between codons. Right: Codon prediction probabilities visualized on an amino acid codon wheel. Orange bars represent HELM’s Hierarchical Cross-Entropy (HXE) loss, while blue bars correspond to the standard Cross-Entropy (XE) loss. HELM assigns higher probabilities to synonymous codons when making predictions, better capturing biological redundancy, whereas XE tends to misassign probability to non-synonymous codons.\nHELM proposes to adopt Hierarchical Cross-Entropy Loss (HXE, see eq1) to better capture synonymous codon relationships.\n\\begin{equation} \\begin{aligned} L_{HXE} = -\\sum_{i} w_i \\cdot y_i \\log \\hat{y}_i \\end{aligned} \\end{equation}\nwhere:\n$y_i$ is the true one-hot label, $\\hat{y}_i$ is the predicted probability for codon $i$, $w_i$ is a weight assigned based on codon similarity based on $\\lambda (C) = exp(-\\alpha h(C))$, and $h(C)$ is the height of the node C in the hierarchy and ($\\alpha \u0026gt; 0$). Unlike standard cross-entropy loss which treats all prediction errors equally, HXE implements a structured hierarchy where prediction errors are penalized differently based on their biological significance within the codon structure. In another words, This encourages the model to prefer synonymous codons over random codon assignments, improving biological plausibility in sequence predictions.\nThe pretraining dataset consists of 15.3 million curated mRNA sequences from the Observed Antibody Space (OAS) database, which includes sequences from over 80 studies. The model was trained using 8 NVIDIA A100 GPUs over 40 epochs. HELM is implemented using multiple architectures, including Transformer, Mamba, and Hyena models, each with 50M parameters.\nKey technical features include:\nCodon-level tokenization strategy (64 codons plus special tokens) Hierarchical Cross-Entropy Loss (HXE) for biologically-informed error weighting Support for both Masked Language Modeling (MLM) and Causal Language Modeling (CLM) training objectives HELM has demonstrated superior performance than a few baseline methods (e.g. RNA-FM, SpliceBERT, and CodonBERT) in several critical areas:\nAccurate prediction of mRNA properties including protein expression and thermostability Precise annotation of antibody regions High-quality mRNA sequence generation In comparative evaluations, HELM achieved significant improvements:\n8% performance gain over standard bio-language models 2x parameter efficiency compared to state-of-the-art models like RNA-FM and CodonBERT Enhanced biological plausibility in generated sequences Improved accuracy in property prediction tasks through better capture of codon hierarchies The model shows particular promise for therapeutic applications, especially in vaccine development and gene therapy. However, authors also acknowledged the following limitations:\nTraining in Euclidean space may not optimally capture hierarchical relationships compared to hyperbolic space The specialized tokenization and pretraining approaches may present integration challenges with existing bioinformatics pipelines Overall, HELM demonstrates how incorporating domain-specific biological knowledge into both data preprocessing and model training can significantly enhance LLM performance. By explicitly modeling the hierarchical nature of codons and using biologically-informed loss functions, HELM achieves better results with fewer parameters compared to conventional approaches. This suggests a promising direction for developing more efficient and biologically meaningful language models in genomics.\nGene Language Models scGPT scGPT (Cui, et al. 2024) is a generative pre-trained transformer model specifically designed for analyzing single-cell RNA sequencing (scRNA-seq) and multi-omics data (Fig. 14). Built on a decoder-only transformer architecture, scGPT processes gene expression data by treating individual genes as tokens and converting each cell\u0026rsquo;s expression profile into a sequence. This approach allows the model to capture complex relationships between genes, cells, and tissues, learning intricate biological patterns in a manner analogous to how traditional GPT models learn linguistic structures.\nFig 14. Overview of scGPT.\nscGPT employs a sophisticated tokenization strategy:\nGene names serve as the primary vocabulary tokens Expression values undergo pre-defined binning transformations to standardize data across different sequencing modalities Condition tokens capture important metadata such as batch identity for technical variation tracking, modality type (e.g., RNA, ATAC-seq, proteomics) and perturbation conditions (e.g., drug treatments, CRISPR knockouts). These condition tokens are crucial for integrating data across different experiments, enabling scGPT to generalize beyond individual datasets and infer biological relationships across diverse conditions. As a fundatational model, scGPT was pre-trained on an extensive dataset of 33 million single-cell RNA sequencing profiles from the CELLxGENE repository. This dataset encompasses 51 organs and 441 studies, providing broad tissue diversity across healthy human cells. The comprehensive training data enables zero-shot generalization to disease-related tasks, as demonstrated in studies involving multiple sclerosis (MS) and cancer datasets.\nThe pre-training process utilizes a comprehensive self-supervised learning approach with four main tasks:\nMasked Gene Prediction: Similar to BERT\u0026rsquo;s masked language modeling, the model predicts expression values for randomly masked gene tokens based on surrounding context.\nGene Expression Imputation: The model learns to reconstruct missing gene expression values, addressing the common challenge of sparse data in single-cell datasets.\nModality-Aware Pre-training: Integration of RNA, ATAC-seq, and proteomics data to develop unified multi-omic representations.\nBatch Effect Reduction: Implementation of contrastive learning techniques to minimize technical variations while preserving biological signals.\nThe pre-training process leverages high-performance A100 GPU clusters and employs FlashAttention for optimized self-attention computations.\nAfter pre-training, scGPT demonstrates remarkable versatility through fine-tuning for various downstream tasks:\nCell-type annotation Batch effect correction Multi-omic data integration Perturbation response prediction Gene Regulatory Network (GRN) inference This adaptability makes scGPT a powerful tool for diverse applications in single-cell genomics research. However, like most LLMs, scGPT faces certain limitations like biases in the training data. Since it is pretrained primarily on publicly available single-cell datasets, rare or underrepresented cell types may not be adequately captured in the model\u0026rsquo;s learned representations. This can potentially impact the model\u0026rsquo;s performance when analyzing low-abundance cell populations or rare cell states, highlighting the importance of considering dataset bias in model applications.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (May 2023). Biomedical LLMs (2): Genomics. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2024-05-12_biollm_genomics/.\nor add the following to your BibTeX file.\n@article{xiao2024_biollm_genomics, title = \u0026#34;Biomedical LLMs (2): Genomics\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2024-05-12_biollm_genomics/\u0026#34; } References Ji, Y., Zhou, Z., Liu, H., \u0026amp; Davuluri, R. V. (2021). DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome. Bioinformatics, 37(15), 2112-2120.\nZhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R., \u0026amp; Liu, H. (2023). Dnabert-2: Efficient foundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006.\nKudo, T., \u0026amp; Richardson, J. (2018). Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\nSennrich, R., Haddow, B., \u0026amp; Birch, A. (2015). Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\nPress, O., Smith, N. A., \u0026amp; Lewis, M. (2021). Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., \u0026amp; Ré, C. F. fast and memory-efficient exact attention with IO-awareness. arXiv; 2022. arXiv preprint arXiv:2205.14135.\nDalla-Torre, H., Gonzalez, L., Mendoza-Revilla, J., Carranza, N. L., Grzywaczewski, A. H., Oteri, F., \u0026hellip; \u0026amp; Pierrot, T. (2023). The nucleotide transformer: Building and evaluating robust foundation models for human genomics. BioRxiv, 2023-01.\nSu, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., \u0026amp; Liu, Y. (2024). Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568, 127063.\nShazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., \u0026hellip; \u0026amp; Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.\nAvsec, Ž., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-Barwinska, A., Taylor, K. R., \u0026hellip; \u0026amp; Kelley, D. R. (2021). Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10), 1196-1203.\nBenegas, G., Batra, S. S., \u0026amp; Song, Y. S. (2023). DNA language models are powerful predictors of genome-wide variant effects. Proceedings of the National Academy of Sciences, 120(44), e2311219120.\nNguyen, E., Poli, M., Faizi, M., Thomas, A., Wornow, M., Birch-Sykes, C., \u0026hellip; \u0026amp; Baccus, S. (2024). Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Advances in neural information processing systems, 36.\nJeong, Y., Gerhäuser, C., Sauter, G., Schlomm, T., Rohr, K., \u0026amp; Lutsik, P. (2023). MethylBERT: A Transformer-based model for read-level DNA methylation pattern identification and tumour deconvolution. bioRxiv, 2023-10.\nYing, K., Song, J., Cui, H., Zhang, Y., Li, S., Chen, X., \u0026hellip; \u0026amp; Gladyshev, V. N. (2024). MethylGPT: a foundation model for the DNA methylome. bioRxiv, 2024-10.\nZvyagin, M., Brace, A., Hippe, K., Deng, Y., Zhang, B., Bohorquez, C. O., \u0026hellip; \u0026amp; Ramanathan, A. (2023). GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics. The International Journal of High Performance Computing Applications, 37(6), 683-705.\nPenić, R. J., Vlašić, T., Huber, R. G., Wan, Y., \u0026amp; Šikić, M. (2024). Rinalmo: General-purpose rna language models can generalize well on structure prediction tasks. arXiv preprint arXiv:2403.00043.\nWang, N., Bian, J., Li, Y., Li, X., Mumtaz, S., Kong, L., \u0026amp; Xiong, H. (2024). Multi-purpose RNA language modelling with motif-aware pretraining and type-guided fine-tuning. Nature Machine Intelligence, 1-10.\nYazdani-Jahromi, M., Prakash, M., Mansi, T., Moskalev, A., \u0026amp; Liao, R. (2024). HELM: Hierarchical Encoding for mRNA Language Modeling. arXiv preprint arXiv:2410.12459.\nCui, H., Wang, C., Maan, H., Pang, K., Luo, F., Duan, N., \u0026amp; Wang, B. (2024). scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nature Methods, 1-11.\nSentencePiece is a tokenization method that segments text into smaller units such as subwords or characters, allowing for efficient handling of various languages and scripts. It employs techniques like Byte-Pair Encoding (BPE) or unigram language model to create a unified vocabulary, facilitating better representation of rare or out-of-vocabulary words.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nByte-Pair Encoding (BPE) tokenization is a data compression algorithm that constructs tokens by iteratively merging the most frequent (statistics from the training corpus) pair of consecutive bytes or characters in a corpus to create new tokens, effectively reducing the vocabulary size while preserving meaningful units. This process continues until a predefined vocabulary size or iteration limit is reached, resulting in a compact representation of the original text data. More reading materials and examples can be found from Huggingface.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInformation leakage could happen if one generates k-mer vocabulary by checking all corpus including test data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe DNABERT-2 paper says the input limit for DNABERT is 512bps.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEach token has 6bps. Therefore, 1k tokens lead to 6kb sequence length.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRoPE encodes absolute positions using a rotation matrix while incorporating relative position dependencies directly into the self-attention mechanism. This approach allows models to handle sequences of different lengths and gradually reduces the influence of distant tokens. RoPE is especially beneficial for extrapolating to longer sequences than those seen during training and has been widely adopted in modern transformer models. Additionally, It also integrates seamlessly with linear self-attention architectures, enabling efficient positional encoding even in models optimized for large-scale processing of long sequences.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe Poisson negative log-likelihood loss is a loss function often used when modeling count data, such as predicting the number of occurrences of an event. This loss is particularly suitable for data that follows a Poisson distribution: $P(k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$, where the rate parameter $\\lambda$ equals both mean and variance of the distribution. Corresponding loss is $\\text{PoissonNLLLoss} = -\\log(P(k|\\lambda)) = \\lambda\nk\\log(\\lambda) + \\log(k!)$, where $k$ is the target and $\\lambda$ is the expected count that the model would like to predict. Pytorch has provided a built-in loss for this here. \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://jiajiexiao.github.io/posts/2024-05-12_biollm_genomics/","summary":"\u003c!-- https://hybridna-project.github.io/HybriDNA-Project/ hybriDNA --\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cspan style=\"color: lightblue\"\u003e[Updated in Jan 2025]: Added \u003ca href=\"#helm\"\u003eHELM\u003c/a\u003e.\u003c/span\u003e\u003c/li\u003e\n\u003cli\u003e\u003cspan style=\"color: lightblue\"\u003e[Updated in Dec 2024]: Added \u003ca href=\"#methylgpt\"\u003eMethylGPT\u003c/a\u003e.\u003c/span\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn \u003ca href=\"https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/\"\u003eprevious post\u003c/a\u003e,\nwe discussed some of the introduction to Large Language Models (LLMs) and how\nthey are constructed, trained, and utilized. Beginning with this post in the\nBiomedical LLMs series, we will explore their applications in biomedical\ndomains. This post will concentrate on a few LLMs for genomics (e.g. DNA and\nRNA).\u003c/p\u003e\n\u003ch2 id=\"dna-language-models\"\u003eDNA Language Models\u003c/h2\u003e\n\u003ch3 id=\"dnabert\"\u003eDNABERT\u003c/h3\u003e\n\u003cp\u003eDNABERT (\u003ca href=\"#Ji2021\"\u003eJi et al., 2021\u003c/a\u003e) is designed to encoder genomic DNA\nsequences by adapting the Bidirectional Encoder Representations from\nTransformers (BERT) model. DNABERT utilizes a Transformer\u0026rsquo;s encoder architecture\ncharacterized by attention mechanisms, which effectively capture both local and\nlong-range dependencies in DNA sequences and offer contextual representation of\nthe input DNA sequences. The encoder-only architecture is identical to the BERT\nbase model, comprising 12 transformer layers, each with 768 hidden units and 12\nattention heads.\u003c/p\u003e","title":"Biomedical LLMs (2): Genomics"},{"content":"The rapid advancements in Natural Language Processing (NLP) have showcased the versatility and efficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in compressing vast amounts of information through unsupervised or self-supervised training, enabling impressive few-shot and zero-shot learning performance. These attributes make LLMs particularly attractive for domains where generating extensive task-specific datasets is challenging, such as in biomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising results, highlighting their potential to address complex problems where data scarcity is a significant barrier. Starting from this post, I am planning to write a series on Biomedical LLMs.\n1. General Introduction to LLMs LLMs are deep learning models designed to understand and generate human language. They leverage vast datasets to learn the statistical properties of language, allowing them to generate coherent and contextually appropriate text. The development of LLMs has been significantly influenced by the introduction of the Transformer architecture (Fig. 1) by Vaswani et al. (2017), which enabled models to efficiently (relatively speaking) capture long-range dependencies in text through self-attention mechanisms. Subsequent models such as BERT (Bidirectional Encoder Representations from Transformers) by Devlin et al. (2019) and GPT (Generative Pre-trained Transformer) by Radford et al. (2018) have set new benchmarks in NLP, demonstrating state-of-the-art performance across various tasks.\nFig 1. Transformer Architecture.. The nice illustration is from D2l.ai.\n2. Training Large Language Models The training of LLMs involves multiple methodologies designed to enhance their language understanding and generation capabilities. The primary training schemes include:\n2.1 Autoregressive Language Modeling (ALM) Autoregressive language modeling involves training the model to predict the next word in a sequence based on the preceding words:\n\\begin{equation} \\begin{aligned} \\pi_\\theta (\\mathbf{y} \\mid \\mathbf{x}) = \\prod_{t} \\pi_\\theta (y_t \\mid \\mathbf{x}, \\mathbf{y}_{\u0026lt;t}), \\end{aligned} \\end{equation}\nwhere $y_t$ is the $t^{\\text{th}}$ token in the response and $y_{\u0026lt;t}$ is tokens in the response before $y_t$, $x$ are optional inputs as conditional generation and $\\pi_\\theta$ is the policy parameterized by $\\theta$ for the LLM model.\nThis approach, utilized by GPT models, enables the generation of text in a sequential manner, ensuring coherence and contextual relevance (Radford et al. 2018). This approach is more and more popular these days. One of the reasons for this trend is not just for the generative capabilities of LLMs, but also because of the belief/hypothesis that the model must be capable understand the language of the world so it can generate coherent and contextually appropriate text. This reminds me of a quote from Richard Feynman on his blackboard at the time of death:\n\u0026ldquo;What I cannot create, I do not understand. \u0026quot; \u0026mdash; Richard P. Feynman\nMoreover, while ALM typically aim to predict the next word in a sequence, a recent study has explored alternative ALM variants that predict multiple subsequent tokens simultaneously (Gloeckle et al. 2024). This Multi-token Prediction approach has demonstrated improved performance, including faster inference and enhanced reasoning capabilities. For more details on these advancements and solutions to the high memory usage issues associated with multiple token prediction, please refer to Gloeckle et al. 2024.\n2.2 Masked Language Modeling (MLM) Masked language modeling, employed by models such as BERT, involves masking certain words in a sentence and training the model to predict these masked words based on the surrounding context.\n\\begin{equation} \\begin{aligned} p(y_{\\text{masked}} \\mid y_{\\text{unmasked}}) = g \\circ f(y_{\\text{unmasked}}), \\end{aligned} \\end{equation}\nwhere $y$ are the tokens, $f$ represents the contextualized embeddings of the unmasked tokens and $g$ is linear probing followed by a softmax operation that returns the probabilities of masked tokens being particular tokens in the context, i.e. $p(y_{\\text{masked}} \\mid y_{\\text{unmasked}})$.\nThis fill-in-the-blank Cloze task in a bidirectional training fashion allows the model to capture contextualized representation of the inputs, leading to improved performance on a range of NLP tasks (Devlin et al., 2019).\n2.3 Other Training Schemes It\u0026rsquo;s worth noting that there are additional training tasks in BERT except for the masked language modeling task. BERT also has a pre-training task that involves next sentence prediction (NSP) that is used to train the model to predict whether two segments are adjacent to each other in a document. This task aims to learn the relationships between segments for downstream tasks such as summarization and question answering that require reasoning about the relationships between pairs of sentences. However, there are other studies showing removing the NSP task can match or slightly improve the downstream tasks (Liu, et al. 2019).\nOther notable training schemes include sequence-to-sequence learning, as implemented in models like T5 (Text-to-Text Transfer Transformer) (Raffel et al., 2020). This approach involves training the model to convert one sequence of text into another. The multitask settings makes it versatile for tasks such as translation, summarization, and question answering.\nMoreover, as I plan to write about later, diffusion models have also been explored in LLMs (Singh, et al. 2023, Wang, et al. 2024). These models iteratively transform a simple noise distribution into a complex data distribution, effectively learning to reverse a diffusion process. This technique, although more common in image generation tasks, is being explored for text generation to enhance the diversity and quality of generated sequences.\n3. Typical LLM Frameworks Modern large language models (LLMs) are built on top of the transformer architecture (Fig. 1), using various architectural frameworks tailored to different purposes. The most prevalent frameworks include:\n3.1 Encoder-Only Encoder-only models, such as BERT, focus on understanding the input text. These models are optimized for tasks like text classification and named entity recognition, where comprehending the input context is crucial (Devlin et al. (2019)). The bidirectional transformer encoder allows the model to capture dependencies in both directions of the sequences, which is excellent for understanding context but problematic for generating text one token at a time. Therefore, encoder-only models are typically used for representation learning in various downstream tasks.\n3.2 Decoder-Only Decoder-only models, such as GPT, are designed for text generation tasks. They excel at producing coherent and contextually relevant text, making them suitable for applications in language modeling, text completion, and creative writing (Radford et al. 2018). The attention is almost always causal (forward directional), meaning the model can see only previous tokens (prefix). With appropriate prompting, decoder-only architectures can also perform downstream tasks like text classification.\n3.3 Encoder-Decoder The encoder-decoder framework, exemplified by models like T5 (Raffel et al., 2020) and BART (Bidirectional and Auto-Regressive Transformers) (Lewis et al., 2020), involves an encoder to process the input text and a decoder to generate the output text. This architecture is particularly effective for tasks that require text transformation, such as translation and summarization.\nHowever, encoder-decoder models may struggle with autoregressive generation. This is due to the bidirectional nature of encoders requiring full context bidirectionally and the need for the encoder to reprocess the entire sequence, including newly generated tokens, for each new token generation. This re-encoding step is computationally expensive and time-consuming, making the autoregressive process less efficient. While encoder-decoder models can theoretically use only the decoder for generation, this approach does not fully leverage the encoder’s parameters, leading to suboptimal use of the model’s capacity and parameters. Therefore, decoder-only architectures are more commonly used for tasks requiring generation, such as text generation and question answering.\n3.4 Decoder-Decoder To be more scalable and resource-efficient for long-context language modeling, YOCO (You Only Cache Once), a decoder-decoder architecture, has been recently proposed (Sun et al., 2024). YOCO consists of two main components: the self-decoder and the cross-decoder (Fig. 2). The self-decoder efficiently processes the input sequence to generate a set of global key-value (KV) caches via efficient self-attention mechanisms (such as sliding-window attention or gated retention). These caches are then reused by the cross-decoder, which employs cross-attention to generate the output sequence. This design allows YOCO to reuse the KV caches efficiently, achieving substantial improvements in memory consumption and inference speed without sacrificing performance compared to decoder-only transformers. (Traditional decoder-only models cache the key-value pairs for all tokens generated so far for each layer of the decoder, leading to multiple layers of caches being maintained simultaneously.) By caching the KV pairs only once and reusing them across multiple layers in the cross-decoder, YOCO significantly reduces the GPU memory footprint. This approach avoids the exponential growth of memory requirements typically seen in decoder-only models, where each layer and each token generation step involve separate KV caching.\nFig 2. Decoder-only vs Decoder-Decoder. (a) GPT\u0026rsquo;s decoder-only architecture. (b) \u0026amp; (c) Decoder-decoder architecture. (d) YOCO inference process. (a) is from Radford et al. 2018 and (b-d) are from Sun et al. 2024.\n3.5 Other Architectures Beyond these frameworks, ongoing research continues to explore new architectures and training methodologies to enhance LLM capabilities. Hybrid models and multimodal models, which integrate text with other data types (e.g., images, audio) (Radford et al. 2021), represent cutting-edge advancements in the field.\nAnother notable innovation is the prefix decoder architecture. This type of hybrid model modifies the masking mechanism of causal decoders (i.e., decoder-only architecture) to enable bidirectional attention over prefix tokens while maintaining unidirectional attention on generated tokens. Consequently, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict output tokens using shared parameters. For a more detailed illustration of such prefix language models, I recommend Cameron Wolfe\u0026rsquo;s blog \u0026ldquo;T5: Text-to-Text Transformers\u0026rdquo; (link1, link2).\n4. Employing Pre-Trained LLMs Pre-trained LLMs serve as foundation models that can be employed for specific tasks using several techniques, starting with zero-shot approaches and moving towards more customized fine-tuning methods.\n4.1 Prompting Prompting involves crafting specific input prompts to guide the model\u0026rsquo;s output. This technique leverages the model\u0026rsquo;s pre-existing knowledge and can be used to elicit specific information or perform particular tasks. Effective prompting can significantly enhance the model\u0026rsquo;s performance on a wide range of tasks without additional fine-tuning that updates the LLM itself. Prompting is particularly effective with autoregressive language models (ALMs) and models with a decoder architecture, such as GPT.\nExample: Consider a pre-trained language model being used in the context of prompting. The input sentence is: \"Tell me a story about poison apple.\" With prompting, the model uses this input to generate a continuation based on its pre-trained knowledge, producing an output such as: \"Once upon a time, in a faraway kingdom, there lived a beautiful princess who...\" Here, the model utilizes its extensive training data to create coherent and contextually appropriate text without additional fine-tuning. 4.2 In-Context Learning In-context learning allows the model to learn tasks by providing examples within the input context, without additional training. This method leverages the model\u0026rsquo;s ability to infer patterns and relationships from the provided examples, enabling it to perform new tasks based on the contextual information alone. This approach works well with models that have a decoder component.\nExample: Consider a pre-trained language model using in-context learning. The model is given the following context: \"Q: What is the capital of France? A: Paris. Q: What is the capital of Germany?\" With in-context learning, the model uses this context to infer the pattern and generate the appropriate continuation: \"A: Berlin.\" Here, the model leverages the provided examples to predict the answer to the new question based on the pattern observed in the context. Additionally, Dai et al. 2022 demonstrate that in-context learning can mimic the effects of explicit fine-tuning in GPT models, essentially serving as a form of implicit fine-tuning (Fig. 3). They illustrate that GPT generates meta-gradients for in-context learning during forward computation. In-context learning then utilizes these meta-gradients, applying them to the model via attention mechanisms. This meta-optimization in ICL is analogous to the explicit fine-tuning process, where model parameters are directly updated through back-propagated gradients.\nFig 3. Comparison of Finetuning and In-Context Learning (ICL) in GPT Models. The upper section illustrates the traditional fine-tuning process where the GPT model is updated with gradients ($ΔW_{FT}$) via back-propagation using pairs of sentences and their corresponding answers. The lower section demonstrates the in-context learning process, where demonstration examples (pairs of sentences and answers) are used to produce meta-gradients ($ΔW_{ICL}$)through forward computation. These meta-gradients are then applied to the model using self-attention and feed-forward network mechanisms to generate answers for query examples. The figure highlights the dual view of meta-optimization in ICL and explicit fine-tuning. The chart is the Fig. 1 from Dai et al. 2022.\n4.3 Instruction Learning Instruction learning/tuning involves providing the model with explicit instructions within the input text to perform a specific task. This method allows the model to understand and execute complex tasks by interpreting the given instructions. Instruction learning is particularly useful for tasks requiring precise control over the model\u0026rsquo;s behavior and is suitable for models with decoder components. The model is fine-tuned on a dataset with examples showing how to follow explicit instructions. This process trains the LLMs to learn how to do in context learning, granting the models\u0026rsquo; capability of zero-shot generalization from multitask training.\nExample: Consider a pre-trained language model being adapted through instruction learning. The instruction is: \"Here is a Chinese sentence that is often used to say Hi. Translate it to English: '你吃了吗?'\" With instruction learning, the model uses this directive to perform a specific task, generating the output: \"Hello, how are you?\" In this case, the model is fine-tuned on a dataset of translation pairs to follow explicit instructions for translating text between languages. 4.4 Chain of Thought (CoT) Extended from in-context learning, CoT prompting enhances the model\u0026rsquo;s reasoning capabilities by guiding it through a step-by-step problem-solving process. This technique can be applied in few-shot or zero-shot scenarios and includes methods like \u0026ldquo;let\u0026rsquo;s think step by step\u0026rdquo; and self-consistency.\n4.4.1 Few-Shot CoT Prompting Few-shot CoT prompting involves providing the model with several examples of the reasoning process needed to solve a task. By showing the model how to work through a problem step-by-step, it learns to apply a similar approach to new problems.\nExample: Consider a pre-trained language model being used with few-shot CoT prompting. The input is: \"Example 1: Q: If you have 5 apples and you give 2 away, how many do you have left? A: You start with 5 apples. You give away 2 apples. So, you have 3 apple left. Example 2: Q: If you have 6 books and you buy 3 more, how many do you have in total? A: You start with 6 books. You buy 3 more books. So, you have 9 books in total.\" New Problem: Q: If you have 10 candies and you eat 8, how many do you have left?\" Using few-shot CoT prompting, the model generates: \"You start with 10 candies. You eat 8 candies. So, you have 2 candies left.\" 4.4.2 Zero-Shot CoT Prompting Zero-shot CoT prompting involves instructing the model to think through the problem step-by-step without providing examples. This approach relies on the model\u0026rsquo;s ability to generalize from its training data.\nExample: Consider a pre-trained language model being used with zero-shot CoT prompting. The input is: \"Q: If you have 9 oranges and you give 3 to your friend, how many do you have left? **Let's think step by step.**\" Using zero-shot CoT prompting, the model generates: \"Let's think step by step. You start with 9 oranges. You give 3 oranges to your friend. So, you have 6 oranges left.\" 4.4.3 Self-Consistency Self-consistency is a technique used to improve the reliability of CoT prompting. It involves generating multiple reasoning paths and then selecting the most consistent answer among them. This method helps mitigate the variability in the model\u0026rsquo;s outputs.\nExample: Consider a pre-trained language model using self-consistency in CoT prompting. The input is: \"Q: If you have 12 balloons and you give 6 to your friend, how many do you have left? Let's think step by step.\" The model generates multiple reasoning paths: \"Path 1: You start with 12 balloons. You give 6 balloons to your friend. So, you have 7 balloons left.\" \"Path 2: You start with 12 balloons. You give 6 balloons to your friend. So, you have 6 balloons left.\" \"Path 2: You start with 12 balloons. You give 6 balloons to your friend. So, you have 6 balloons left.\" The final answer is determined by the consistency across paths: \"6 balloons.\" 4.4.4 Least-to-Most Prompting Least-to-most prompting is a teaching strategy that helps students learn new skills by using a sequence of prompts that start with the least intrusive and progress to the most intrusive. It breaks down a complex task into simpler sub-tasks and solves each one step-by-step. This method helps the model handle more complex problems by simplifying the process.\nExample: Consider a pre-trained language model using least-to-most prompting. The input is: \"Q: Solve the equation 2x + 3 = 11 step-by-step.\" The model breaks down the problem: \"Step 1: Subtract 3 from both sides. 2x = 8. Step 2: Divide both sides by 2. x = 4.\" The final answer is: \"x = 4.\" To learn more prompting engineering, check out the Learn Prompting website. Of course, except for hard prompting, there are other ongoing research and applications that aim to generate optimal prompts or soft prompts (a numerical vector like the adapter, see more in prefix-tuning) via another LLM model.\n4.5 Retrieval-Augmented Generation (RAG) RAG combines retrieval-based methods with generation capabilities to improve the accuracy and relevance of the model\u0026rsquo;s outputs. In RAG, the model first retrieves relevant documents or information from a large corpus and then generates a response based on this retrieved information. This approach enhances the model\u0026rsquo;s performance on tasks requiring up-to-date or domain-specific knowledge.\n4.6 Vocabulary Extension Vocabulary extension involves adding new tokens to the model\u0026rsquo;s existing vocabulary to better handle domain-specific terminology. This is crucial in fields like biomedicine, where specialized terms frequently appear that were not part of the original corpus (i.e., out-of-vocabulary). The process typically involves:\nUpdating the Tokenizer: Include new tokens by training a new tokenizer on the domain-specific corpus and merging its vocabulary with the existing one. Modifying the Embedding Layer: Resize the embedding matrix to accommodate the new tokens and initialize their embeddings, either randomly or using informed methods. Fine-tuning the Model: Adjust the new embeddings and model parameters by fine-tuning the model on the new domain corpus. Evaluating the Model: Ensure the model effectively handles the new tokens by evaluating it on a validation set from the new domain and making necessary adjustments. This process enables the LLM to understand and generate text in specialized domains effectively.\nApart from extending the vocabulary and fine-tuning a pre-trained LLM, other strategies like feature fusion can handle unknown tokens in specific domains. Feature fusion involves adding additional features to the embeddings of known tokens. This can include using pre-trained embeddings from various sources or incorporating domain-specific information. For example, when working with LLMs trained on DNA sequences, methylation status may be available in the target domain corpus but not in the original training corpus. By combining these different types of features, the model\u0026rsquo;s performance is enhanced, especially in handling domain-specific terminology.\n4.7 Supervised Fine-Tuning Supervised fine-tuning (SFT) involves continuing the training of a pre-trained model using domain-specific or task-specific data. SFT is a specific type of fine-tuning where the task-specific dataset used for fine-tuning includes labeled data. This labeled data is used to provide explicit guidance to the model on how to adjust its parameters. This process adjusts the model parameters to better suit the nuances of the new domain or new task. For example, a pre-trained LLM might be fine-tuned on a dataset of clinical notes to improve its ability to understand and generate medical text, significantly enhancing its performance on specialized tasks. SFT can be employed for both GPT-style and BERT-style LLMs.\n4.8 Alignment via RFHF Alignment through reinforcement learning from human feedback (RFHF) aims to fine-tune the unsupervised LMs to align the model\u0026rsquo;s outputs with human values and preferences, ensuring that the generated responses are accurate, ethical, and contextually appropriate (Ziegler, et al. 2019, Ouyang, et al. 2022). In this approach, human feedback is used to reward or penalize the model\u0026rsquo;s predictions, guiding it to produce more desirable and reliable outputs. This method is particularly useful for ensuring that LLMs generate responses that are not only accurate but also ethically and contextually appropriate.\nTo obtain human feedback, additional data called comparison data that show different human rankings on different model outputs given the same input. Such a comparison data will be used to build a reward model as human evaluators. Human evaluators review the model\u0026rsquo;s outputs and provide feedback, which is used to adjust the model\u0026rsquo;s parameters as fine-tuning via minimize the following loss:\n\\begin{equation} \\begin{aligned} \\mathcal{L_r(\\pi_t)} = -\\mathbb{E}_{x \\sim p_d, y \\sim \\pi_t} \\left[ r(x, y) - \\beta \\log \\frac{\\pi_t(y \\mid x)}{\\pi_r(y \\mid x)} \\right], \\end{aligned} \\end{equation}\nwhere $r$ is the reward function reflecting human preferences in the comparison data $p_d$. $\\pi_r$ is the original reference model used for regularizing $\\pi_t$ with Kullback–Leibler divergence. $\\beta$ is the hyperparameter for controlling the regularization strength. The objective here is to get large reward without deviating from the original reference LLM too much.\nTo optimize eq 3 for human preference alignment, there are several approaches broadly categorized into reward-based methods like Proximal Policy Optimization (PPO) (Ouyang, et al. 2022) and reward-free methods like Direct Preference Optimization (DPO) (Rafailov, et al. 2024) and Self-Play Preference Optimization (SPPO) (Wu, et al. 2024).\n4.8.1 PPO After a reward model $r_\\phi$ is constructed using the human-labeled comparison data, eq 3 can be explicitly optimized with online RL algorithms. PPO is a policy gradient method in RL algorithm designed to stabilize training by using a clipped objective function (Schulman, et al. 2017). Compared to other RL algorithms, the key idea for PPO is to update the policy in small steps to prevent large deviations from the previous policy, which can lead to instability.\n4.8.2 DPO Direct Preference Optimization (DPO) simplifies RFHF by directly optimizing preference probabilities without training a separate reward model (Rafailov, et al. 2024). It leverages the log-likelihood ratio of preferred responses to non-preferred ones. The DPO loss function is:\n\\begin{equation} \\begin{aligned} \\mathcal{L_{\\text{DPO}}}(\\pi_t; \\pi_r) = -\\mathbb{E}_{(x,y_w,y_l) \\sim \\mathcal{D}} \\left[ \\log \\sigma \\left( \\beta \\log \\left( \\frac{\\pi_t(y_w | x)}{\\pi_r(y_w | x)} \\right) - \\beta \\log \\left( \\frac{\\pi_t(y_l | x)}{\\pi_r(y_l | x)} \\right) \\right) \\right], \\end{aligned} \\end{equation}\nwhere $y_w$ and $y_l$ are the preferred and less preferred responses given the input $x$, respectively, $\\sigma$ is the sigmoid function and $\\beta$ is a scaling parameter. This method directly targets the improvement of preference probabilities, which can lead to more stable and efficient training compared to methods requiring a reward model.\nDPO avoids the complexity and potential biases introduced by training a separate reward model. It aligns the language model with human preferences through a straightforward optimization process. However, DPO might struggle with data sparsity and non-transitive preferences, which can limit its effectiveness in some scenarios.\n4.8.3 SPPO Self-Play Preference Optimization (SPPO) formulates the alignment problem as a two-player constant-sum game, aiming to find the Nash equilibrium policy that consistently aligns with human preferences (Wu, et al. 2024). SPPO uses a self-play mechanism to iteratively refine the model by generating responses and evaluating them using a pre-trained preference model. The policy update is given by:\n\\begin{equation} \\begin{aligned} \\pi_{t+1}(y | x) \\propto \\pi_t(y | x) \\exp \\left( \\eta P(y \\succ \\pi_t | x) \\right), \\end{aligned} \\end{equation}\nwhere $P(y \\succ \\pi_t | x) = \\mathbb{E}_{y\u0026rsquo; \\sim \\pi_t(\\cdot | x)}[P(y \\succ y\u0026rsquo; | x)]$ is the winning probability.\nThe SPPO loss function ensures that the policy iteratively improves by fitting the log-probabilities of the model’s responses to the empirical winning probabilities. This approach effectively handles intransitive and irrational human preferences by directly working with preference probabilities, which cannot be trivially achieved by symmetric pairwise loss functions like DPO. SPPO was shown significant improvements in alignment tasks, outperforming iterative DPO and other methods (e.g. Identity Preference Optimization IPO) in various benchmarks without the need for external supervision.\n4.9 Efficient Fine-Tuning As the size of foundational LLMs (Large Language Models) today is typically quite large, fine-tuning these models can be computationally challenging. Efficient fine-tuning techniques aim to reduce the computational resources required for fine-tuning large models. This enables large language models to be adapted to specific tasks with significantly reduced computational costs, making them more accessible and practical for a broader range of applications.\nA few common Parameter-Efficient Fine-Tuning (PEFT) methods are briefly described below.\nFig 4. Model architectures for different adaptation. (a) Prefix-Tuning, (b) LoRA,(c) Series Adapter, and (d) Parallel Adapter. The chart is the Fig. 1 from Hu, et al. 2023. Using this fig here for illustration purpose.\n4.9.1 Low-Rank Adaptation (LoRA) Low-Rank Adaptation (LoRA) reduces the number of trainable parameters by learning low-rank updates to the pre-trained weights. This method adapts only a small subset of the model\u0026rsquo;s parameters, allowing for quick and resource-efficient updates. LoRA is particularly useful in scenarios with limited computational resources.\nThe key idea behind LoRA is to approximate the weight update matrix $\\Delta W$ using two lower-rank matrices $A$ and $B$:\n\\begin{equation} \\begin{align*} W = W + \\Delta W \\\\\\ \\Delta W = A \\times B, \\end{align*} \\end{equation}\nwhere $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times k}$, with $r$ being much smaller than the original dimensions $d$ and $k$. This approximation reduces the number of parameters from $d \\times k$ to $d \\times r + r \\times k$.\nFig 5. LoRA. Imagine a visual representation showing the original large matrix $W$ and its low-rank approximation through matrices $A$ and $B$. The GIF is from HuggingFace. I recommend checking the link for more information about hte adapters.\n4.9.2 Adapter Modules Adapter modules involve adding small, trainable layers between the layers of a pre-trained model 1. These adapters can be fine-tuned for specific tasks without altering the original model parameters. This approach significantly reduces the amount of training data and computational power required compared to full fine-tuning.\nMathematically, if $h$ is the hidden state of a layer, an adapter module can be represented as:\n\\begin{equation} \\begin{aligned} h\u0026rsquo; = h + f_{\\text{adapter}}(h), \\end{aligned} \\end{equation}\nwhere $f_{\\text{adapter}}$ is a lightweight feed-forward network with a bottleneck architecture, typically much smaller than the original layer. Please read Fig 4 (c \u0026amp; d) for checking series abd parallel adapters, two common ways to place the adapters:\n4.9.3 Soft Prompting and Prefix-Tuning Unlike providing exact instruction as hard prompts, soft prompting involves adding a series of trainable tokens (soft prompts) to the input of a pre-trained model. These soft prompts compose a small continuous vector, that is prepended to the input sequence (Fig 4(a)). This soft prompt vector is fine-tuned while the rest of the model\u0026rsquo;s parameters remain fixed. This method is especially useful when adapting the model for tasks like classification, generation, or answering questions, without needing extensive retraining or fine-tuning of the model itself.\nGiven an input sequence $x = [x_1, x_2, \\ldots, x_n]$, soft prompting modifies it to:\n$x\u0026rsquo; = [p_1, p_2, \\ldots, p_m, x_1, x_2, \\ldots, x_n]$, where $[p_1, p_2, \\ldots, p_m]$ are the learned prefix vectors.\nExample: Consider a pre-trained language model being adapted to a specific task, such as sentiment analysis. The original input sentence is: \"I had a great day at the park.\" With soft-prompting, a learned soft prompt is prepended to this input: $[p_1, p_2, p_3, p_4, p_5]$ + \"I had a great day at the park.\" Here, $[p_1, p_2, p_3, p_4, p_5]$ are the soft prompting vectors. During fine-tuning, only these soft prompting vectors are updated while the rest of the model's parameters remain fixed. This allows the model to adapt to the sentiment analysis task using minimal additional computation. Prefix-tuning extends the idea of soft prompts by not only prepending trainable vectors to the input sequence but also integrating these vectors into intermediate stages of the model. This technique involves learning a series of continuous vectors, or \u0026ldquo;prefixes,\u0026rdquo; that are inserted not just at the input but potentially at multiple points within the transformer layers (see the Illustration of prefix tuning in Sebastian Raschka\u0026rsquo;s blog). This operation effectively modifies both the input and the intermediate representations throughout the model. Compared to soft prompting, prefix-tuning likely offers enhanced control over the model\u0026rsquo;s behavior, potentially leading to improved performance on complex tasks.\n4.9.4 Other Parameter-Efficient Fine-Tuning Techniques Other PEFT techniques focus on modifying only a subset of the model\u0026rsquo;s parameters or applying quantization techniques to reduce computational overhead. These methods maintain model performance while minimizing the resources needed for fine-tuning. Some examples are:\nBitFit (Zaken, et al. 2021): Only the bias terms of the model are fine-tuned. Mathematically, for a weight matrix $W$ and bias $b$, only $b$ is updated:\n$W\u0026rsquo; = W$\n$b\u0026rsquo; = b + \\Delta b$\nQAT (Quantization Aware Training) (Liu, et al. 2023): Quantization Aware Training applies quantization during training, which reduces the precision of the model parameters, leading to lower computational requirements.\n5. Plans for this Series This introductory post should have provided sufficient fundamentals about large language models (LLMs). Subsequent posts in this series will delve into specific applications of LLMs in the biomedical domain. The planned topics include\nGenomic Language Models: Covering general genomic applications, including DNA/RNA LLMs and gene LLMs. Protein Language Models: Exploring the use of LLMs in understanding protein structures and functions. Chemistry Language Models: Highlighting the role of LLMs in chemical research, including drug discovery and molecular analysis. Other Biomedical LLMs: Discussing models for universal biological sequence LLMs, medical imaging, generalized biological understanding through multimodal data integration, and other emerging applications in the biomedical field. While this intro post focuses on some general intro to LLMs, I look forward to more exploration of how LLMs are transforming biomedical research and applications in the coming posts of this series.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (May 2023). Biomedical LLMs: Intro. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/.\nor add the following to your BibTeX file.\n@article{xiao2024_biollm_intro, title = \u0026#34;Biomedical LLMs (1): Intro\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/\u0026#34; } References Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \u0026hellip; \u0026amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\nDevlin, J., Chang, M. W., Lee, K., \u0026amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nRadford, A., Narasimhan, K., Salimans, T., \u0026amp; Sutskever, I. (2018). Improving language understanding by generative pre-training. OpenAI.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \u0026hellip; \u0026amp; Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., \u0026hellip; \u0026amp; Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140), 1-67.\nMukul Singh, José Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. 2023. CodeFusion: A Pre-trained Diffusion Model for Code Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11697–11708, Singapore. Association for Computational Linguistics.\nWang, X., Zheng, Z., Ye, F., Xue, D., Huang, S., \u0026amp; Gu, Q. (2024). Diffusion Language Models Are Versatile Protein Learners. arXiv preprint arXiv:2402.18567.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., \u0026hellip; \u0026amp; Zettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., \u0026hellip; \u0026amp; Wei, F. (2024). You Only Cache Once: Decoder-Decoder Architectures for Language Models. arXiv preprint arXiv:2405.05254.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., \u0026hellip; \u0026amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., \u0026hellip; \u0026amp; Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., \u0026hellip; \u0026amp; Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., \u0026amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.\nRafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., \u0026amp; Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36.\nWu, Y., Sun, Z., Yuan, H., Ji, K., Yang, Y., \u0026amp; Gu, Q. (2024). Self-Play Preference Optimization for Language Model Alignment. arXiv preprint arXiv:2405.00675.\nHu, Z., Wang, L., Lan, Y., Xu, W., Lim, E. P., Bing, L., \u0026hellip; \u0026amp; Lee, R. K. W. (2023). Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933\nZaken, E. B., Ravfogel, S., \u0026amp; Goldberg, Y. (2021). Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199.\nLiu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., \u0026hellip; \u0026amp; Chandra, V. (2023). Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888.\nDai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., \u0026amp; Wei, F. (2022). Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559.\nGloeckle, F., Idrissi, B. Y., Rozière, B., Lopez-Paz, D., \u0026amp; Synnaeve, G. (2024). Better \u0026amp; faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737.\nLoRA is sometimes treated as a special case of an adapter module. In this post, I separated them as two different approaches given the popularity of LoRA.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jiajiexiao.github.io/posts/2024-05-10_biollm_intro/","summary":"\u003cp\u003eThe rapid advancements in Natural Language Processing (NLP) have showcased the versatility and\nefficacy of Large Language Models (LLMs). These models have demonstrated significant capabilities in\ncompressing vast amounts of information through unsupervised or self-supervised training, enabling\nimpressive few-shot and zero-shot learning performance. These attributes make LLMs particularly\nattractive for domains where generating extensive task-specific datasets is challenging, such as in\nbiomedical applications. Recent attempts to apply LLMs in biomedical contexts have yielded promising\nresults, highlighting their potential to address complex problems where data scarcity is a\nsignificant barrier. Starting from this post, I am planning to write a series on Biomedical LLMs.\u003c/p\u003e","title":"Biomedical LLMs (1): Intro"},{"content":"\u0026ldquo;Large p small n\u0026rdquo; describes a scenario where the number of features ($p$) is much greater than the number of observations ($n$) for model training. While it is not a new problem, it continues to pose significant challenges in real-world applications of machine learning, especially for domains lacking rich data or fast and cheap data generation processes. In this blog post, I\u0026rsquo;ll document my recent thoughts on the \u0026ldquo;large p small n\u0026rdquo; problem.\n1. Toy Problem Setup For simplicity and easy illustration, let\u0026rsquo;s look at a binary classification problem with linearly separable data, where at least one hyperplane can perfectly distinguish between the two classes.\nThe equation of a hyperplane in a $p$-dimensional space is given by:\n\\begin{equation} \\begin{aligned} w_1 x_1 + w_2 x_2 + \u0026hellip; + w_p x_p + b = 0. \\end{aligned} \\end{equation}\nHere, $x_1$, $x_2$, \u0026hellip;, $x_p$ are the coordinates of a point (called features) in the $p$-dimensional space, and $w_1$, $w_2$, \u0026hellip;, $w_p$, and $b$ are special numbers (called weights and bias) that determine the exact position and the norm direction of the hyperplane. Now, if we have a new point ($x\u0026rsquo;$) and want to know which group it belongs to, we can do two things:\nGeometrically: We can look at where the point is located in relation to the line. If it\u0026rsquo;s on one side of the line, it belongs to one group; if it\u0026rsquo;s on the other side, it belongs to the other group.\nAlgebraically: We can plug the coordinates of the point into the left side of the eq 1 and calculate the result. If the result is positive, the point belongs to one group; otherwise, it belongs to the other group.\nThese weights and bias constitute $p+1$ unknown parameters that govern predictions. Our goal is to find the \u0026ldquo;true\u0026rdquo; values of these parameters that define the class assignments in the underlying data generation process, so that we can reliably predict the class of any future unseen point in the $p$-dimensional space. To achieve this, we typically convert this into an optimization problem and use maximum likelihood estimation based on the observed samples. This involves finding the optimal values of the parameters that minimize a loss function such as binary cross-entropy $\\mathcal{L_b}$ or hinge loss $\\mathcal{L_h}$:\n\\begin{equation} \\begin{aligned} \\mathcal{L_b} (y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\cdot \\log(\\hat{y}_i) + (1 - y_i) \\cdot \\log(1 - \\hat{y}_i)], \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\mathcal{L_h}(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\cdot \\hat{y}_i), \\end{aligned} \\end{equation}\nwhere $y_i$ is the true label of the $i^{th}$ observation, and $\\hat{y}_i$ is the predicted label or the generalized form of predicted probability for positive class assignment of the $i^{th}$ observation derived by applying an activation function $\\sigma$, such as the sigmoid (which is a smoothed step function), to the weighted sum:\n\\begin{equation} \\begin{aligned} \\hat{y_i} = \\sigma(w_1 x_{i1} + w_2 x_{i2} +\u0026hellip; + w_p x_{ip} + b). \\end{aligned} \\end{equation}\nWhen finding a minimum value of a function, in math and physics, we often to check for points where the gradient is zero and the Hessian matrix is positive definite 1. That means we may want to first solve $\\nabla_{w, b} \\mathcal{L} = 0$ to find the critical points in the parameter space. However, there appear to have no closed form solution due to the involved nonlinearity in deriving $\\hat{y}_i$ in this case (Lipovetsky2015 or see discussions in link1 and link2). Consequently, gradient descent-based algorithms are often employed to find values for $w$ and $b$ that minimize the loss function $\\mathcal{L}$.\n2. Not All Solutions Are Equally Generalizable While binary cross-entropy and hinge loss are both convex concerning $\\hat{y}$ 2, this convexity is not strict 3 due to the possible zero values of second-order derivatives of the losses with respect to $\\hat{y}$. So, there exist infinitely many sets of weight and bias values capable of separating data points. Moreover, as the feature dimension increases, the volume of the feature space grows exponentially, reducing the likelihood of obtaining an unbiased or well-representative sample for model training exponentially 4. As a result, the infinitely many separating hyperplanes are not equally generalizable to unseen data that are far from the sampled space.\nAs seen, in large $p$ small $n$ scenarios, even with a simple linear model exhibiting perfect performance on the training set, obtaining parameters conducive to predicting labels accurately on unseen test data can be challenging. When it involves non-linear components, the convex optimization can turn into a non-convex one. This will be more difficult because the number of local minima can increase along with input dimension $p$. Any corresponding performance degrades in the unseen test set can be a result of overfitting on the limited training set.\n3. Tackling the Problem of Large P Small N To address this problem, we may often need to leverage additional information to guide feature engineering, model design, model training, and so on.\n3.1 Feature selection Feature selection is probably the simplest thing we may consider to address the large $p$ small $n$ problem as it aims to cut the number of features so $p$ and $n$ are more comparable.\nIn order to know which features to keep for building models, we typically leverage some assumptions or intuition to decide which features are more useful for the problems we try to predict. For example, we may assume that the features that have a high association with the target variable are more predictive than the ones that have a low association with the target variable. We may also (iteratively) try different feature sets for modeling and see which combination of features could lead to the most favorable performance in the validation sets.\nWhile feature selection is simple and intuitive, the dropped features may actually be critical. In long-tailed problems, the dropped features may be the ones that are more likely to have a high variance in the training set. In this case, the model trained with a limited training set and reduced feature set may not be able to generalize well to the unseen test set.\n3.2 Regularization Regularization is another common approach to improve the robustness and generalization in the large $p$ small $n$ problem. The most common way to regularize the model training process is to add a penalty term to the loss function to penalize the model complexity, avoiding overfitting.\n3.2.1 $L_p$ regularization To penalize having non-zero values of the model parameters, $l_p$ regularization adds $L_P$ norm of the model\u0026rsquo;s parameters 5 raised to the power of $p$ to the loss function as below:\n\\begin{equation} \\begin{aligned} \\mathcal{L_{\\text{total}}} = \\mathcal{L_{\\text{original}}} + \\lambda_p \\mathcal{L_{p}}, \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\mathcal{L_p} = ||w||^p_p = \\sum_{j=1}^{p} |w_j|^p, \\end{aligned} \\end{equation}\nwhere $\\lambda_p$ is a hyperparameter for regularization strength and $p$ is the order of the norm with typically non-negative value 6.\nDifferent values of $p$ may have different effects on the model training process (Goodfellow, et al 2016). For example, adding $L_2$ regularization can shrink the weights toward 0 and is equivalent to multiply your likelihood a gaussian prior in the maximum a posteriori (MAP) estimation from the view point of Bayesian. $L_1$ regularization is equivalent to applying a Laplacian prior in the MAP estimation and leads to sparser solutions than $L_2$ 7. A more extreme regularization is $L_0$ that adds the sum of number of non-zero weights to the loss to force the model have more zero weights for certain features than $L_1$ does 8.\nWhile $L_0$ and $L_1$ regularization may sometimes be used as a gradient-based feature selection method, they may introduce difficulties in optimization as the $L_0$ norm is not convex and the $L_1$ norm is not differentiable at the origin. Therefore, $L_2$ regularization might be more commonly used in practice.\nMoreover, since the Hessian for $\\mathcal{L_2}$ is positive definite 9, this makes the loss function landscape strictly convex as long as the original loss function $L_{original}$ in the parameter space is convex. This property of $L_2$ regularization ensures a single global optimal solution for simple but widely adopted linear models like logistic regression. In our previous toy problem, when applying $L_2$ regularization, there will be a unique hyperplane being found by the optimization process.\n3.2.2 Other regularization techniques In addition to $L_p$ regularization, there are other forms of regularization that allow us to incorporate domain knowledge into the model training process. For example, when there is a spurious correlation in the training data, the naively trained model may establish some unwanted behaviors such as a correlation between the model outputs and bias attributes that are spuriously correlated to the target variables (and features). Such a correlation indicates that the model leverages the spurious correlation to make predictions, leading to poor generalization in the test data that lacks such spurious correlation. To avoid this, we can add a penalty term $\\mathcal{L_{\\text{corr}}}$ to the loss function to penalize the model not to have such spurious correlation.\n\\begin{equation} \\begin{aligned} \\mathcal{L_{\\text{corr}}} = \\frac{1}{M} \\sum_{i=1}^{M} |(\\hat{y}_i -\\overline{\\hat{y}}) \\cdot (a_i- \\overline{a})|, \\end{aligned} \\end{equation}\nwhere $|(\\hat{y}_i -\\overline{\\hat{y}}) \\cdot (a_i- \\overline{a})|$ quantifies the magnitude of the correlation between model output $\\hat{y}_i$ and bias attribute $a_i$ for the $i$th sample among select $M$ samples in the training data. Except for this simple example, we may also come up with more complex regularization like what is done in generative adversarial network (GAN)(Goodfellow, et al 2014) and contrastive learning (Oord, et al 2018).\nWe may also apply controls on the model training dynamics to regularize the model. For example, we can stop the model training early when the validation loss is not improving even though the learning curve so far shows that the training loss is continuously decreasing. This is called early stopping, which prevents the model from overfitting the training data. We can also apply dropout, which randomly turns off some of the neurons during training to force the model to learn more robust features (Srivastava2014). This regularization also provides some data augmentation, which adds some synthetic training data to increase $n$. We may also introduce additional relevant tasks to regularize the model. In particular, we may use different portions of the model for different tasks. The shared components will likely better capture the key components in the data, and the resultant model thus more robust.\n3.3 Feature engineering Feature engineering is another way to incorporate domain knowledge into the model training process. This involves using expert knowledge to transform the raw features into a lower-dimensional space of more relevant features. Dimensionality reduction techniques can also be used for this purpose. While feature engineering is widely used in conventional machine learning, its effectiveness depends on the reliability and completeness of the expert knowledge.\n3.4 Architectural Biases A sister method of feature engineering is to design the model with appropriate architectural biases. The structure of the model itself can also impose potentially useful biases. For example, Convolutional neural netowrks (CNN) can learn how to extract local features in an image with hundreds to hundreds of thousands of pixels using weight-sharing convolution filters. These filters are applied along input features and provide translation-equivariant feature maps 10, which are much more efficient and effective than using mlp or feature engineering approaches. The translation-equivariance property allows the model to learn to recognize the same object in different locations in the image. Without such a bias, the model may require a lot of more training data to learn.\nHowever, the architectural bias can also be the bottleneck that limits the model generalization capability. For example, the CNN model may be biased towards leveraging the local patterns but fail to correctly recognize the distant global structure like a vision transformer can do (Dosovitskiy2020). Moreover, due to a lack of rotation-equivariance, the CNN model may fail to recognize rotated objects. In this case, we may need to have data augmentation via rotation or design a model that is rotation-equivariant.\n3.5 Pre-training and Foundation Models In recent years, foundation models pre-trained on large-scale datasets have become increasingly popular for tackling the large $p$ small $n$ problem. These models are trained on broad datasets using self-supervised learning objectives, which allows them to learn general-purpose feature representations that can be fine-tuned for specific tasks with limited data. For example, a model pre-trained on ImageNet can be fine-tuned on a small dataset of medical images to achieve good performance on disease classification. Foundation models in natural language processing, such as BERT (Devlin2018) and GPT (Brown2020), have also shown impressive performance on a wide range of tasks with limited fine-tuning data. These models are trained on massive text corpora and learn to capture complex linguistic patterns and world knowledge. By leveraging the knowledge learned during pre-training, foundation models can achieve good performance on downstream tasks with few or no labeled examples (i.e., few-shot or zero-shot).\nAs seen, with these pre-trained models, the models don\u0026rsquo;t just provide reasonable architectural biases but also better weight initialization, which helps effectively integrate large $p$ features than the model trained from scratch from the limited small $n$ training data. It also can significantly reduce the training time and generalization error.\nSummary All these methods, including feature selection, feature engineering, regularization, model design, and pre-training, are based on our prior understanding of the data generation process and the model training. They are sometimes called inductive biases, which refer to the assumptions, preferences, or prior knowledge that a learning algorithm incorporates to generalize beyond the training data and make predictions on unseen data. These biases guide the learning process and help the model to prioritize certain solutions or hypotheses over others. Whether the inductive biases being leveraged are sound or not determines whether we could generalize the model to unseen data.\nFor the large $p$ small $n$ problem, it is often believed that the most relevant information for modeling the problem lies in a low-dimensional manifold (Oord, et al 2018). Some methods use fewer parameters to learn this latent space but rely on stronger inductive biases, while others use more parameters with weaker inductive biases. In large $p$ small $n$ cases, models with more parameters are typically more prone to overfitting, requiring more data to estimate the optimal parameters.\nSince the ImageNet moment, the combination of big data and deep learning with suitable inductive biases has led to numerous successes in various applications. For domains with limited data, successful examples like AlphaFold2 highlight the importance of inductive biases (Jumper2021). Developing appropriate inductive biases for large $p$ small $n$ problems remains an active area of research.\nAlthough collecting more features or modalities of data may offer unique advantages, it is crucial to recognize that expanding the feature set can be costly and increase the risk of data biases and overfitting. Furthermore, acquiring additional samples for training and evaluation can be a challenging task sometimes. This issue is prevalent across various predictive modeling domains, including healthcare, business, and science. Consequently, investing resources to augment feature sets necessitates careful strategic planning and consideration to ensure that the benefits outweigh the potential drawbacks. Yep, what a large $p$ for small $n$ ;)\nIn summary, the large $p$ small $n$ problem presents unique challenges that require a balance between model complexity, inductive biases, and data availability. Addressing these challenges is crucial for developing effective predictive models in data-limited domains.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (April 2024). What a large p for small n. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/.\nor add the following to your BibTeX file.\n@article{xiao2023howtoachieverobustai, title = \u0026#34;What a large p for small n\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;April\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/\u0026#34; } References Lipovetsky, S. (2015). Analytical closed-form solution for binary logit regression by categorical predictors. Journal of applied statistics, 42(1), 37-49. https://stackoverflow.com/questions/37997253/can-we-use-normal-equation-for-logistic-regression https://sebastianraschka.com/faq/docs/logistic-analytical.html#is-there-an-analytical-solution-to-logistic-regression-similar-t Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016). Deep learning. MIT press. Chapter 7. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u0026hellip; \u0026amp; Bengio, Y. (2014). Generative adversarial nets. Advances in neural information processing systems, 27. Oord, A. V. D., Li, Y., \u0026amp; Vinyals, O. (2018). Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. Theodoridis, S., \u0026amp; Koutroumbas, K. (2006). Pattern recognition. Elsevier. Chapter 5, Section 5.9. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \u0026amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1), 1929-1958. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., \u0026hellip; \u0026amp; Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Devlin, J., Chang, M. W., Lee, K., \u0026amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., \u0026hellip; \u0026amp; Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901. Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., \u0026hellip; \u0026amp; Hassabis, D. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583-589. To check if the Hessian matrix is positive definite, we can examine whether all eigenvalues of the matrix are positive. In the 1D case, we can also simply use the second derivative test to check if the function is convex. If the second derivative is positive, the function is convex. If the second derivative is negative, the function is concave. If the second derivative is zero, the function is a flat line. It\u0026rsquo;s worth to note that, at a critical point, the function derivative can be undefined (such as $x=0$ for $f(x) = |x|$).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe convexity of the loss function in the parameter space depends on the choice of activation function. If the activation function is convex, the loss will be convex in the parameter space, as the weighted sum is a linear (and thus convex) operation. On the other hand, like a typical neural network with at least two linear layers and some form of nonlinear activation function after each layer, the loss function landscape in the parameter space is not convex given the Hessian matrix $H$ is not positive semidefinite (meaning the eigenvalues of $H$ are non-negative). The composition of the loss function with the non-linear activation functions and the complex architecture of neural networks introduces non-convexity. Training a neural network thereby involves navigating a non-convex optimization landscape, which can have multiple local optima and saddle points (meaning the Hessian is indefinite, i.e., its eigenvalues have both positive and negative values).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStrictly convex guarantees that there is only one minimum. Different types of convexity are roughly told via the following:\n$f$ is convex if and only if $f\u0026rsquo;\u0026rsquo;(x) \\geq 0$ for all $x$ $f$ is strictly convex if and only if $f\u0026rsquo;\u0026rsquo;(x) \u0026gt; 0$ for all $x$ (This is sufficient but not necessary for strictly convexity. When the function is not not differentiable, the gradient is replaced by the sub-gradient at the non-smooth point $x$.) $f$ is strongly convex if and only if $f\u0026rsquo;\u0026rsquo;(x) \\geq m \u0026gt; 0$ for all $x$ (Strong convexity can provide faster convergence and tighter error bounds to the minimum compare to strictly convex function) \u0026#160;\u0026#x21a9;\u0026#xfe0e; This is an example of curse of dimensionality in sampling. For machine learning, a typical rule of thumb is that there should be at least 5 training examples for each dimension or 10 times the VC dimension in the representation space (Theodoridis2006). If one wants to apply this rule, the number of training examples required to learn a model in the training set grows exponentially with the number of dimensions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe calculation of $L_p$ regularization usually ignores the bias terms and moving averaged values in batch normalization since they don\u0026rsquo;t contribute to overfitting. However, for convenience, in many implementations, such as the widely adopted library PyTorch, all parameters that require updating by backpropagated gradients may be counted by the optimizer\u0026rsquo;s default weight decay calculation. Weight decay is a form of regularization that is sometimes (e.g., SGD without momentum or AdamW that has decoupled weight decay calculation) equivalent to $L_2$ regularization.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor $p=0$, $L_0$ norm basically counts non-zero elements and $0^0\\equiv0$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLaplacian prior has higher probability density near zero compared to a Gaussian prior, thus promoting sparsity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n$L_0$ regularization directly penalizes the number of non-zero weights, while $L_1$ regularization indirectly promotes sparsity by shrinking weights more aggressively than $L_2$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe Hessian for $\\mathcal{L_2}$ is a diagonal matrix with 2 as the diagonal elements. So the eigenvalues of the Hessian are equal to 2, which are always positive. Q.E.D.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTranslational equivariance means that the model can recognize the same object in different locations in the image. In math, a function or transformation is equivariant if it maintains a certain relationship between its input and output when both are transformed. In other words, if $f$ is equivariant with respect to transformations $T$ and $S$, then $f(T(x))=S(f(x))$ for all $x$. A stronger version of equivariance is invariance. A function or transformation is invariant if its output remains unchanged when its input is transformed in a certain way. For example, if a function $f(x)$ is invariant under translation, it means that $f(x+a)=f(x)$ for any value of $a$. For CNN, convolutional filters are translation-equivariant while max-pooling is translation-invariant.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jiajiexiao.github.io/posts/2024-04-29_large_p_small_n/","summary":"\u003cp\u003e\u0026ldquo;Large p small n\u0026rdquo; describes a scenario where the number of features ($p$) is much greater than the\nnumber of observations ($n$) for model training. While it is not a new problem, it continues to pose\nsignificant challenges in real-world applications of machine learning, especially for domains\nlacking rich data or fast and cheap data generation processes. In this blog post, I\u0026rsquo;ll document my\nrecent thoughts on the \u0026ldquo;large p small n\u0026rdquo; problem.\u003c/p\u003e","title":"What a large p for small n"},{"content":"In my previous post, I highlighted the growing influence and adoption of Artificial Intelligence (AI) and machine learning (ML) systems, discussing how they attain \u0026ldquo;intelligence\u0026rdquo; through a careful \u0026ldquo;data diet.\u0026rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers to robust performance and reliable deployment. In particular, covariate shift (eq 1) and concept drift (eq 2) are two major types of OOD frequently encountered in practice, demanding mitigation for robust model deployment.\n\\begin{equation} \\begin{aligned} \\text{Covariate shift:} \\quad P_\\text{S} (X) \\neq P_\\text{T} (X) \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\quad \\text{Concept drift:} \\quad P_\\text{S} (Y | X) \\neq P_\\text{T} (Y | X) \\end{aligned} \\end{equation}\nIn this post, we delve into strategies to tackle OOD and enhance the robustness of AI/ML models.\n1. Baseline: Quality Control and Normalization In various discussions today, people often talk about data quality, batch/cohort effects, or \u0026ldquo;garbage in, garbage out\u0026rdquo;. These are actually quite relevant to robustness of your model. As a result, the first thing we should consider prioritizing is to establish quality control in your data generation/collection pipeline and to conduct data normalization. For scenarios prone to data biases (e.g., batch effects in biological experiments), designing control measurements becomes crucial for later data normalization. Quality control and normalization ensure that the data\u0026rsquo;s quality is suitable for model training and inference, and that the inputs are on comparable scales.\nFig 1. Quality control and normalization workflow adopt in digital pathology. Prostate (a) and lung (b) tissue images stained with hematoxylin and eosin were normalized against target images and evaluated by pathologists. Source image is from Fig2 in Michielli, et al (2022).\nFig. 1 illustrates a clinical workflow in digital pathology. Despite variance in stain levels and random artifacts, stain normalization significantly improves image quality and enhances clinical diagnostic confidence (Michielli, et al (2022)). When developing and deploying computer vision (CV)-based AI/ML systems for assisting pathologists, stain normalization and quality controls help mitigate covariate shifts in the stain images. In other words, after these data preprocessing steps, the marginal distribution of images in source and target domains becomes comparable. Consequently, OOD problems transform into IID ones.\n$$ \\text{Biases} \\xrightarrow{\\text{e.g. batch effects}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{QC}]{\\text{normalization}} P_\\text{S} (X\u0026rsquo;) = P_\\text{T} (X\u0026rsquo;) $$\n2. Domain Adaptation when Target Domain is Accessible 2.1 Instance-Reweighting Despite our efforts in quality control and normalization, covariate shifts may persist. Such situations often indicate selection bias, where samples from the source domain may not cover all possible feature distributions, failing to fully reflect the target domain. While acquiring less biased or more representative data seems intuitive, it can be prohibitively costly in terms of both money and time, often requiring cross-functional efforts over months to years. Consequently, computational tactics or mitigations become essential and may prompt inquiries from managers or even CxOs.\nTo address this, let\u0026rsquo;s begin by checking for any known information about the target domain. The observation of covariate shifts implies some knowledge about the target domain, such as the statistical distributions of features. This information becomes valuable for guiding the use of source domain data to build a model that performs well in the target domain. Such a goal is also known as domain adaptation, because the aim is to adapt the model trained on the source domain to generalize effectively in the target domain with different distributions.\nFig 2. Instance-reweighting adapts the classifier trained in source domain to generalize to target domain. Source images are from Jong (2017).\nInstance-reweighting is a domain adaptation method leveraging the target domain distribution. To illustrate, I just use the great examples from Johann de Jong\u0026rsquo;s blog. Fig. 2 displays the distributions of features x1, x2, labels of each data point, and learned and ground truth decision boundaries. Due to selection biases, the source domain exhibits different marginal distributions compared to the target domain (Fig. 2a). Training a classifier solely on source domain data yields a decision boundary diverging from the ground truth for the target domain (Fig. 2b). Instance-reweighting involves adjusting each training instance\u0026rsquo;s weight in the source domain to match the target domain distribution (Fig. 2c). This reweighted training significantly improves the learned decision boundary\u0026rsquo;s performance in the target domain. Instance-reweighting is widely adopted when instance-specific considerations are needed for model training and evaluation. For example, addressing problems with long-tailed distributions involves static reweighting (constant sample weights) or dynamic reweighting (e.g., via focal loss 1) to penalize minority groups more, resulting in more robust performance against rare events.\nIn summary, instance-reweighting aims to mitigate encountered covariate shifts by adjusting the sample distribution. With the reweighting scheme matching the target domain, the reweighted source domain distribution $P_\\text{S}\u0026rsquo; (X)$ aligns with the target domain distribution $P_\\text{T} (X)$.\n$$ \\text{Biases} \\xrightarrow{\\text{e.g. selection biases}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow{\\text{reweighting}} P_\\text{S}\u0026rsquo; (X) = P_\\text{T} (X) $$\nThe additional knowledge used to derive the weights introduces some inductive bias for the model; thus, the accuracy of this additional knowledge about the target domain can be critical to the model\u0026rsquo;s robustness.\n2.2 Semi-Supervised Learning In addition to target domain statistics, if we have access to unlabeled data from the target domain, we can explore other domain adaptation methods leveraging the intrinsic structure behind the data to improve OOD performance. For instance, employing a semi-supervised learning algorithm allows incorporating unlabeled data from the target domain during model training. The initial model is trained based on the source domain data. Subsequently, this model is applied to the unlabeled target domain data to generate pseudo-labels for those unlabeled samples. Samples with confident pseudo-labels are selected as additional training data, and the model is retrained alongside the source domain samples. This iterative process refines the model, enhancing its performance in the target domain.\nFig 3. Semi-supervised learning aids domain aptation. (a) Massive unlabeled data representing the target domain is useful to overcome selection biases in the source domain and assist the model in generalizing to the target domain. (b) Pseudo-labeling algorithm iteratively augments the source domain data and regularizes the model training.\n2.3 Test-Time Training Additionally, Test-Time Training (TTT) (Sun, et al (2020)) can be explored even when there is no access to the target domain until running model testing. This technique introduces additional self-supervision tasks that can be applied to unlabeled data from the target domain. In an image classification task example as shown in Fig. 4, the model first projects the images into a latent space via an encoder. Then, the latent representation will be used for predicting the rotation angle of the images in addition to predicting the object label of the images. Self-supervised targets can be easily obtained since you know the angle at which the image is rotated in the data-augmentation process. During testing, we now have access to the target domain data as it is input for the model for making predictions. Each test image can be augmented via rotation and passed to the model for self-supervised learning. This self-supervised learning offers a chance to update the encoder based on the target domain, which learns how to project the target domain images into a comparable latent space relative to the source domain. This is the test-time training.\nFig 4. Test-Time Training. Source image is from the authors\u0026rsquo; page (link) of TTT (Sun, et al (2020)).\nBoth semi-supervised learning and test-time training alleviate covariate shifts by seeking data augmentation to get equivalent IID.\n$$ \\text{Biases} \\xrightarrow[\\text{batch effects}]{\\text{e.g. selection biases}} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{self-supervised regularization}]{\\text{data augmentation}} P_\\text{S} (X\u0026rsquo;) = P_\\text{T} (X) $$\nWhile these are effective methods and tactics in many real-world applications, there may be other factors limiting their adoption. For example, in scenarios with strong regulations, such as when the deployed model needs to be fully locked and requires FDA approval, using the target domain data (e.g., clinical trial data and samples collected post-approval) to update the model may not be allowed or under regulation. For applications that require low latency in inference time, TTT may be too slow to be deployed. All these mitigations require domain-specific consideration before being pursued.\n2.4 Transfer Learning and Fine-Tuning When we have access to the target domain\u0026rsquo;s labeled data during model development stage, although it has a very limited sample size compared to the source domain, we can conduct transfer learning and fine-tuning to adapt the model to the new domain.\nTransfer learning aims to apply knowledge learned from one domain or one task to another related domain or task, where the knowledge is often encoded as learnable parameters in deep neural networks nowadays. The rationale behind transfer learning is that there is transferable knowledge across related domains and tasks. Thus, it is beneficial to start from the pre-trained network based on the source domain with lots of data, rather than training the network from scratch based on the target domain with a limited amount of data. Transfer learning typically freezes the parameters pre-trained based on the source domain but, on top of that, adds a few additional layers whose parameters are fitted based on the target domain.\nSimilarly, fine-tuning also starts from the same pre-trained network along with possible optional layers. However, in contrast to transfer learning 2, fine-tuning also updates the weights of the pre-trained network or a subset of its layers based on the target domain.\nFig 5. Domain adpation learns domain-invariant transformations and aligns domain distributions. Source image is from Fig. 1 in Choudhary2020, et al (2020), where domain adaptation is treated as a transductive transfer learning method. Here, this image illustrates the idea that covariate shift disappears once the different domains are aligned.\nEssentially, both transfer learning and fine-tuning adapt the parameters learned from the source domain and seek further minimum adjustments to make the source and target domains comparable in the projection space (i.e., latent space) of the features. Like other domain adaptation approaches we\u0026rsquo;ve seen previously, this mitigates the covariate shift and allows the model to generalize to the target domain (Fig. 5).\n$$ \\text{Related tasks or domains} \\xrightarrow{} P_\\text{S} (X) \\neq P_\\text{T} (X) \\xrightarrow[\\text{fine-tuning}]{\\text{transfer learning}} P_\\text{S} (X\u0026rsquo;) = P_\\text{T} (X\u0026rsquo;) $$\n3. Domain Generalization when Target Domain is Inaccessible So far, we have examined relatively simple OOD cases. However, more challenging scenarios can arise. In some instances, there might be no reliable prior information or even access to the target domain when training and locking the model for deployment. This challenge is often encountered in areas with limited training data and stringent regulations, where capturing a representative set becomes particularly difficult.\nMachine learning literature uses the term domain generalization to characterize the goal of building robust models for target domains that are entirely inaccessible during model development. This presents a more challenging but potentially more needed extension of domain adaptation.\nApart from covariate shift, another OOD challenge we haven\u0026rsquo;t addressed is concept drift. It can seem daunting when the relationships between features and labels differ in the target and source domains, and this shift is unknown until after building, selecting, and deploying the models. Well, performance degrade in shifted target domain may not be a big issue in low-stakes scenarios, just further train the model or retrain. However, it\u0026rsquo;s a common challenge in healthcare, where AI/ML-based or AI/ML-derived products must meet primary and secondary goals in clinical trials for disease diagnosis and treatment.\nSo, what can we do in these more difficult cases? Consider a scenario where high school students are only allowed to take the real SAT test once. They should be allowed to take as many mocks as they want, right? Would that be helpful? I guess the more closely the mocks can reflect the real test, the higher the chance to achieve similar performance in the actual exam 3. Similarly, in domain generalization, we still need to think about how we can make the source domain data more like the target domain.\nIn the realm of concept drift, the relationships between Y and X are subject to change. In reality, there can also be situations where both P(Y|X) and P(X) change across domains. The key question is whether there are features or projections of features that establish a stable relationship with target labels, regardless of the domains.\n3.1 Correlation vs Causality In our quest for a more stable relationship between features and targets, let\u0026rsquo;s revisit how AI/ML models are trained.\nModels utilize differences between model outputs and targets to update parameters. This leads to that fact that the model leverages the correlation between the features and targets to learn. A feature more correlated with the targets makes the model more likely to use it for predictions.\nFig 6. Correlation and causality. X and Y are two random variables that appear to be correlated. When digging into possible data generation process, it can be simplified as either they have a causal relationship or they have a common cause Z.\nHowever, correlation is not a stable causal relationship; it can be spurious for various reasons such as sample collection biases. According to Reichenbach\u0026rsquo;s common cause principle (Hitchcock2021), if we observe a correlation or association between two random variables, it means either one of the variables causes the other or there is a third variable that causes both (known as confounding) (Fig. 6). Causal relationships are more stable than correlation, as spurious correlations can easily change across domains or environments.\nFor instance, consider a predictive model trained on medical data in the source domain, where an attribute like \u0026ldquo;number of hospital visits\u0026rdquo; shows a high correlation with disease outcomes due to selection biases. This attribute might seem crucial in the source domain, but once the selection biases disappear in the target domain, the correlation weakens, and the attribute loses its predictive power for disease outcomes. This scenario resembles a concept drift, highlighting opportunities to address OOD by identifying domain-invariant components in features that have a (ideally) causal relationship with target labels.\n3.2 Multitask Learning and Adversarial Training To identify invariant components in features, classical approaches like feature selection and engineering might come to mind. These handcrafted pre-processing methods rely on additional prior knowledge and are often employed in statistical learning and settings with small training sizes. However, such prior knowledge, acting as an inductive bias, may limit further performance improvements. For more complex problems with reasonable training sizes, we need an end-to-end training framework to learn invariant components in features with a stable relationship to target labels.\nMultitask learning provides such a framework, allowing flexible representation learning. As depicted in the left part of Fig. 7, features can be encoded into a latent representation that predicts multiple attributes related to the main task (original target label) and auxiliary tasks (other attributes of sample instances). This facilitates the model to extract a more meaningful dense representation for predictions. Similar to Test-Time Training, well-designed auxiliary tasks can offer useful regularization on the networks, preventing overfitting on the main task.\nFig 7. Domain-adversarial training a neural network that learns both class and domain label. A neural network can be divided into encoding and decoding parts. The left side illustrates a feature extractor $G_f$ encoding inputs $X$ into latent features $f$. The right side shows latent features $f$ being decoded to predict class label $y$ and domain label $d$. While the loss $L_y$ for the class label is normally backpropagated to update the whole network, the loss $L_d$ for the domain label needs to be reversed when used for adversarial training the feature extractor. Source image is from Fig1 in Ganin, et al (2016).\nIn situations with biased attributes showing high correlation with the target label (confounding), it\u0026rsquo;s crucial for the network not to exploit such shortcuts. Adversarial training becomes relevant in this context, as it can explicitly penalize any direct or indirect use of biased attributes and confounders. The right-hand side of Fig. 7 illustrates the decoding part in multitask learning along with adversarial training. The latent feature is used to predict both class label and domain label. However, since the domain label may introduce confounding effects, one may want the constructed latent space to be less predictive of the domain label. Thus, the prediction loss for the domain label is reversed during backpropagation to the encoding layers. This process is known as adversarial training and can be effective in mitigating known biases in the source domain if being well tuned. See eq3 for exact gradient descent operation for the whole training process in math4.\n\\begin{equation} \\begin{align*} \\theta_{y} \u0026amp;= \\theta_{y} - \\eta \\frac{\\partial L_y}{\\partial \\theta_{y}} \\\\\\ \\theta_{d} \u0026amp;= \\theta_{d} - \\lambda \\frac{\\partial L_d}{\\partial \\theta_{d}} \\\\\\ \\theta_{f} \u0026amp;= \\theta_{f} - \\left( \\eta \\frac{\\partial L_y}{\\partial \\theta_{y}} - \\lambda \\frac{\\partial L_d}{\\partial \\theta_{d}} \\right) \\end{align*} \\end{equation}\nThrough these approaches, the goal is to find a more meaningful and less biased representation across domains, mitigating the concept drift issue.\n$$ \\text{Confounders, biases, etc} \\xrightarrow{} P_\\text{S} (Y|X) \\neq P_\\text{T} (Y|X) \\xrightarrow{} P_\\text{S} (Y|X\u0026rsquo;) = P_\\text{T} (Y|X\u0026rsquo;) $$\nUnlike domain adaptation seen previously, these approaches leverage previously ignored meta information that may reflect variance within the source domain itself. These methods don\u0026rsquo;t require access to the target domain at all, making them suitable for domain generalization. Moreover, they can be advantageous, especially when there\u0026rsquo;s no need for access to bias or sensitive attributes during inference in the target domain. On the flip side, these methods may involve more complex training and learning dynamics due to additional regularization terms.\n3.3 Causality-inspired Representation Disentanglement and Invariant Risk Minimization When performing representation learning, we can further ask if we can segregating a portion that holds more causal relevance to the target labels, and another portion that is closely associated with confounders or bias attributes. As discussed in the previous post of this series, a vision model trained on a source domain with images of cows on grassland may exhibit misclassification when confronted with a cow on ice (Causality2024). It\u0026rsquo;s obvious that the pixels of a cow should be a causal component for correct recognition of a cow while the pixels of background is related to the dataset biases (Fig. 8a).\nFig 8. Illustration for Representation Disentanglement. (a) An image of a cow standing on a grassland can be decomposed into a cow and the background of the class land. For image recognition of a cow, the pixels of the cow are the causal factor with an invariant relationship with the concept label of a cow, while the background is with spurious correlation with the label of a cow. (b) Illustration for how causality-inspired representation disentanglement may look like. Raw inputs $X^e$ are encoded into $X_{\\text{inv}}$ and $X_{\\text{spu}}$, which are invariant across domains/environments (denoted as $e$) and spuriously correlated to environments, respectively. $X_{\\text{inv}}$ and $X_{\\text{spu}}$ should be independent from each other conditionally on the original class label $Y$ and environment $e$. Later, $X_{\\text{inv}}$ and $X_{\\text{spu}}$ are decoded to $y_c$ and $y_e$ for predicting the original class of interest and domain/environment label, respectively. This results in three loss terms, covering prediction errors for $Y$ and $e$ and conditional independence requirements. Source image is from a talk given by Koyejo in 2023ICML.\nTo address this, we can design the neural network to encourage disentanglement of the latent representation based on a causality-inspired decomposition (Fig. 8b). This approach is similar to the multitask learning framework discussed in last section, with the distinction that the latent space is now divided into two components. A key enhancement involves introducing a regularization term to promote the conditionally independent disentanglement of these components. This additional regularization ensures the separation of domain-invariant and domain-specific components during training. With the domain-invariant (hopefully causal) component from the latent representation space, we can now find a more stable $P(Y|X)$ across domains, mitigating the concept drift challenge.\n$$ \\text{Confounders, biases, etc} \\xrightarrow{} P_\\text{S} (Y|X) \\neq P_\\text{T} (Y|X) \\xrightarrow{} P_\\text{S} (Y|X_{\\text{inv}}) = P_\\text{T} (Y|X_{\\text{inv}}) $$\nMoving beyond disentanglement, the pursuit of fostering the invariance of learned representations across diverse domains or environments is encapsulated in Invariant Risk Minimization (IRM) (Arjovsky, et al (2019)). In contrast to the conventional training approach solely focused on minimizing empirical risk, known as Empirical Risk Minimization (ERM), as illustrated in more details in previous post, IRM takes a step further. By minimizing the risk across different environments, IRM renders the model less sensitive to variations that are irrelevant to the causal factors. The result is a representation that not only disentangles causal and spurious components but also ensures the invariance of causal components across diverse domains, thereby fortifying the model\u0026rsquo;s generalization capabilities. While IRM may only present significant improvement over EMR in scenarios involving anti-causal data-generation process (Wang \u0026amp; Veitch (2023)), IMR itself is so intriguing and worth a separate blog post or series in the future.\n3.4 Multimodal Integration and Alignment We\u0026rsquo;ve covered various tactics to enhance OOD robustness in AI/ML models. Let\u0026rsquo;s delve into one more tactic: Multimodal Integration and Alignment. This approach might not be commonly mentioned when talking about OOD robustness, but it\u0026rsquo;s an emerging strategy that proves effective. Before exploring the details of how Multimodal Integration and Alignment contribute to robustness improvement, let\u0026rsquo;s examine an example as shown below.\nFig 9. A cow playing saxophone on ice. Images were generated DALL·E 3.\nFig. 9 was generated by DALL·E 3 after receiving a text prompt of \u0026ldquo;a cow playing saxophone on ice\u0026rdquo; (link). Remarkably, the model behind DALL·E 3 seems to accurately understand various concepts, such as the cow, saxophone, and ice. This is particularly impressive given the fact that various biases present in real-world data and what such a prompt describes doesn\u0026rsquo;t exist in reality. The ML model involved in this example integrates two modalities: vision and text (Betker, et al (2023)). These modalities are integrated and aligned to match each concept before generating images based on the prompt. While the image generation part is beyond the scope of this post, multimodal integration and alignment represent a crucial tactic for enhancing the robustness of AI/ML models.\nFig 10. Contrastive Language-Image Pre-training. Source image from Fig 1 in Radford, et al (2021).\nFig. 10 illustrates Contrastive Language-Image Pre-training (CLIP), the core technique enabling vision-language integration and alignment in DALL·E. To achieve multi-modal pre-training, various images and their corresponding captions pass through an image encoder and text encoder, respectively. These encoders extract and represent the summary of information from an image $i$ and a caption $j$ as latent vectors $I_i$ and $T_j$, respectively. Training involves making the latent vectors for paired image and caption inputs ($I_i$ and $T_i$) as similar as possible, while for non-paired inputs, the vectors should be as different as possible. This process aligns the vision latent space with the text latent space, employing a contrastive learning strategy discussed in \u0026ldquo;How AI/ML Models Learn\u0026rdquo; in the last post (Xiao (2023)). CLIP leverages rich information from each modality input, capturing invariant concepts embedded in the latent space of the two modalities. Consequently, CLIP mitigates the concept drift issue. With such a pre-trained latent space, one can further conduct few-shot learning or zero-shot prediction.\n3.5 Debiasing Training Tricks In the previously discussed tactics, gradient-based learning plays a significant role. Several training techniques exist to mitigate biases in models during training. For instance, if positive and negative samples are known to be sampled from biased attribute groups, a practical approach is to design a batch sampler ensuring that all positive and negative samples within a batch originate from the same bias group. By doing so, backpropagated gradients merely reflect the target attribute of interest rather than those bias attributes.\nHowever, when the bias attribute is unknown, alternative methods come into play. One strategy involves identifying bias groups based on the latent representations of samples during the learning process. By controlling learning dynamics or applying appropriate regularization according to the latent representations, the model can be adjusted to mitigate the adverse effects of spurious correlations between biased and target attributes. Given the length of this post, I recommend interested readers explore specific examples provided in references such as Yang2023, Hong2021 and Nam2020 for further insights into these debiasing techniques.\n4. Concluding Remarks: The Pas de Deux of Data and Models In this post, we explored various strategies to address out-of-distribution (OOD) challenges, encompassing both covariate shift and concept drift, in the pursuit of robust AI/ML models. Our discussion covered domain adaptation and domain generalization methods, considering scenarios with and without prior information about the target domain. At a high level, these strategies revolve around acquiring additional data or devising more suitable model training schemes.\nBefore concluding, it\u0026rsquo;s essential to reflect on the impact of data and model architecture on performance. The top panels in Fig. 11 illustrate different fitting conditions concerning model sizes. Panels A to C depict the classic bias and variance trade-offs, where the goal of statistical learning is to approach an ideal fit (i.e., ground truth) with a reasonable number of parameters. However, with the rise of deep neural networks and improved hardware capabilities, overparameterized models have become more prevalent (Panel D in Fig. 10). These models exhibit high learning capacity to directly fit every data point, showcasing the double-decent phenomenon (Nakkiran, et al. (2021)). This phenomenon challenges the conventional bias and variance tradeoff in statistical learning. However, what\u0026rsquo;s more important here is, this toy example suggests us two modeling options: ideal fit and direct fit when faced with data.\nFig 11. Double decent phenomenon and visualization of interpolation and extrapolation zoons. Source image from Fig. 1 in Hasson, et al (2020).\nMeanwhile, when comparing the generalization in this toy case with the known ideal fit, we implicitly evaluate the accuracy of the model\u0026rsquo;s interpolation 5 and extrapolation 6. Extrapolation is generally more challenging and less accurate than interpolation, and OOD is more likely to occur in the extrapolation zone (Fig. 11G). Thus, achieving reliable extrapolation is crucial for OOD robustness. When dealing with impoverished data, seeking an ideal fit model with potential help from prior knowledge and inductive biases is still an attractive approach, especially considering its potentially better extrapolation ability compared to a direct-fit model. However, for cases with abundant data, the learning capacity of an overparameterized model may be appreciated more. Such a direct-fit on big data results in a larger interpolation zone and a smaller extrapolation zone, contributing to model robustness by relying more on interpolation than extrapolation (Fig. 11F).\nOverall, for simple problems, an ideal fit model trained through appropriate learning strategies can provide reliable extrapolation for OOD. In more complex real-world problems, finding such an ideal fit model may be challenging. However, with rich data fed to overparameterized models, the interpolation zone becomes larger, and the model\u0026rsquo;s inability to extrapolate becomes less of a liability. This example underscores the complementary nature of models and data for generalization and robustness. Appreciating the pas de deux of data and models is crucial when building trustworthy AI/ML systems. Additionally, there are other requirements for trustworthy AI/ML, such as calibration/quality of uncertainty, fairness, explainability and transparency, and privacy, which will be explored in future discussions on the road to making model predictions trustworthy decisions.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (Jan 2024). Toward Robust AI Part (2): How To Achieve Robust AI. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/.\nor add the following to your BibTeX file.\n@article{xiao2023howtoachieverobustai, title = \u0026#34;Toward Robust AI Part (2): How To Achieve Robust AI\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2024\u0026#34;, month = \u0026#34;Jan\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/\u0026#34; } References Michielli, N., Caputo, A., Scotto, M., Mogetta, A., Pennisi, O. A. M., Molinari, F., \u0026hellip; \u0026amp; Salvi, M. (2022). Stain normalization in digital pathology: Clinical multi-center evaluation of image quality. Journal of Pathology Informatics, 13, 100145.\nJong, J. (2017). Transfer learning: domain adaptation by instance-reweighting. Retrieved Jan 06, 2024, from https://johanndejong.wordpress.com/2017/10/15/transfer-learning-domain-adaptation-by-instance-reweighting/.\nLin, T. Y., Goyal, P., Girshick, R., He, K., \u0026amp; Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).\nSun, Y., Wang, X., Liu, Z., Miller, J., Efros, A., \u0026amp; Hardt, M. (2020, November). Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning (pp. 9229-9248). PMLR.\nChoudhary, A., Tong, L., Zhu, Y., Mendelson, D., Rubin, D., Litjens, G., \u0026hellip; \u0026amp; Zhu, J. (2020). Advancing medical imaging informatics by deep learning-based domain adaptation. Yearbook of medical informatics, 29(01), 129-138.\nHitchcock, C., \u0026amp; Rédei, M. (2021). Reichenbach’s Common Cause Principle (E. N. Zalta, Ed.). Stanford Encyclopedia of Philosophy; Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/entries/physics-Rpcc/\nGanin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., \u0026hellip; \u0026amp; Lempitsky, V. (2016). Domain-adversarial training of neural networks. Journal of machine learning research, 17(59), 1-35.\nCausality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from https://ff13.fastforwardlabs.com/#how-irm-works.\nKoyejo, S. (2023). On learning domain general predictors. https://icml.cc/virtual/2023/28441.\nArjovsky, M., Bottou, L., Gulrajani, I., \u0026amp; Lopez-Paz, D. (2019). Invariant risk minimization. arXiv preprint arXiv:1907.02893.\nWang, Z. \u0026amp; Veitch, V.. (2023). The Causal Structure of Domain Invariant Supervised Representation Learning. arXiv preprint arXiv:2208.06987.\nBetker, J., Goh, G., Jing, L., TimBrooks, †., Wang, J., Li, L., LongOuyang, †., JuntangZhuang, †., JoyceLee, †., YufeiGuo, †., WesamManassra, †., PrafullaDhariwal, †., CaseyChu, †., YunxinJiao, †., \u0026amp; Ramesh, A. (2023) Improving Image Generation with Better Captions.\nChen, W., Wang, W., Liu, L., \u0026amp; Lew, M. S. (2021). New ideas and trends in deep multimodal content understanding: A review. Neurocomputing, 426, 195-215.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., \u0026hellip; \u0026amp; Sutskever, I. (2021, July). Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR.\nXiao, Jiajie. (Dec 2023). Toward Robust AI Part (1): Why Robustness Matters. JX’s log. Available at: https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/.\nHasson, U., Nastase, S. A., \u0026amp; Goldstein, A. (2020). Direct fit to nature: an evolutionary perspective on biological and artificial neural networks. Neuron, 105(3), 416-434.\nNakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., \u0026amp; Sutskever, I. (2021). Deep double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and Experiment, 2021(12), 124003.\nYang, Z., Huang, T., Ding, M., Dong, Y., Ying, R., Cen, Y., \u0026hellip; \u0026amp; Tang, J. (2023). BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. arXiv preprint arXiv:2306.03355.\nHong, Y., \u0026amp; Yang, E. (2021). Unbiased classification through bias-contrastive and bias-balanced learning. Advances in Neural Information Processing Systems, 34, 26449-26461.\nNam, J., Cha, H., Ahn, S., Lee, J., \u0026amp; Shin, J. (2020). Learning from failure: De-biasing classifier from biased classifier. Advances in Neural Information Processing Systems, 33, 20673-20684.\nFocal loss adds a modulating term to conventional cross-entropy loss, focusing learning on hard misclassified examples. It dynamically scales the cross-entropy loss during the training process to penalize hard misclassified samples more than others (Lin, et al (2017)).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFine-tuning may be considered as a type of transfer learning method by people sometimes. By this definition, transfer learning may involves updating the weights of the pre-trained model as well. Meanwhile, the optional additional layers added in fine-tuning is also called adapters. Updating the entire pre-trained model can be computationally expensive due to its size, so a popular approach called efficient fine-tuning focuses on updating only the adapters. This trend has blurred the distinction between transfer learning and fine-tuning, and the terms are sometimes used interchangeably. I personally prefer to distinguish them a bit so that it can be clearer to readers how the training was actually done.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOne may also think about toughening the mock exams more than the actual test. This approach ensures that achieving high performance in the mock exams translates to good or even better performance in the real test. But here, consistent performance in mock exams and real test is emphasized. Thus similarity between mocks and real test are desired.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n$\\eta$ and $\\lambda$ in eq3 are two learning rates that update different modules in the network.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInterpolation is the process of estimating values within the range of known data points. In the context of machine learning, it refers to predicting or estimating values for data points that fall within the observed range of the training data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExtrapolation, on the other hand, involves predicting values for data points that extend beyond the range of the observed data. It\u0026rsquo;s an extension of the model\u0026rsquo;s predictions beyond the range of the training data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jiajiexiao.github.io/posts/2024-01-06_how_robust_ai/","summary":"\u003cp\u003eIn my previous \u003ca href=\"https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/\"\u003epost\u003c/a\u003e, I highlighted the growing influence and adoption of Artificial Intelligence\n(AI) and machine learning (ML) systems, discussing how they attain \u0026ldquo;intelligence\u0026rdquo; through a careful\n\u0026ldquo;data diet.\u0026rdquo; However, a fundamental challenge arises from out-of-distribution (OOD), posing barriers\nto robust performance and reliable deployment. In particular, covariate shift (\u003ca href=\"#eq1\"\u003eeq 1\u003c/a\u003e) and\nconcept drift (\u003ca href=\"#eq2\"\u003eeq 2\u003c/a\u003e) are two major types of OOD frequently encountered in practice,\ndemanding mitigation for robust model deployment.\u003c/p\u003e","title":"Toward Robust AI (2): How To Achieve Robust AI"},{"content":"Brilliant AI/ML Models Remain Brittle Artificial intelligence (AI) and machine learning (ML) have garnered significant attention for their potential to emulate, and sometimes surpass, human capabilities across diverse domains such as vision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable Diffusion has fueled optimism, with many speculating not if, but when, Artificial General Intelligence (AGI) will emerge.\nYet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical models. They are trained to transform inputs into predictive outputs, which includes tasks like classification, regression, media generation, data clustering, and action planning. Despite the awe-inspiring results, the deployment of even the most sophisticated models reveals a fundamental fragility.\nThis fragility becomes apparent in terms of unexpected or unreliable predictions. For example, you may have experienced or heard that a chatbot spewing gibberish instead of useful information. This phenomenon is called hallucinations, where the model generates text that is irrelevant or nonsensical concerning the given inputs and desired outputs. Such hallucinations are arguably inevitable in auto-regressive large language models (LLMs).\nThe implications of this fragility are profound, particularly in risk-sensitive applications. Errors from the AI/ML systems can have severe consequences. In healthcare, a misdiagnosis by an AI-powered diagnostic tool or test can lead to severe impacts on patient health and quality of life. Similarly, in autonomous vehicles, a computer vision system\u0026rsquo;s failure to accurately detect objects can result in fatal accidents.\nWhile AI/ML models often demonstrate impressive performance across numerous benchmarks during model development phases, these real-world errors persist. Degradation in performance, along with unforeseen errors, remains a significant challenge. As AI/ML technologies become increasingly integrated into society, the need for robust performance becomes paramount. The tremendous potential of AI/ML must be harnessed responsibly to ensure these models function reliably in the complex and dynamic real-world environment.\nThe Data Diet: How AI/ML Models Learn To understand why AI/ML models can stumble, we need to slightly peek under the hood at how they learn. Think of it like training a personal chef: you provide them with recipes and feedback (labels or rewards), and they gradually figure out how to transform ingredients (inputs) into delicious dishes (outputs). With this analogy, we\u0026rsquo;ll see how major types of AI/ML models learn as below.\nSupervised Learning: The most common approach, where you give the model both features (like image pixels) and labels (like \u0026ldquo;cat\u0026rdquo; or \u0026ldquo;dog\u0026rdquo;). The training process is to update the model parameters in order to reduce the error between the predicted outputs and the groundtruth labels. It\u0026rsquo;s like handing a chef a recipe book with labeled ingredients. While this method offers clarity and precision, acquiring annotated datasets can be costly.\nReinforcement Learning: A trial-and-error approach where the model explores and learns from rewards and punishments. Supervised learning also applies here, as the feedback from the reinforcement learning environment serves as labels, guiding the model to adjust its policy or value-action function for optimal long-term planning. Imagine a chef experimenting with different ingredient combinations without following a recipe and adjusting based on your reactions. That may be challenging since it requires you to taste all experimental dishes and share feedbacks.\nUnsupervised Learning: Unlike supervised learning that finds the relationship between features and labels, unsupervised learning aims to extract inherent structures or patterns from unlabeled data. It\u0026rsquo;s like a chef intuitively discerning flavor profiles and accumulating, free from the constraints of explicit recipes or examination of ingredient labels. Unsupervised methods present their own set of challenges, as models must decipher complex data structures based on simply feature values.\nSelf-Supervised Learning: Cleverly design proxy tasks that help models learn without explicit labels, like masking parts of an image or sentence and asking the model to fill in the blanks. Alternatively, one can also train the model to assess if two augmented versions of an input originate from the same base in the latent projection space, which is also called contrastive learning. These are like challenging a chef to identify mystery ingredients or create dishes from a limited pantry, which trains the chef to understand relationships among ingredients, recipes and dishes. Afterwards, the chef can likely handle more abstract or more creative meal requests from you. The self-supervised learning method eliminates the need for labeled data by using the inherent structure of the data itself, enabling the model to learn a (compressed) representation that captures intrinsic patterns within the inputs. As a result, self-supervised learning becomes more and more popular than classic unsupervised learning these days.\nRegardless of learning methods, in the training process, your AI chef is constantly adjusting their internal recipe book (model parameters) to improve their culinary skills. In other words, across these learning paradigms, a central tenet emerges: based on a set of training data, models continually adapt and refine their configurations, aiming to optimize alignment between their predictions and desired outcomes. But just like any human chef, they can be misled by faulty ingredients or biased information. Compared to the data used for model development, any discrepancies or shifts (i.e. so-called dataset shift (Hein2022)) in the distribution of data encountered during deployment may degrade performance. Unfortunately, as describing more in the next section, such dataset mismatch is common that results in AI/ML model fragility. We\u0026rsquo;ll delve deeper into these challenges, exploring the implications of distributional shifts and charting pathways to bolster AI model resilience.\nCommon yet Tricky Out-Of-Distribution We\u0026rsquo;ve seen that AI/ML models are taught to align the model outputs to desired targets based on a specific set of training data (Fig. 1). This training paradigm helps the model find \u0026ldquo;optimal\u0026rdquo; parameter values, ensuring accurate alignment between predictions and targets. However, the effectiveness of AI/ML models hinges on the similarity between the test data and the training data. In essence, the more congruent the test data is with the training data, the more reliable the model\u0026rsquo;s performance tends to be. This effectiveness pattern is common in machine learning practice.\nFig 1. Illustration of AI/ML Model Learning Process. AI/ML models, represented as parameterized hypothesis functions Hθ, transform inputs X into outputs Hθ(X). Through iterative training and optimization, the parameters θ are adjusted to minimize the discrepancy L between the model\u0026rsquo;s outputs and the target values Y.\nTo explore this phenomenon further, let\u0026rsquo;s delve into the terminologies commonly employed in contemporary literature. The dataset used for training is referred to as the source domain, while the dataset used for testing is termed the target domain (Kouw2018). These datasets are typically categorized as either Independent and Identically Distributed (IID) or Out-of-Distribution (OOD) (Hein2022). It\u0026rsquo;s crucial to understand that the effectiveness pattern mentioned earlier, rooted in a fundamental principle of PAC learning 1, assumes optimal consistency between the test data and the training data (Mohri2018). This alignment of data distribution, often referred to as the IID assumption when deploying predictive models, is a benchmark for reliable model performance. However, real-world scenarios often deviate from this idealized setting, presenting challenges in model generalization for OOD data. For instance, an AI model trained exclusively on standard bacterial DNA might misclassify a novel bacterial species, mistaking it for a known variant, rather than acknowledging uncertainty (Ren2019). Likewise, image classifiers may falter when presented with objects in unfamiliar poses or contexts that deviate from their training data (like a cow on ice (Causality2024)).\nFormally, IID and OOD are commonly defined by assessing the equality between the joint probability distributions of features and labels in both the source and target domains:\n\\begin{equation} \\begin{aligned} \\text{IID:} \\quad P_\\text{S} (X, Y) = P_\\text{T} (X, Y) \\end{aligned} \\end{equation}\n\\begin{equation} \\begin{aligned} \\text{OOD:} \\quad P_\\text{S} (X, Y) \\neq P_\\text{T} (X, Y) \\end{aligned} \\end{equation}\nThis joint distribution can be deconstructed into the conditional probability, denoted as P(Y|X), which encapsulates the relationship between inputs X and targets Y, and the marginal probability P(X), which focuses solely on inputs. OOD scenarios predominantly manifest in two distinct forms 2:\nCovariate Shift: The conditional probability holds fixed across domains but input marginal probabilities differ. This is probably the most prevalent type of for OOD. For example, training data may lack samples for a particular feature range observed at test-time and thus make it hard for the model to reliably infer unseen regimes. (see toy example in Fig2). Covariate shifts are often seen when there are some selection biases or there are batch effects on the data generation processes. Fig 2. Model Behavior under Covariate Shift. In the source domain, data points with x \u0026gt; 2 are absent, whereas the target domain features numerous such instances. Consequently, the model\u0026rsquo;s performance is compromised for x \u0026gt; 2 in the target domain.\nConcept Drift: The conditional probability between inputs and targets itself shifts across domains, even if input distributions look similar. Relationships learned during training fail to transfer (see toy example in Fig3). Concept drift can be seen when there are any changes in mechanistic changes in the data generation process that may be even harder to anticipate in advanced compared to covariate shifts. Fig 3. Model Behavior under Concept Drift. The relationship between x and y evolves across domains, rendering the previously learned model inadequate for the target domain.\nWhile the landscape of OOD encompasses various nuanced scenarios (e.g. both P(Y|X) and P(X) may vary across domains), these two categories cover most common situations. As illustrated in Figs 2 and 3, even basic examples of covariate shift and concept drift can pose challenges. From a mathematical standpoint, it\u0026rsquo;s established that IID ensures consistent performance across both source and target domains. However, achieving such consistency in an OOD context proves more challenging. In moe details, a hypothesis model ℎ\u0026rsquo;s empirical risk 3 in the target domain, denoted as \\( R_{\\text{T}}(h) \\) , can be estimated by the source domain loss ℓ weighted by the ratio between the joint distributions in the target and source domain as below:\n\\begin{equation} \\begin{align*} R_{\\text{T}}(h) \u0026amp;\\equiv \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\ell(h(x), y) P_{\\text{T}}(x, y) dx \\\\\\ \u0026amp;= \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\frac{\\ell(h(x), y) P_{\\text{T}}(x, y)}{P_{\\text{S}}(x, y)} P_{\\text{S}}(x, y) dx \\\\\\ \u0026amp;= \\sum_{y \\in Y_{\\text{T}}} \\int_{\\mathcal{X_{\\text{T}}}} \\ell(h(x), y) P_{\\text{S}}(x,y) \\frac{P_\\text{T}(x, y)}{P_\\text{S}(x, y)} dx \\\\\\ \u0026amp;\\approx \\frac{1}{n} \\sum_{i=1, x_i \\in \\mathcal{X_{\\text{S}}}, y_i \\in Y_{\\text{S}}}^{n} \\ell(h(x_i), y_i) \\frac{P_\\text{T}(x_i, y_i)}{P_\\text{S}(x_i, y_i)}. \\end{align*} \\end{equation}\nAs demonstrated by the equations above, achieving equality between the estimated target risk $\\widehat{R}_{\\text{T}}(h)$ and the estimated source risk \\( \\widehat{R}_{\\text{S}}(h) \\) typically requires \\( P_{\\text{T}}(x, y) = P_{\\text{S}}(x, y) \\) unless \\( \\ell_{\\text{T}}(h(x), y) = \\ell_{\\text{S}}(h(x), y) = 0 \\) .\nIn practice, while OOD scenarios are common, our goal remains: to achieve accurate and robust performance irrespective of whether we\u0026rsquo;re dealing with IID or OOD data. That is the requirement of robust AI/ML regardless of the IID or OOD. Consequently, the pursuit of designing AI/ML models that are resilient to a variety of OOD scenarios is crucial to ensure robust and dependable performance.\nSummary In wrapping up, this post has elucidated the foundational aspects of constructing compelling AI/ML models and shed light on the potential hurdles they encounter, particularly when confronted with OOD data. Understanding these challenges underscores the pressing need for robust AI. Ensuring that our AI systems can handle diverse and unexpected scenarios isn\u0026rsquo;t just a technical challenge—it\u0026rsquo;s crucial for their real-world applicability and trustworthiness. As we look ahead, bolstering AI\u0026rsquo;s resilience will be paramount. Join me in the forthcoming blog post, where we will explore in-depth strategies to fortify AI against these uncertainties and pave the way for more dependable and resilient machine learning solutions.\nCitation If you find this post helpful and are interested in referencing it in your write-up, you can cite it as\nXiao, Jiajie. (Dec 2023). Toward Robust AI Part (1): Why Robustness Matters. JX\u0026rsquo;s log. Available at: https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/.\nor add the following to your BibTeX file.\n@article{xiao2023whyrobustness, title = \u0026#34;Toward Robust AI (1): Why Robustness Matters\u0026#34;, author = \u0026#34;Xiao, Jiajie\u0026#34;, journal = \u0026#34;JX\u0026#39;s log\u0026#34;, year = \u0026#34;2023\u0026#34;, month = \u0026#34;Dec\u0026#34;, url = \u0026#34;https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/\u0026#34; } References Hein, M., Joaquin Quiñonero-candela, Sugiyama, M., Schwaighofer, A., \u0026amp; Lawrence, N. D. (Eds.). (2022). Dataset Shift in Machine Learning (Neural Information Processing). The MIT Press.\nMohri, M., Rostamizadeh, A. and Talwalkar, A. (2018) Foundations of Machine Learning. Cambridge, MA: The MIT Press. Chapter 2: The PAC Learning Framework, Available at: https://mitpress.ublish.com/ebook/foundations-of-machine-learning--2-preview/7093/9.\nKouw, W. M., \u0026amp; Loog, M. (2018). An introduction to domain adaptation and transfer learning. arXiv preprint arXiv:1812.11806.\nRen, J., Liu, P. J., Fertig, E., Snoek, J., Poplin, R., Depristo, M., \u0026hellip; \u0026amp; Lakshminarayanan, B. (2019). Likelihood ratios for out-of-distribution detection. Advances in neural information processing systems, 32.\nCausality for Machine Learning. Chapter 3: Causality and Invariance, Retrieved December 17, 2024, from https://ff13.fastforwardlabs.com/#how-irm-works.\nPAC learning stands for Probable Approximately Correct (PAC) learning framework, which is a foundational concept in computational learning theory that provides guarantees on the generalization performance of a learner.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe joint distribution of P(X, Y) can also be expressed in terms of P(X|Y) and P(Y). Thus, literature sometimes also mentions a third OOD scenario called label shift, meaning the P(Y) varies across domains while P(X|Y) stays stable.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEmpirical risk is a measure of the average loss incurred by a hypothesis model ℎ on a given dataset. In simpler terms, it quantifies how well a hypothesis fits the observed data. In a broader sense, the risk of a hypothesis ℎ is the expected loss it will incur when applied to new, unseen data, drawn from the underlying distribution. This is a measure of how well the hypothesis generalizes to new data. The empirical risk serves as an estimate or proxy for the true risk. When we train a model on a finite dataset, we compute its empirical risk to assess its performance on that dataset (Kouw2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://jiajiexiao.github.io/posts/2023-12-17_why_robust_ai/","summary":"\u003ch2 id=\"brilliant-aiml-models-remain-brittle\"\u003eBrilliant AI/ML Models Remain Brittle\u003c/h2\u003e\n\u003cp\u003eArtificial intelligence (AI) and machine learning (ML) have garnered significant attention for their\npotential to emulate, and sometimes surpass, human capabilities across diverse domains such as\nvision, translation, and planning. The popularity of groundbreaking models like ChatGPT and Stable\nDiffusion has fueled optimism, with many speculating not \u003cem\u003eif\u003c/em\u003e, but \u003cem\u003ewhen\u003c/em\u003e, Artificial General\nIntelligence (AGI) will emerge.\u003c/p\u003e\n\u003cp\u003eYet, beneath the in silico surface, AI/ML systems remain at their core parametrized mathematical\nmodels. They are trained to transform inputs into predictive outputs, which includes tasks like\nclassification, regression, media generation, data clustering, and action planning. Despite the\nawe-inspiring results, the deployment of even the most sophisticated models reveals a\nfundamental fragility.\u003c/p\u003e","title":"Toward Robust AI (1): Why Robustness Matters"},{"content":"Greetings! This is JJ, and I am thrilled to welcome you to my corner of the internet! Taking inspiration from Lilian Weng, whose blog has been an invaluable resource during my studies and work in AI/ML, I\u0026rsquo;ve decided to also share my learning notes, thoughts, and updates through here.\nWhy Blogging? While I used to constantly write some diary during my childhood, I have to admit that I haven\u0026rsquo;t done so for quite a while. Blogging on this site, for me, it may be more than just a digital diary. I hope to crystallize my thoughts, document my learning experiences, and engage in meaningful conversations with readers.\nLearning from the Best In aspiring to emulate the dedication and passion that Lilian brings to her work, I\u0026rsquo;ve also been influenced by a conversation with Xingyou Song. Our discussion, held in front of DeepMind\u0026rsquo;s booth at 2023 ICML, encouraged me to consider blogging as a medium for sharing practical insights. In my employment history so far, publishing the research and development work that I lead and contribute has not been an organizational priority or highly restricted. I see blogging as a means to contribute to the research community while navigating the intricate landscape of addressing urgent medical needs, such as early cancer diagnosis and therapeutic intervention.\nWhat to Expect This blog is a personal expression and does not reflect the opinions or positions of my current or former employers. I am committed to avoiding any disclosure of sensitive information from my professional engagements. Despite these constraints, I\u0026rsquo;m enthusiastic to log a diverse mix of content, encompassing my learning and experiences in AI/ML, insights from the intersection of technology and science, and reflections on the challenges and triumphs of bridging these two dynamic worlds. I might also throw in some random, fun tidbits. Stay tuned!\n","permalink":"https://jiajiexiao.github.io/posts/2023-12-03_hello_world/","summary":"\u003cp\u003eGreetings! This is JJ, and I am thrilled to welcome you to my corner of the internet! Taking\ninspiration from \u003ca href=\"https://lilianweng.github.io/\"\u003eLilian Weng\u003c/a\u003e, whose blog has been an\ninvaluable resource during my studies and work in AI/ML, I\u0026rsquo;ve decided to also share my learning\nnotes, thoughts, and updates through here.\u003c/p\u003e\n\u003ch2 id=\"why-blogging\"\u003eWhy Blogging?\u003c/h2\u003e\n\u003cp\u003eWhile I used to constantly write some diary during my childhood, I have to admit that I haven\u0026rsquo;t done so\nfor quite a while. Blogging on this site, for me, it may be more than just a digital diary. I hope\nto crystallize my thoughts, document my learning experiences, and engage in\nmeaningful conversations with readers.\u003c/p\u003e","title":"Hello World"},{"content":"Hi 👋, I'm Jiajie \"JJ\" Xiao. Thank you for visiting my personal website and taking an interest in my journey.\nI am passionate about contributing to a better world through the lens of science and technology. Currently, I serve as the Sr. Director of Machine Learning and Head of Computational Sciences at Curve Biosciences, where I lead the development of cutting-edge AI/ML-powered molecular diagnostic platforms for chronic disease surveillance. My team focuses on building innovative computational solutions that transform how we monitor and detect diseases. Prior to this role, I was a Staff Machine Learning Scientist at Freenome, where I worked on multi-domain (e.g. multiomics, EHR) models to facilitate sensitive and specific blood-based early cancer diagnosis. My expertise also extends to therapeutic development, including ML-guided protein design and drug discovery, gained through my experiences at GSK and WFU.\nI specialize in adapting state-of-the-art AI/ML advancements to navigate challenges posed by various types of high-dimensional and noisy data, thereby driving data-informed decision-making. Described as a fast learner, active researcher, and collaborative servant leader, I thrive on addressing complex, real-world problems with cross-functional teams. I am committed to using my expertise to make a positive impact on people\u0026rsquo;s lives.\nResearch Interests As a physicist with interest in mitigating human suffering from diseases, I came across multidisciplinary areas such computational biophysics, chemistry and biology. I have been investigating microscopic system through molecular dynamics studies as well as the macroscopic one in population-wise genomic discovery.\nMy journey into machine learning began in the middle of my PhD in Physics and has since become a focal point in my recent endeavors. Presently, my research is concentrated on applying advanced machine learning methodologies to tackle pressing medical challenges. I find joy in the process of researching on cutting-edge AI/ML and their applications that facilitate life-saving innovations. I am motivated by the potential impact of my research on AI4Science.\n","permalink":"https://jiajiexiao.github.io/about/","summary":"\u003ch1 align=\"center\"\u003eHi 👋, I'm \u003ca href=\"https://jiajiexiao.github.io/about/\" target=\"blank\"\u003e\nJiajie \"JJ\" Xiao\u003c/a\u003e.\u003c/h1\u003e\n\u003c!-- \u003ch3 align=\"center\"\u003eDescription \u003c/h3\u003e --\u003e\n\u003c!-- \u003cp align=\"left\"\u003e \u003cimg src=\"https://komarev.com/ghpvc/?username=jiajiexiao\u0026label=Profile%20views\u0026color=0e75b6\u0026style=flat\" alt=\"jiajiexiao\" /\u003e \u003c/p\u003e --\u003e\n\u003c!-- \u003ca target=\"_blank\" align=\"center\"\u003e\n  \u003cimg align=\"right\" top=\"500\" height=\"300\" width=\"400\" alt=\"GIF\" src=\"https://media.giphy.com/media/SWoSkN6DxTszqIKEqv/giphy.gif\"\u003e\n\u003c/a\u003e --\u003e\n\u003c!-- \u003ca target=\"_blank\" align=\"center\"\u003e\n  \u003cimg align=\"right\" top=\"500\" width=\"200\" alt=\"Avatar\"  src=\"profile.jpeg\" \u003e\n\u003c/a\u003e --\u003e\n\u003c!-- \u003ca target=\"_blank\" align=\"center\"\u003e\n  \u003cimg align=\"right\" style=\"border-radius: 50%;\" width=\"200\" alt=\"profile\" src=\"../static/images/profile.jpeg\"\u003e\n\u003c/a\u003e --\u003e\n\u003ca target=\"_blank\" align=\"center\"\u003e\n  \u003cimg align=\"right\" style=\"border-radius: 60%;\" width=\"200\" alt=\"profile\" src=\"/images/profile.jpeg\"\u003e\n\u003c/a\u003e\n\u003cp\u003eThank you for visiting my personal website and taking an interest in my journey.\u003c/p\u003e\n\u003cp\u003eI am passionate about contributing to a better world through the lens of science\nand technology. Currently, I serve as the Sr. Director of Machine Learning and\nHead of Computational Sciences at Curve Biosciences, where I lead the\ndevelopment of cutting-edge AI/ML-powered molecular diagnostic platforms for\nchronic disease surveillance. My team focuses on building innovative\ncomputational solutions that transform how we monitor and detect diseases. Prior\nto this role, I was a Staff Machine Learning Scientist at Freenome, where I worked\non multi-domain (e.g. multiomics, EHR) models to facilitate sensitive and\nspecific blood-based early cancer diagnosis. My expertise also extends to\ntherapeutic development, including ML-guided protein design and drug discovery,\ngained through my experiences at GSK and WFU.\u003c/p\u003e","title":""}]